{"post": {"title": "Where is FSR 3?", "subreddit": "Amd", "selftext": "?", "ups": 72, "permalink": "/r/Amd/comments/16qxvht/where_is_fsr_3/", "num_comments": 118}, "replies": [{"selftext": "Maybe the real FSR3 are the friends we made along the way", "ups": 1, "depth": 0, "author": "dauntaun", "replies": [{"selftext": "Everyone is like \"where is FSR-3\" and no one is like \"how is FSR-3\"", "ups": 1, "depth": 1, "author": "stormlight89", "replies": [{"selftext": "WHO is FSR-3?", "ups": 1, "depth": 2, "author": "Hashaggik", "replies": [{"selftext": "I'll do you one better! WHY is FSR 3!", "ups": 1, "depth": 3, "author": "vidati", "replies": [{"selftext": "90% of human beings: \"Because an invisible man in the sky created humans which in turn created it\"", "ups": 1, "depth": 4, "author": "bakedEngineer", "replies": [{"selftext": "(Least obsessive atheist on Reddit)", "ups": 1, "depth": 5, "author": "langstonboy", "replies": []}]}]}]}]}, {"selftext": "Friends wont make my 5700xt run Cyberpunk 2077 at 60fps on high.", "ups": 1, "depth": 1, "author": "Outsajder", "replies": [{"selftext": "Neither fsr3 my man", "ups": 1, "depth": 2, "author": "aveferrum", "replies": [{"selftext": "If its like DLSS 3 it should.\n\nNo RT obviously.", "ups": 1, "depth": 3, "author": "Outsajder", "replies": [{"selftext": "It's supposedly better than dlss 3.5", "ups": 1, "depth": 4, "author": "RK_NightSky", "replies": []}]}]}]}, {"selftext": "oof", "ups": 1, "depth": 1, "author": "CharacterPurchase694", "replies": []}]}, {"selftext": "Q4 of 2023.", "ups": 1, "depth": 0, "author": "X_irtz", "replies": [{"selftext": "31.12.2023 23:59:59", "ups": 1, "depth": 1, "author": "ag3on", "replies": []}, {"selftext": "It comes out when the new elder scrolls come out.", "ups": 1, "depth": 1, "author": "Yodawithboobs", "replies": []}]}, {"selftext": "The only official statement I've seen from GPUopen (the people who actually make FSR) was before the end of 2023. \n\nAs is so often the case with efforts to release something as open-source, they may be stuck in a legal review as they make sure there aren't any potential conflicts.", "ups": 1, "depth": 0, "author": "mad_mesa", "replies": [{"selftext": "How could there possibly be any conflicts? It almost feels like you're implying AMD will plagiarize someone.", "ups": 1, "depth": 1, "author": "Dranzule", "replies": [{"selftext": "It's to ensure they are not plagiarizing someone. The code is open source and therefore can't include the works of others or it'll be tied up in legal hell after release. They're trying to avoid that. It's a normal process.", "ups": 1, "depth": 2, "author": "Arkrobo", "replies": [{"selftext": "Would like to think everyone would've ensured that BEFORE they even started developing it.", "ups": 1, "depth": 3, "author": "Dranzule", "replies": [{"selftext": "It can happen by accident though. You code independently and it starts to look like someone else's end result. I'm sure they tried to avoid it, they now confirm it with legal teams. It's a pretty normal part of releasing things to avoid IP issues.", "ups": 1, "depth": 4, "author": "Arkrobo", "replies": [{"selftext": "Sure, but I'd still like to imagine that would have been a thought far before they reached the final stages of development.", "ups": 1, "depth": 5, "author": "Dranzule", "replies": [{"selftext": "There's always copyright trolls and hungry lawyers waiting to strike on *literally* anything.", "ups": 1, "depth": 6, "author": "handymanshandle", "replies": []}]}]}, {"selftext": "You can be as sure and certain every step of the way. Doesn't mean they won't still go through a verification &amp; certification process by the end. You cannot make mistakes like infringement, so it's *good* to be diligent. I'd wager their legal and engineering teams are well aware of this importance.", "ups": 1, "depth": 4, "author": "Omegachai", "replies": []}, {"selftext": "How the hell would that work?\n\nPlease inspect my new bridge.  I haven't built it yet, but it will be very good.", "ups": 1, "depth": 4, "author": "hedoeswhathewants", "replies": [{"selftext": "Except this is software development. Copyright issues can arise whether written code is too similar to another which already has authorship. You may make something that does the same as other people, as long as how you do it is different. AMD can build temporal upscalers, as long as they don't exactly copy whatever someone else has already done.\nIf AMD is reviewing how they built their upscaler to avoid copyright issues AFTER they had already planned it, it almost feels like wasted manpower, if actual issues arise.", "ups": 1, "depth": 5, "author": "Dranzule", "replies": []}]}]}]}]}]}, {"selftext": "Nowhere", "ups": 1, "depth": 0, "author": "feorun5", "replies": []}, {"selftext": "Give it couple more months", "ups": 1, "depth": 0, "author": "Edgaras1103", "replies": [{"selftext": "They said September. And I quote from anantech article linked in comments below: The first two games to get FSR 3 support will be the aforementioned Forspoken, as well as the recently launched Immortals of Aveum. **AMD expects FSR 3 to be patched in to both games here in September \u2013 presumably towards the later part of the month**", "ups": 1, "depth": 1, "author": "Ponald-Dump", "replies": [{"selftext": "i see. Then give it couple more months", "ups": 1, "depth": 2, "author": "Edgaras1103", "replies": [{"selftext": "I see what you did there", "ups": 1, "depth": 3, "author": "Ponald-Dump", "replies": []}]}, {"selftext": "Where did they say September? in the gamescom presentation they only said \"early fall\".", "ups": 1, "depth": 2, "author": "Pazret", "replies": [{"selftext": "https://www.pcworld.com/article/2041810/fsr-3-amds-open-dlss-3-rival-starts-supercharging-games-in-september.html", "ups": 1, "depth": 3, "author": "Ponald-Dump", "replies": [{"selftext": "Where do you see AMD statement?", "ups": 1, "depth": 4, "author": "CrazyBaron", "replies": [{"selftext": "https://www.anandtech.com/show/20032/amd-teases-fsr-3-and-hyprrx-updated-radeon-performance-technologies-available-in-september\n\nYou think multiple outlets are pulling September out of their asses? Read the article, it specifically states \u201cseptember launches\u201d\n\nDirect quote since I\u2019m getting downvoted by people who are too lazy to click and read: The first two games to get FSR 3 support will be the aforementioned Forspoken, as well as the recently launched Immortals of Aveum. **AMD expects FSR 3 to be patched in to both games here in September \u2013 presumably towards the later part of the month**", "ups": 1, "depth": 5, "author": "Ponald-Dump", "replies": [{"selftext": "Wont be first time for them? What is problem with giving direct quite from AMD in any of them?\n\nFor all we know AMD stated early fall.", "ups": 1, "depth": 6, "author": "CrazyBaron", "replies": [{"selftext": "And when is early fall? September.", "ups": 1, "depth": 7, "author": "Ponald-Dump", "replies": [{"selftext": "Well if you want to be technical about it, definition of fall and it months varies by regions.", "ups": 1, "depth": 8, "author": "CrazyBaron", "replies": [{"selftext": "Is AMD headquartered in Australia? No? Then that\u2019s irrelevant.", "ups": 1, "depth": 9, "author": "Ponald-Dump", "replies": []}]}, {"selftext": "Autumn started literally yesterday for the northern hemisphere (23 September), so no.\n\nEarly fall could easily be the first half of autumn, so like last week of October.\n\nAMD never stated September anywhere, just media outlets making assumptions like always.", "ups": 1, "depth": 8, "author": "Pazret", "replies": [{"selftext": "Direct quote from the article you clearly didnt read: The first two games to get FSR 3 support will be the aforementioned Forspoken, as well as the recently launched Immortals of Aveum. **AMD expects FSR 3 to be patched in to both games here in September \u2013 presumably towards the later part of the month**", "ups": 1, "depth": 9, "author": "Ponald-Dump", "replies": []}]}]}]}]}]}]}]}, {"selftext": "&gt;AMD expects\n\nWhere did they *say* September?", "ups": 1, "depth": 2, "author": "danny12beje", "replies": [{"selftext": "Did you forget the rest of that quote? Specifically where it says *AMD expects* and then *September*?", "ups": 1, "depth": 3, "author": "Ponald-Dump", "replies": [{"selftext": "Um no. Do you not know what \"presumably\" means?\n\nIt's an assumption of the Editors, not what AMD said.\n\nAlso \"expects\" doesn't mean \"announced\" or \"confirmed\".", "ups": 1, "depth": 4, "author": "danny12beje", "replies": [{"selftext": "Read. The. Quote. **In its entirety**. They said, and I quote AGAIN since comprehension isn\u2019t working for you \u201c\u2026..September \u2014 **presumably toward the latter part of *the* month** \n\nThat means: September, and they *presume* the latter part of *that* month. (By they I mean the author since you\u2019re still not tracking)\n\nAnd if AMD expects September, that means they said September. Did they guarantee September? Obviously not. But they *SAID* September. Which is the point I\u2019m making", "ups": 1, "depth": 5, "author": "Ponald-Dump", "replies": [{"selftext": "....my guy the presumably part isn't AMD's statement. \n\nJesus Christ some of you mfers have trouble with reading comprehension.", "ups": 1, "depth": 6, "author": "danny12beje", "replies": [{"selftext": "Holy hell you\u2019re dense. No shit it\u2019s not part of their statement, that part came from the writer. The \u201cAMD expects *AND* September\u201d came from AMD. THE WRITER PRESUMES that AMD means the *latter part of the month*. How is this so difficult for you to understand?", "ups": 1, "depth": 7, "author": "Ponald-Dump", "replies": []}]}]}]}]}]}, {"selftext": "Still September", "ups": 1, "depth": 2, "author": "Dillonvillon", "replies": [{"selftext": "Yep there\u2019s still time", "ups": 1, "depth": 3, "author": "Ponald-Dump", "replies": []}]}]}]}, {"selftext": "The two games that's supposed to have it flopped so bad that one of the studios shut down and the other laid of half their staff.\n\nSo who know what's going on there.", "ups": 1, "depth": 0, "author": "From-UoM", "replies": [{"selftext": "What games were those", "ups": 1, "depth": 1, "author": "NewestAccount2023", "replies": [{"selftext": "Forspoken with Luminous Studio who got shutdown with staff being split amongst Square Enix's multiple division or let go. Can you believe that Forspoken cost over 100 million?\n\nImmortals of Aveum with Ascendant Studio laying off half their staff just last week after the games flop.", "ups": 1, "depth": 2, "author": "From-UoM", "replies": []}]}]}, {"selftext": "After dlss 4 comes out", "ups": 1, "depth": 0, "author": "neueziel1", "replies": []}, {"selftext": "Everybody asks \"Where is FSR 3?\". Nobody asks \"How is FSR 3?\"", "ups": 1, "depth": 0, "author": "vladi963", "replies": [{"selftext": "How is FSR 3 ?", "ups": 1, "depth": 1, "author": "Melodias3", "replies": []}, {"selftext": "It\u2019s delay and lack of much info tells me how it\u2019s gunna be. Not bad, but very much so in the shadow of DLSS 3 even, not to mention DLSS 3.5", "ups": 1, "depth": 1, "author": "ibeerianhamhock", "replies": []}]}, {"selftext": "FSR 3 is the friends we made along the way.", "ups": 1, "depth": 0, "author": "TechieTravis", "replies": []}, {"selftext": "Nvidia is announcing DLSS 4 before FSR3 comes out.", "ups": 1, "depth": 0, "author": "ShowBoobsPls", "replies": []}, {"selftext": "Fall started 3 days ago, it lasts 3 months, early fall still has some time.\n\nLikely lost in legal to make sure there's no issue when publishing the code source.", "ups": 1, "depth": 0, "author": "Cultural_Analyst_918", "replies": []}, {"selftext": "Some day FSR 3 is only relevant for their Console Section, to boost the Lifeline of current Gens a little more and let them catch up with up to date Techniques\n\nThey only can catch up on PC if FSR 3 is like [this](https://imgur.com/gallery/GrmyQjS) injection Stuff, without breaking the Engine (Card).", "ups": 1, "depth": 0, "author": "WorldwideDepp", "replies": []}, {"selftext": "Good question i am wondering the same, september is almost over and we where promised FSR3 in september.", "ups": 1, "depth": 0, "author": "Melodias3", "replies": []}, {"selftext": "Hiding in the closet.", "ups": 1, "depth": 0, "author": "Yodawithboobs", "replies": []}, {"selftext": "Well it said September so if that's the case then maybe sometime this week but not then idk maybe sometime this fall", "ups": 1, "depth": 0, "author": "strongr_togethr", "replies": []}, {"selftext": "At home.", "ups": 1, "depth": 0, "author": "Mysteoa", "replies": []}, {"selftext": "''Soon\"", "ups": 1, "depth": 0, "author": "squadraRMN", "replies": []}, {"selftext": "I don't want FSR 3 if it comes with the same horrible temporal upscaling that comes with FSR 2. They really need to work on FSR 2 first, they're like 4 years behind NVIDIA now with DLSS 3.5", "ups": 1, "depth": 0, "author": "Mcdreamy808", "replies": [{"selftext": "DF said that \u201cFSR 3 is impressive to say the least\u201d, but they just got hands on Forspoken and Arvenus", "ups": 1, "depth": 1, "author": "Filianore_", "replies": [{"selftext": "*Hands-on* is being generous. They saw it playing on a screen with no actual control. They themselves chose to call it an \"eyes-on\".", "ups": 1, "depth": 2, "author": "dadmou5", "replies": []}]}]}, {"selftext": "Let them cook bro", "ups": 1, "depth": 0, "author": "amoghthebad", "replies": []}, {"selftext": "AMD deadlines mean nothing they constantly overpromise and don't deliver. HyperRX was supossed to come out in the first half of the year but it didn't", "ups": 1, "depth": 0, "author": "GamerOne48", "replies": []}, {"selftext": "I think it ready as a technology but has not been properly implemented in the games yet.Maybe in oct but can't be sure.", "ups": 1, "depth": 0, "author": "Aggravating-Mix2054", "replies": []}, {"selftext": "They said this fall", "ups": 1, "depth": 0, "author": "Buris", "replies": []}, {"selftext": "Pretty sure they said 2024", "ups": 1, "depth": 0, "author": "H-Man991", "replies": [{"selftext": "That\u2019s for driver-based FG, FSR3 was announced this fall, \u201ca few more weeks\u201d about a month ago", "ups": 1, "depth": 1, "author": "Buris", "replies": [{"selftext": "Wait, I'm confused. So, the FS3 releasing has to be implemented by the devs then ? I thought the driver level version was supposed to come out this year and work on every game. That's a major blow if so, I don't think too many games are going to have FSR3 this year.", "ups": 1, "depth": 2, "author": "SillyRecover", "replies": [{"selftext": "Im expecting the driver level FG to be shit not just image quality-wise but latency too, because they have no Anti-Lag+ to combat the increased latency of frame gen..\n\nLike if Nvidia offered frame gen without reflex, I wouldn't use that.", "ups": 1, "depth": 3, "author": "ShowBoobsPls", "replies": [{"selftext": "FS3 has built-in latency reduction solution. And Anti-lag + does exist but only for RDNA3", "ups": 1, "depth": 4, "author": "SillyRecover", "replies": [{"selftext": "The driver level FSR 3 only has the normal anti-lag which is pretty bad compared to reflex.", "ups": 1, "depth": 5, "author": "ShowBoobsPls", "replies": [{"selftext": "The driver level FSR 3 will have access to both Anti-Lag and Anti-Lag+, depending on the game in question. For example, Witcher 3 supports both AL and AL+, so you can combine both of them with driver level FSR 3.", "ups": 1, "depth": 6, "author": "punished-venom-snake", "replies": [{"selftext": "Yes but the anti-lag + has to be implemented per game basis.", "ups": 1, "depth": 7, "author": "ShowBoobsPls", "replies": [{"selftext": "And that's not an inherent issue with driver level FSR 3 which should work with all DX11/12 games. It's an issue with the implementation of AL+ itself which I think AMD is looking into.", "ups": 1, "depth": 8, "author": "punished-venom-snake", "replies": [{"selftext": "I think latency is an inherent issue to the driver level frame gen. Well all frame gens. AL+ is just a remedy to it just like reflex.", "ups": 1, "depth": 9, "author": "ShowBoobsPls", "replies": []}]}]}]}, {"selftext": "Oh, okay, I don't know what's going on. There's too many moving parts and pieces around the single solution of frame gen.", "ups": 1, "depth": 6, "author": "SillyRecover", "replies": []}, {"selftext": "If a game supports anti-lag+ I don't know why you couldn't have both enabled as they were meant to be used together- It's going to be wild because if a game naturally has bad input lag it most likely will be completely awful, but if a game has great input lag and well above 60FPS from the start enabling driver-level frame generation could a good trick", "ups": 1, "depth": 6, "author": "Buris", "replies": [{"selftext": "If a game supports anti-lag +, it most likely has a native FSR 3 support.\n\nA game that you would use the driver level frame gen on would most likely have the normal anti-lag only", "ups": 1, "depth": 7, "author": "ShowBoobsPls", "replies": [{"selftext": "[https://www.reddit.com/r/Amd/comments/16bwmjn/complete\\_list\\_of\\_games\\_that\\_support\\_antilag/](https://www.reddit.com/r/Amd/comments/16bwmjn/complete_list_of_games_that_support_antilag/)  \n\n\nFSR3 is only coming to 3 of these titles, at least this year", "ups": 1, "depth": 8, "author": "Buris", "replies": []}]}]}]}]}, {"selftext": "I'm waiting for FS3 to come out before buying a new card. If FSR3 is good, I'm going AMD. If it's dogshit then I'll just have to deal with Nvidia. Nvidia is price gouging ( both are ) but I can't deal with being 5 years behind on important software features.", "ups": 1, "depth": 4, "author": "SillyRecover", "replies": []}, {"selftext": "Anti-Lag+ can and will be used in conjunction with driver-level frame generation, but it's not currently widely supported, so far just 16 games", "ups": 1, "depth": 4, "author": "Buris", "replies": [{"selftext": "Yes and as far as I know more than 16 games exist. So with the rest of the games you need to use standard anti lag.\n\nFuture games are a different deal.", "ups": 1, "depth": 5, "author": "ShowBoobsPls", "replies": [{"selftext": "Anti-Lag+ does not require a temporal upscaling solution already be built into the engine in order to implement it, FSR3 does.\n\nBy the end of the year there could be 50+ Anti-Lag+ games and only 5 FSR3 titles. As far as I know it's only an hour or two dev time to add Anti-Lag+ to games that already support Reflex-\n\nRegardless, if a game has low input lag and pre-FG FPS is above 70 to begin with driver-level frame generation won't be a bad experience. Definitely won't be as good as DLSS3 FG or FSR3, but it shouldn't be bad either", "ups": 1, "depth": 6, "author": "Buris", "replies": []}]}]}]}]}]}]}, {"selftext": "People still don't get that amd gear is supposed to age like fine wine. It's not the fresh juice that is nvidia. You know, you pay it, and you get it. This fine wine, which needs time. You pay now, and maybe in a couple of years you will get the tech.", "ups": 1, "depth": 0, "author": "Dull_Wasabi_5610", "replies": [{"selftext": "&gt; and maybe in a couple of years\n\nYou'll open the cask and find it turned to vinegar.", "ups": 1, "depth": 1, "author": "dookarion", "replies": []}]}, {"selftext": "Same place as AMD's relevance these days... nobody can find it.", "ups": 1, "depth": 0, "author": "penguished", "replies": []}, {"selftext": "They said early fall either this week or at some point in October.", "ups": 1, "depth": 0, "author": "Sensitive_Step2920", "replies": []}, {"selftext": "In drivers development section in buildings/offices of AMD stored in ssd, hdd, external storage..", "ups": 1, "depth": 0, "author": "speznatzz", "replies": []}, {"selftext": "After black friday when you upgraded and dont actually need it anymore. No,\u2026 take your time, make it good.", "ups": 1, "depth": 0, "author": "noid-", "replies": []}, {"selftext": "Lost in AI development", "ups": 1, "depth": 0, "author": "amit1234455", "replies": []}, {"selftext": "here", "ups": 1, "depth": 0, "author": "nemanja694", "replies": []}, {"selftext": "M.I.A", "ups": 1, "depth": 0, "author": "PRTLite", "replies": []}, {"selftext": "Given that they will release fsr 3. I think it will take another time to apply it to your favorite games. It really depends on respective developers.", "ups": 1, "depth": 0, "author": "AntiqueWeb8525", "replies": []}, {"selftext": "What if FSR 3 was the real one piece?", "ups": 1, "depth": 0, "author": "BestPloot", "replies": []}, {"selftext": "Don\u2019t want them to rush it, let them take as long as they want polishing it. Everyone will soon complain when they rush to release it and it ends up being unpolished and buggy", "ups": 1, "depth": 0, "author": "Sourrzx", "replies": []}, {"selftext": "obviously it's not ready for release, or else you wouldn't stop hearing about it from them. Let it come out when it's ready! They need it to be good enough to be competitive! high stakes! I know the delay not doing them any favors, but it's better to release something good enough rather than bad product", "ups": 1, "depth": 0, "author": "srgtDodo", "replies": []}, {"selftext": "I'm over here wondering when BG3 will have FSR 2.1... :/", "ups": 1, "depth": 0, "author": "Asazel000", "replies": []}, {"selftext": "They said September 30?", "ups": 1, "depth": 0, "author": "RoleCode", "replies": []}, {"selftext": "How is FSR 3?", "ups": 1, "depth": 0, "author": "Relevant_Force_3470", "replies": []}, {"selftext": "Will the real FSR 3 please stand up.", "ups": 1, "depth": 0, "author": "uniq_username", "replies": []}, {"selftext": "On the private PC from Lisa Du :)", "ups": 1, "depth": 0, "author": "mewkew", "replies": []}, {"selftext": "A lot of people with 6800XT. Do you think FSR 3 will save CP2077 with RTOD?", "ups": 1, "depth": 0, "author": "AngCorp", "replies": []}, {"selftext": "Didn't they say this month?", "ups": 1, "depth": 0, "author": "Healthy_BrAd6254", "replies": []}, {"selftext": "FSR 3 will present itself once you're ready", "ups": 1, "depth": 0, "author": "ZeinThe44", "replies": []}, {"selftext": "Once GTA 6 is released. It will be a AMD sponsored game.", "ups": 1, "depth": 0, "author": "Choco__butternut", "replies": []}, {"selftext": "Fsr 3 will still be shit compared to dlss, AMD has not the manpower nor the tech to pull the voodoo magic Nvidia is doing.", "ups": 1, "depth": 0, "author": "Yodawithboobs", "replies": []}, {"selftext": "September 2023 , here another 6 days to wait it", "ups": 1, "depth": 0, "author": "savvasr200", "replies": []}]}
{"post": {"title": "Ryzen 5700x with 7800xt", "subreddit": "Amd", "selftext": "Hi, haven\u2019t built a PC in over 10 years, but am looking at getting one built after my Laptops been cooking trying to play Starfield. You think this is a good pairing? Will the 5700x bottleneck the 7800xt?", "ups": 15, "permalink": "/r/Amd/comments/16qlrbw/ryzen_5700x_with_7800xt/", "num_comments": 16}, "replies": [{"selftext": "Definitely push for am5, even if you have to step down your CPU. You can upgrade your CPU later to newer generations.", "ups": 19, "depth": 0, "author": "dirthurts", "replies": []}, {"selftext": "IF you are building from scratch.. ie... purchasing all new components...\n\nI'd recommend getting a Ryzen 5 7600'ish B650 combo with 2x16gb or 2x32gb 6000 rated RAM kit behind that 7800xt\n\nGet an 850wtt 80+Gold or better rated power supply from a reputable brand... check the \"PSU Tier List\" and check hardware review sites before purchasing anything\n\nBut.. to directly answer your question.. yes, that would be a good match up for 1440p gaming\n\nEither way... AM4 or AM5.. I've had great success on both platforms running a plethora of ASRock motherboards\n\nMy wife is still rocking her 5900x on an ASRock B550m Phantom Gaming 4 w/ 4x16gb 3200 CAS16 kit @ 3600 CAS16, ASRock 5700xt Phantom Gaming D, Seasonic Focus 850wtt 80+Platinum\n\nMy daughter and I run AM5 setups with ASRock motherboards", "ups": 9, "depth": 0, "author": "D33-THREE", "replies": []}, {"selftext": "have a 5700x and 7900xt no issues here but i already had am4 so it was just a cpu upgrade id go with am5 or if your stuck with am4 get the 5800x3d", "ups": 2, "depth": 0, "author": "Arcticwolfi6", "replies": []}, {"selftext": "7600 non x on am5 is about the same speed as a 5700x. I'd go for that if you're looking to save on cost and you can upgrade it to a higher core count one in later generations of AM5", "ups": 2, "depth": 0, "author": "ofon", "replies": []}, {"selftext": "If Starfield is a priority, get AM5 as the 7000 series Ryzens perform much better in that game. 7600/7600X are the value for money budget options, 7800X3D if you want the best and are willing to pay for it.", "ups": 2, "depth": 0, "author": "PloughYourself", "replies": []}, {"selftext": "What's your budget? The AM4 platform is dead, so it's better to try and build with a 7600 and DDR5 RAM instead of the 5700X and DDR4 RAM. The 7600 is also much faster for gaming.", "ups": 2, "depth": 0, "author": "OptionPleasant7133", "replies": [{"selftext": "&gt;The AM4 platform is dead, so it's better to try and build with a 7600 \n\n\\*7500f is what should be recommended instead of a 7600.", "ups": 1, "depth": 1, "author": "Kiriima", "replies": [{"selftext": "Sure if you can find one.... not in the US.", "ups": 1, "depth": 2, "author": "OptionPleasant7133", "replies": [{"selftext": "You can import it.  \n[https://www.shopblt.com/cgi-bin/shop/shop.cgi?action=thispage&amp;thispage=011003000503\\_BHH2602P.shtml&amp;order\\_id=!ORDERID](https://www.shopblt.com/cgi-bin/shop/shop.cgi?action=thispage&amp;thispage=011003000503_BHH2602P.shtml&amp;order_id=!ORDERID)!  \n12 in stock right now.  If you don't get those you can also import it from Aliexpress sellers and wait 10-14 days for delivery.  Either way it IS available.", "ups": 1, "depth": 3, "author": "DuskOfANewAge", "replies": [{"selftext": "Lol fair enough...but that costs 200$, which is very close to the price of a 7600 (220$). \n\nAnyway whatever, OP can buy whatever is the cheapest AM5 part that makes sense.", "ups": 1, "depth": 4, "author": "OptionPleasant7133", "replies": []}]}]}]}, {"selftext": "Yeah, unless someone's trying to make a budget build with like 5500 or 5600 with RX 6600, AM5 is much better. Especially since OP has budget for 7800 XT.", "ups": 1, "depth": 1, "author": "popop143", "replies": []}]}, {"selftext": "If you have a MicroCenter nearby,  check out their bundles.", "ups": 1, "depth": 0, "author": "ItsBugginOuT", "replies": []}, {"selftext": "AM5 with PCIe 5.0 if you're building a new system. \n\nRecently built a new 7800X3D setup and it was cheaper than I anticipated. Would recommend.", "ups": 1, "depth": 0, "author": "SimplyTemporary2023", "replies": [{"selftext": " the price gap is pretty small  most of it is just the cpu cost", "ups": 1, "depth": 1, "author": "Desperate_Bug_119", "replies": []}]}, {"selftext": "There's no point getting 5700x for a new gaming build. Get a Ryzen 5 7600 or wait for a 7500F to come to your region. (it's already available in europe)", "ups": 1, "depth": 0, "author": "colonelwaffle77", "replies": []}, {"selftext": "Perfect pairing actually, but at this point of time just go for AM5. Start off with a cheap CPU like the R5 7600, you will still have a good experience and you have upgrades on the table.", "ups": 1, "depth": 0, "author": "UC347", "replies": []}]}
{"post": {"title": "7800X3D to pair with 7800XT?", "subreddit": "Amd", "selftext": "I'm looking to build my first ever rig and with the way the market is at the moment, imo it seems team red is the way to go. But are the CPU and GPU combo mismatched at all?\n\nAlso what 7800XT cards are best to go for or avoid, between:\n\n\u2022Sapphire Pulse\n\u2022Sapphire Pure\n\u2022Sapphire Nitro+\n\u2022Powercolor Red Devil\n\u2022Powercolor Hellhound\n\u2022XFX Qick 319\n\u2022XFX Merc 319", "ups": 3, "permalink": "/r/Amd/comments/16qqdrn/7800x3d_to_pair_with_7800xt/", "num_comments": 22}, "replies": [{"selftext": "the 7800x3d is a great pick for gaming and it should be paired with the best of the best of the gpu .. but in your situation it also give you room for gpu upgrade in the future..", "ups": 3, "depth": 0, "author": "Vraluki90", "replies": []}, {"selftext": "id go for 7600 + 7900xt", "ups": 10, "depth": 0, "author": "searchableusername", "replies": [{"selftext": "Yeah I got a 7600x and 7800xt, he's putting way too much money into CPU unless he plays exclusively competitive pvp games.", "ups": 0, "depth": 1, "author": "Eldorian91", "replies": [{"selftext": "No but the games I play are very CPU intensive", "ups": 5, "depth": 2, "author": "lmaoAbu", "replies": [{"selftext": "are you certain? the only games that rely on the cpu as much as other games rely on the gpu are like grand strategy games", "ups": 2, "depth": 3, "author": "searchableusername", "replies": [{"selftext": "and various simulators.", "ups": 3, "depth": 4, "author": "buildzoid", "replies": []}]}, {"selftext": "What games are you playing that are CPU intensive?", "ups": 1, "depth": 3, "author": "Toast_Meat", "replies": [{"selftext": "Football Manager mainly", "ups": -1, "depth": 4, "author": "lmaoAbu", "replies": [{"selftext": "I don't think that will tax the 7600 to 100% utilization no? I'm not sure though.", "ups": 2, "depth": 5, "author": "popop143", "replies": [{"selftext": "It depends on the amount of leagues loaded and the amount of detail the database is in", "ups": 1, "depth": 6, "author": "lmaoAbu", "replies": []}]}]}]}, {"selftext": "are you playing in 1080p?  If not, spend roughly twice as much on gpu as you spend on cpu.", "ups": 0, "depth": 3, "author": "Eldorian91", "replies": []}]}]}]}, {"selftext": "I would go for 7800x3d and 7900xt+ if possible due to that extra vram\n\nIf not and your going the 7800xt route i would say Sapphire nitro+ 7800xt. Got the same model 7900xtx and temps is extremely good even at low fanspeed\n\nTake a look here playing ac Valhalla maxed settings 1440p with a 5800x3d with slight oc\n\nhttps://youtu.be/eYbv5-AV24A?si=i4UK-56K03FmFySg", "ups": 1, "depth": 0, "author": "Nord5555", "replies": []}, {"selftext": "7800xt is the best value card right now new. The cpu pairing is fine. I have a 7800xt and a 6950xt and both are paired with my 7950x3d (somewhat similar gaming to your choice). \n\nI have XFX, my buddy a sapphire.  I have several other and cards\n\nhere is a ROUGH ranking:\n\n1. power colour Hellhound\n2. Sapphire Nitro\n3. Power colour Red devil\n4. XFX (either)\n5. Sapphire pulse/pure\n\nlong story short. ALL of these are great cards. and all of these are rock solid. just get whichever is cheaper and available. \n\n\nyou don't need to avoid any of these brands whatsoever. just avoid gigabyte.", "ups": 0, "depth": 0, "author": "Method__Man", "replies": [{"selftext": "In my place Hellhound and PURE is ~US$524, Nitro $560 and Red devil $575. Any suggestions? just get the cheapest?", "ups": 1, "depth": 1, "author": "chris_854", "replies": [{"selftext": "Red devil is the best imo. I literally don't hear anything with the fans, it's almost like they don't exist.", "ups": 1, "depth": 2, "author": "Aurey2244", "replies": []}]}]}, {"selftext": "Just buy 4090, AMD doesn't have equivalent.", "ups": -10, "depth": 0, "author": "speznatzz", "replies": []}, {"selftext": "The 7800X3D is a hecking good CPU.  Unless you're playing at 1080p, it's more CPU than you really need to saturate a 7800XT.  \n\nOn the other hand, if you're willing to pay a couple hundred dollars extra for a CPU that will probably last a good year or two longer than a 7600, then it's there.  If you like games that *love* that big ole L3 cache like Stellaris or MS Flight Sim, there's none better.  If you just want to be 100% sure that you have enough CPU for gaming and don't want to have to worry about it at all for quite a while, it'll do that at least until the end of this console generation.", "ups": 1, "depth": 0, "author": "INITMalcanis", "replies": []}, {"selftext": "I have a 7800x3d with the Powercoler 7800 XT Red Devil and really like the setup.", "ups": 1, "depth": 0, "author": "ElRaydeator", "replies": []}, {"selftext": "Ok there are a lot of clueless people.\n\nIt matters what you play!\n\nIf you play MMOs you\u2019ll get waaaay more out of that 7800X3D. If you play mostly single player (with exceptions for big open world cpu bound games) a 7600/7700 will probably do fantastic and a beefier GPU will help more.", "ups": 1, "depth": 0, "author": "Azelar", "replies": []}, {"selftext": "Sapphire Nitro+", "ups": 1, "depth": 0, "author": "MEGA_GOAT98", "replies": []}, {"selftext": "Might as well just get a 7900 xtx", "ups": 1, "depth": 0, "author": "wewerecreaturres", "replies": []}, {"selftext": "Running that config now 1440p .\nGlorious", "ups": 1, "depth": 0, "author": "kaisersolo", "replies": []}]}
{"post": {"title": "7800 XT \u201ccoming soon\u201d", "subreddit": "Amd", "selftext": "Any idea why it still says \u201ccoming soon\u201d on their website as the GPU is already out since 6th of September? I\u2019m from Europe.", "ups": 22, "permalink": "/r/Amd/comments/16pygyp/7800_xt_coming_soon/", "num_comments": 26}, "replies": [{"selftext": "Sold out", "ups": 24, "depth": 0, "author": "H-Man991", "replies": []}, {"selftext": "Europe 3rd world country for AMD..", "ups": 21, "depth": 0, "author": "Amstradcpc664", "replies": [{"selftext": "its a weird statement as amd sells as many cards as nvidia here\n\nEU just slapped Intel with a 400m fine again  \nseems more like other countries need to fix their anti consumer practices", "ups": -6, "depth": 1, "author": "RBImGuy", "replies": [{"selftext": "&gt; its a weird statement as amd sells as many cards as nvidia here\n\nNo, they do not. MF is not a represenation of aggregate EU sales.", "ups": 16, "depth": 2, "author": "Zednot123", "replies": [{"selftext": "True but AMD really is more popular here compared to the US it seems.\n\nDefinitely for DIY CPUs but that's because Intel is sleeping. And Radeon GPUs also seem to be slightly more popular here.\n\nCan't explain it.", "ups": 5, "depth": 3, "author": "ZaadKanon69", "replies": []}]}]}]}, {"selftext": "they are sold out and waiting for the new batch", "ups": 10, "depth": 0, "author": "pesca_22", "replies": [{"selftext": "They are not sold out in EU, they where never for sale in EU.", "ups": 1, "depth": 1, "author": "PrettyQuick", "replies": []}]}, {"selftext": "No idea why. Ordered mine on the 6th in one of the local tech stores that is partnered with amd and got it a week later.", "ups": 3, "depth": 0, "author": "RK_NightSky", "replies": [{"selftext": "In which country are you based?", "ups": 2, "depth": 1, "author": "musicdean23", "replies": [{"selftext": "Bulgaria, i'm probably one of the first and few that have it in my country", "ups": 3, "depth": 2, "author": "RK_NightSky", "replies": []}]}]}, {"selftext": "No idea, but the prices of 7700's XT are getting almost as high as 7800's XT since the latter are pretty much impossible to buy in the desired versions. \n\nAlso other ridiculous things happen like i.e. the highly praised Powercolor Hellhound 78000 XT got sold out immediately and those shops that still have it on offer try to sell it for 200 euro more than the launch price 2 weeks ago - around 750 instead of 550. Aside from this there were pretty much no stock updates in those 2 weeks and the prices of the few remaining 7800's are raising basically daily.\n\nI was hyped for the 7800 XT but wanted to wait a bit after the release to see the reviews. What AMD did with the launch is absolutely atrocious though and I will buy the 4070 instead - I don't want to support a company that shits on their customers like this.", "ups": 3, "depth": 0, "author": "vielokon", "replies": [{"selftext": "I was also looking forward to getting the RX 7800, but rather than waiting for more reviews, I waited until retailers get more stock and start competing on the price, because the first day everything seemed at least 20 \u20ac over MSRP.\n\nBoy, was I wrong. Two and half weeks later, stock is still low and prices even higher. And not only that: the only two retailers that participated in the Starfield bundle promotion in my country no longer offer the Starfield codes on the model I wanted. Boom, $100 of value gone, just like that.\n\nSo now I'm annoyed for waiting so long for nothing, and not really interested in getting the new card at all.\n\nApparently, manufacturers sometimes throttle the supply to create artificial scarcity and keep the prices high. Well, if that's what you are doing, AMD, guess what: you can keep it. I'm already bidding on a second hand RX 6800, which has very similar performance and much lower price.", "ups": 3, "depth": 1, "author": "he29", "replies": [{"selftext": "My reason to wait aside from the reviews was also to see the prices drop a bit. Big mistake - the 7800 TUF is now 610 EUR, while two weeks ago, at launch, it went for 560. Stupid. I guess my 6600XT will have to suffice until some good deals on 4070's appear.", "ups": 2, "depth": 2, "author": "vielokon", "replies": []}, {"selftext": "&gt; Apparently, manufacturers sometimes throttle the supply to create artificial scarcity and keep the prices high.\n\nThey learned from seeing how much profit margin the automakers were making by having scarcity for new cars.  And with only two companies making mid-to-high grade video cards, they can easily get away with it.\n\nWe need two more legitimate competitors to enter the video cards field, IMHO.  Hopefully Intel will continue to improve its product so that they can become the third competitor.", "ups": 1, "depth": 2, "author": "WhippersnapperUT99", "replies": []}]}, {"selftext": "To be fair I think most ppl knew that these were gonna fly off the shelves. The exact reason why I didn't wait and decided to goto my microcenter 30min early, I got the red devil 7800xt and was also the very first at that store to get one of the variants. And if I remember correctly Nvidia cards have sold out many times and scalpers have done the exact same except worse during crypto mining uproar", "ups": 2, "depth": 1, "author": "Aurey2244", "replies": []}, {"selftext": "Moores Law is Dead said his sources said retail will be getting massive shipments of 7700XTs and 7800XTs in the next few weeks. They will be plentiful soon", "ups": 0, "depth": 1, "author": "NunButter", "replies": [{"selftext": "His 'sources' are him making wild guesses of several different incredibly likely outcomes, publishing them, and then usually getting at least one guess right. Except when all of them are bullshit, because he knows nothing about anything.", "ups": 3, "depth": 2, "author": "LongFluffyDragon", "replies": []}, {"selftext": "Looks like i might lose interest on this gen and wait for 8800 instead. Stupid policy from AMD side.", "ups": 2, "depth": 2, "author": "LechHJ", "replies": []}]}, {"selftext": "Why are you blaming AMD for what third-party sellers did?", "ups": 0, "depth": 1, "author": "DuskOfANewAge", "replies": []}]}, {"selftext": "In Austria we have them...", "ups": 2, "depth": 0, "author": "Frajhamster", "replies": []}, {"selftext": "Because is amd dude", "ups": -8, "depth": 0, "author": "DirectorGood9020", "replies": []}, {"selftext": "Stock has been low.", "ups": 1, "depth": 0, "author": "geko95gek", "replies": []}, {"selftext": "Non existing stock in Latinamerica, or it hasn't been released yet.", "ups": 1, "depth": 0, "author": "Middle-Ad-2980", "replies": []}, {"selftext": "I've got mine thursday, Asrock. Waited about 2 weeks.", "ups": 1, "depth": 0, "author": "Yos22", "replies": []}, {"selftext": "Got mine today (sapphire pure edition)", "ups": 1, "depth": 0, "author": "Dradolin", "replies": []}, {"selftext": "We have it in Estonia, the price goes up and down though. When they came we got one for my boyfriend for 600 euros, then when I thought it is time to get one for me it was like 750 euros (xfx merc) now is 620.", "ups": 1, "depth": 0, "author": "Individual_Union_433", "replies": []}]}
{"post": {"title": "23.9.2 makes browsers and videos lag.", "subreddit": "Amd", "selftext": "The previous day i installed 23.9.2 and moving browser windows was laggy at first. Then after a few minutes of playing youtube video the whole system was lagging and stop responding. After pressing WindowsKey+D and after waiting a few seconds for the system to respond i was in Desktop and the system framerate was stable again. For some reason the new drivers made the browsing experience just horrible when browser window was open(i tried this with all browsers and cleaned all system cookies).I reverted to 23.9.1 and all problems solved.\n\nAnyone else experiencing problems with 23.9.2?\n\nSystem: Ryzen 5 2600x, RX6600", "ups": 77, "permalink": "/r/Amd/comments/16pnkie/2392_makes_browsers_and_videos_lag/", "num_comments": 136}, "replies": [{"selftext": "This driver also causing some system interrupt issues for me. After a few hours the OS freezes up and the desktop is like 0.2fps.\n\nAfter I reverted back to 23.9.1 everything is fine.\n\nwin11, ryzen 5900x, rx6900xt", "ups": 29, "depth": 0, "author": "wwfdoink", "replies": [{"selftext": "I reverted too. I'll wait to install the driver after 23.9.2.", "ups": 4, "depth": 1, "author": "sotiroyklas", "replies": []}, {"selftext": "Same", "ups": 5, "depth": 1, "author": "Psycho__Gamer", "replies": []}, {"selftext": "6800XT, same here, even tried to DDU and clean install 23.9.2, but still the issue persists\n\nHad to revert to 23.9.1", "ups": 5, "depth": 1, "author": "squadraRMN", "replies": []}, {"selftext": "same with me, reverted back to 23.9.1 and everything is fine now. Ryzen 5600, rx 6700xt", "ups": 4, "depth": 1, "author": "NICK_GOKU", "replies": []}, {"selftext": "Same on my 6800XT. 23.9.2 intermittently freezes up the whole OS, whilst 23.9.1 works perfectly fine.", "ups": 3, "depth": 1, "author": "VincentKoeman", "replies": []}, {"selftext": "How do you revert to the old driver? Do you just uninstall the AMD software on the windows app list or do I need to look into DDU?\n\nI feel like the problem came from the VSYNC for me. Adrenaline sometimes gives me S.O. FPS (not even NaN, and I can't figure out what S.O. means)... and since I turned VSYNC off, no more freezes, for now...", "ups": 1, "depth": 1, "author": "penywinkle", "replies": [{"selftext": "Just download the previous version and the installer will take of the rest. Download from the official AMD website. DDU is definitely the best way to do it.", "ups": 1, "depth": 2, "author": "NICK_GOKU", "replies": []}]}, {"selftext": "OH MY GOD DUDE. I thought it was my RAM that was causing the issue because I tuned it. \n\nI never thought is gonna be the drivers.", "ups": 1, "depth": 1, "author": "LickingMySistersFeet", "replies": []}]}, {"selftext": "RX 6800XT, all good here (HW acceleration on)", "ups": 23, "depth": 0, "author": "Omegachai", "replies": [{"selftext": "Thought my computer was messed but really started happening since this update. Also have 6800xt", "ups": 5, "depth": 1, "author": "Fun-Contract-2486", "replies": []}, {"selftext": "HW acceleration is back?", "ups": 1, "depth": 1, "author": "FirefighterClear6669", "replies": []}]}, {"selftext": "try disabling MPO. [here's a tool](https://github.com/RedDot-3ND7355/MPO-GPU-FIX) that does that and some others (like disabling ULPS)", "ups": 9, "depth": 0, "author": "TheBloodNinja", "replies": [{"selftext": "It works, thanks !", "ups": 1, "depth": 1, "author": "Lion021", "replies": []}]}, {"selftext": "Mine kept getting a white flickering when streaming, turned out to be a windows problem and had to disable mpo via regedit. All fixed. https://youtu.be/F89Xs_0ak38?si=9Dj6WTxRLTjPtTfR", "ups": 8, "depth": 0, "author": "haribo_2016", "replies": []}, {"selftext": "Red dead redemption 2 is lagging and stuttering very badly after the update. Reverted to 23.9.1 \n\nGPU: 6800XT", "ups": 6, "depth": 0, "author": "sasidhar4033", "replies": [{"selftext": "Lagging even after shader rebuild? Or is it lagging because of shader rebuild?", "ups": 3, "depth": 1, "author": "TomiMan7", "replies": [{"selftext": "Even after the shader rebuild. Just after reverting, it is gone.", "ups": 1, "depth": 2, "author": "sasidhar4033", "replies": []}]}]}, {"selftext": "Rx 6700 XT, i can confirm this problem after update from 23.9.1\n\nMpv player freeeze W10 for a couple of seconds no matter what api or if hardware acceleration is enabled or not", "ups": 5, "depth": 0, "author": "Skoll9", "replies": [{"selftext": "Uninstalled 23.9.2 with DDU and have done clean reinstall of 23.9.1\n\nAnd this problem is gone", "ups": 2, "depth": 1, "author": "Skoll9", "replies": []}, {"selftext": "RX 6700XT here as well. I disabled the AMD Freesynch Premium in Adrenaline and the problem seem to stop.", "ups": 1, "depth": 1, "author": "YagamiYuu", "replies": [{"selftext": "I am on a mere Freesync with sdr display\n\nAnd that feature is actually visible and good compared whatever benefits of .2 driver for RDNA1/RDNA2 is", "ups": 1, "depth": 2, "author": "Skoll9", "replies": []}]}]}, {"selftext": "7900XTX no issues", "ups": 17, "depth": 0, "author": "Buris", "replies": [{"selftext": "Yup no issues with same gpu. I had to restart my computer after tho because of a flickering screen, but never had any problems after that.", "ups": 3, "depth": 1, "author": "iShotTheShariff", "replies": []}, {"selftext": "Ditto.", "ups": 2, "depth": 1, "author": "Opteron170", "replies": []}]}, {"selftext": "Had the same issue yesterday after updating to 23.9.2. All browsers and video playing were hanging my system for good. Just removed drivers from programs and sticking with windows provided for now. \n\np.s. Saw an advice on amd forums - disable the quick sync, but didn't check it.", "ups": 4, "depth": 0, "author": "Timofeuz", "replies": []}, {"selftext": "Same. I will try disabling hardware acceleration, but not promises. Its unbearable right now.\n\nEdit: yup, that fixed it. It was mentioned in update megathread, but it was lower down in the comments.", "ups": 5, "depth": 0, "author": "SadTito", "replies": [{"selftext": "I'm surprised about this thread. Has this ever not been a problem in the last few years?\n\nI'm still on 23.8.2 on a 6700 XT and happened to start the Netflix app for the first time in a year. It's still rubberbanding every few minutes and hanging my entire system every 30-60 minutes. (Audio still plays, black screens, no reactivity to keypresses including keyboard not toggling the caps-lock led when pressed) I disabled hardware acceleration in Chrome years ago due to similar problems which fixed a bunch of problems completely, and don't have problems when using other players.\n\nIt's like every time idle GPU usage is improved, stability when using applications that mildly tax the gpu gets worse.", "ups": 2, "depth": 1, "author": "UQRAX", "replies": [{"selftext": "I'm still on 22.5.1 because that's the only driver that doesn't make my whole PC randomly freeze while playing DOTA 2. All my inputs are locked, and the only solution is to force restart it. *shrug*\n\nDidn't test the newest ones yet, but I prefer to keep it as is for peace of mind. Maybe I'll try some new driver at some point, but I'm already tired of searching for a solution.", "ups": 1, "depth": 2, "author": "Colpus", "replies": []}]}, {"selftext": "Does H.A makes the browser faster and will i lose anything if i disable it?", "ups": 1, "depth": 1, "author": "sotiroyklas", "replies": [{"selftext": "You are just offloading the work to cpu instead of gpu", "ups": 3, "depth": 2, "author": "countpuchi", "replies": []}]}, {"selftext": "Where do you disable it?", "ups": 1, "depth": 1, "author": "Azhrei", "replies": [{"selftext": "in the settings of your browser, discord etc", "ups": 1, "depth": 2, "author": "nwgat", "replies": [{"selftext": "Fixed nothing for me, I've had constant driver crashes when playing a YouTube video on a seperate monitor while a game is running on the main monitor. Very annoying since I only just got the 7800 XT.", "ups": 1, "depth": 3, "author": "Azhrei", "replies": []}]}]}]}, {"selftext": "Yeah, got same problems with system not responding or heavy stuttering (RX 6900 XTU on X670E Board and Ryzen 9 7950) on playing youtube videos with latest Firefox Beta.\nAfter reinstalling again the latest AMD chipset drivers it was much better.", "ups": 3, "depth": 0, "author": "Massder_2021", "replies": []}, {"selftext": "7800xt all good here", "ups": 8, "depth": 0, "author": "feorun5", "replies": [{"selftext": "Same", "ups": 1, "depth": 1, "author": "jayjr1105", "replies": []}]}, {"selftext": "Try [Disable MPO](https://nvidia.custhelp.com/app/answers/detail/a_id/5157).\n\nThis applies to AMD as well as Nvidia.", "ups": 7, "depth": 0, "author": "ruet_ahead", "replies": []}, {"selftext": "For me on 7900XTX I was getting what looked like memory corruption or the like, quick black line like 100-200 pixels wide would flicker across the screen either vertically or horizontally. I downgraded back to 23.9.1 and it's gone.", "ups": 4, "depth": 0, "author": "FeshawHusky", "replies": []}, {"selftext": "Yup, kept causing freezes for me with each click of a mouse. Rolled the drivers back and issues disappeared.", "ups": 2, "depth": 0, "author": "Aluja89", "replies": []}, {"selftext": "Same here glad iam not paranoid i was a bout to get a new mobo", "ups": 2, "depth": 0, "author": "HatchBuggy", "replies": []}, {"selftext": "Did a fresh install and updated. Now I see some white flashes when opening browser tabs. Got some driver failure while trying to connect a tablet screen too", "ups": 2, "depth": 0, "author": "siralmasy", "replies": []}, {"selftext": "Yes", "ups": 2, "depth": 0, "author": "superpwny", "replies": []}, {"selftext": "Same issue on 6700xt", "ups": 2, "depth": 0, "author": "zxch2412", "replies": []}, {"selftext": "Yeah legit lost 40fps in bf2042 since this update 23.9.2 for 7900 XTX", "ups": 2, "depth": 0, "author": "desprrr", "replies": [{"selftext": "How does it feel to play that game ? Aside from this 23.9.2 issue ?\nAll my friends told me to play something else.", "ups": 0, "depth": 1, "author": "ZeinThe44", "replies": []}, {"selftext": "UPDATE: i dont know how but the 23.9.2 lock my fps to an unstable 50-60 FPS in low setting and same in ultra settings[.](https://60.Now) Back to 23.9.1 full ultra 150 FPS.", "ups": 1, "depth": 1, "author": "desprrr", "replies": [{"selftext": "check your adaptive V sync or similar setting in your AMD console. With driver updates AMD decides they will screw up your settings for no reason.", "ups": 1, "depth": 2, "author": "Bhagswag", "replies": []}]}, {"selftext": "Disable metrics, takes 20% gpu utilisation for some reason, you should see around 8W board power in tunning tab.", "ups": 1, "depth": 1, "author": "Crptnx", "replies": []}]}, {"selftext": "I usually just wait a week or 2 before updating drivers. Whether you wanna do the same, up to you. Try disabling MPO if you haven't already.", "ups": 2, "depth": 0, "author": "X_irtz", "replies": [{"selftext": "That's what I'll do from now on . I think the next driver update will fix the issue.", "ups": 1, "depth": 1, "author": "sotiroyklas", "replies": []}]}, {"selftext": "This version was a nightmare on my 6700 XT. First thing I noticed was screen flashing in apps like Outlook (FreeSync issue), also, Edge would just completely hang and make the whole system unresponsive. Reverted back to 23.9.1 and issues are gone.", "ups": 2, "depth": 0, "author": "flox1", "replies": []}, {"selftext": "same, rx 6600 xt", "ups": 2, "depth": 0, "author": "hannymad", "replies": []}, {"selftext": "Rx 7900 xtx here, downgraded to 23.9.1.  new update made warzone unstable, crashed every 10 minutes", "ups": 2, "depth": 0, "author": "revofx99", "replies": []}, {"selftext": "6900 xt and R5 3600 here... no problems with the driver so far...in-game as well as desktop", "ups": 2, "depth": 0, "author": "Dynamitrios", "replies": [{"selftext": "Till 2 days ago I had exactly the same setup. Personally i had a massive bottleneck...You must be Greek as well or maybe i am mistaken. In that case i am sorry.", "ups": 1, "depth": 1, "author": "Leocodone", "replies": [{"selftext": "\u0395\u03af\u03bc\u03b1\u03b9", "ups": 2, "depth": 2, "author": "Dynamitrios", "replies": [{"selftext": "\u039a\u03b1\u03bb\u03ac \u03c4\u03bf \u03ba\u03b1\u03c4\u03ac\u03bb\u03b1\u03b2\u03b1. \u0388\u03bb\u03b1 \u03c0\u03b1\u03c4\u03c1\u03af\u03b4\u03b1!!! \u0395\u03bc\u03ad\u03bd\u03b1 \u03bc\u03bf\u03c5 \u03ba\u03bf\u03bb\u03bb\u03ac\u03b5\u03b9 \u03c3\u03c4\u03bf\u03c5\u03c2 browsers \u03ba\u03b1\u03b9 \u03b4\u03b5\u03bd \u03bc\u03c0\u03bf\u03c1\u03ce \u03bd\u03b1 \u03ba\u03ac\u03bd\u03c9 \u03c4\u03af\u03c0\u03bf\u03c4\u03b1 \u03b1\u03bb\u03bb\u03ac \u03c0\u03b9\u03bf \u03c0\u03bf\u03bb\u03cd \u03c3\u03c4\u03bf opera. \u0395\u03ba\u03b5\u03af... \u03ac\u03c3\u03c4\u03b1 \u03bd\u03b1 \u03c0\u03ac\u03bd\u03b5!!! \u0391\u03bb\u03bb\u03ac \u03c0\u03b1\u03c1\u03b1\u03c4\u03ae\u03c1\u03b7\u03c3\u03b1 \u03cc\u03c4\u03b9 \u03c4\u03b1 Windows 11 \u03ad\u03c7\u03bf\u03c5\u03bd \u03bc\u03b5\u03b3\u03b1\u03bb\u03cd\u03c4\u03b5\u03c1\u03bf \u03b8\u03ad\u03bc\u03b1...", "ups": 1, "depth": 3, "author": "Leocodone", "replies": [{"selftext": "\u0393\u03b9'\u03b1\u03c5\u03c4\u03cc \u03ba\u03b1\u03b9 \u03c0\u03b1\u03c1\u03b1\u03bc\u03ad\u03bd\u03c9 \u03b1\u03ba\u03cc\u03bc\u03b1 \u03c3\u03c4\u03bf W10 \ud83d\ude42", "ups": 2, "depth": 4, "author": "Dynamitrios", "replies": [{"selftext": "\u039a\u03b1\u03b9 \u03c0\u03bf\u03bb\u03cd \u03ba\u03b1\u03bb\u03ac \u03ba\u03ac\u03bd\u03b5\u03b9\u03c2. \u0395\u03b3\u03ce \u03ac\u03bb\u03bb\u03b1\u03be\u03b1 mobo \u03ba\u03b1\u03b9 \u03bc\u03b5\u03c4\u03ac \u03b1\u03c0\u03cc \u03bc\u03b9\u03b1 \u03b1\u03bd\u03b1\u03b2\u03ac\u03b8\u03bc\u03b9\u03c3\u03b7 \u03b1\u03c3\u03c6\u03b1\u03bb\u03b5\u03af\u03b1\u03c2 \u03c3\u03c4\u03b1 10 \u03bc\u03b5 \u03c0\u03ae\u03b3\u03b5 \u03c3\u03c4\u03b1 11 \u03c7\u03c9\u03c1\u03af\u03c2 \u03bd\u03b1 \u03bc\u03bf\u03c5 \u03b4\u03ce\u03c3\u03b5\u03b9 \u03b5\u03c0\u03b9\u03bb\u03bf\u03b3\u03ae \u03bc\u03b7\u03bd \u03c3\u03bf\u03c5 \u03c0\u03c9. 10\u03c1\u03b9\u03b1 \u03ba\u03b1\u03b9 \u03be\u03b5\u03c1\u03cc \u03c8\u03c9\u03bc\u03af ;)", "ups": 2, "depth": 5, "author": "Leocodone", "replies": []}]}]}]}]}]}, {"selftext": "Same problem, installing 23.9.1 fixes it..\n\n6900xt", "ups": 2, "depth": 0, "author": "NiceWeaknee", "replies": []}, {"selftext": "This New drivers making my computer crashed with kernel 41 I go back to 22.5.1 and everything is fine. I have a 5700xt and never had I ever had issues with it.", "ups": 2, "depth": 0, "author": "Brave-Audience1078", "replies": []}, {"selftext": "RX 6800 and I have similar issues as you. I also get massive freeze frame semi frequently when I tab in and out of games.", "ups": 2, "depth": 0, "author": "Niaoru", "replies": []}, {"selftext": "I just got an rx 6800 and I was having these issues when even opening my web browser or trying to watch a stream on discord. I disabled hardware acceleration on both and now they work fine. I'm just gonna downgrade and wait for 23.9.3", "ups": 2, "depth": 0, "author": "MilkGamingChannel", "replies": []}, {"selftext": "This problem has existed for 5 months.\n\nThe first review was left on May 5th.\n\nhttps://answers.microsoft.com/zh-hans/microsoftedge/forum/all/microsoft/094485e4-235b-4b92-9816-e7a3a9f36ade\n\nThen June 27th. I wrote about this problem on the guru3d forum.\n\nhttps://forums.guru3d.com/threads/amd-software-adrenalin-edition-23-10-01-41-add-vulkan-extensions-driver-download-and-discussion.448453/#post-6144702\n\nI have just one question: what did the vanguard team do for five months?", "ups": 3, "depth": 0, "author": "buzarini", "replies": []}, {"selftext": "Same exact problem with me, when gaming everything is perfect but when browsing or using apps like Netflix, VLC Player etc... there is a chance that clicking anything will cause my system to hang or slowdown for a couple of seconds, even just opening a new tab in Opera GX will cause it to happen. Using 23.9.2\n\nTried everything too, DDU clean install (twice even), Clear Browser Cache and reinstall browser, was close to reinstalling Windows too lol. Will revert back to previous driver and hopefully it fixes it.  \n\nRyzen 7 5700G | RX6600XT", "ups": 2, "depth": 0, "author": "AnriRB26", "replies": []}, {"selftext": "Same issue with RX6800 so best to avoid 23.9.2 if on RX6000 series.  Rolled back to 23.8.2 stable.", "ups": 5, "depth": 0, "author": "mi7chy", "replies": []}, {"selftext": "Version 23.9.2 is based on branch 23.20. The first test UWP version from branch 23.20 was released on May 5th. And there they discovered a problem with friezes due to the browser, but AMD ignored this problem. 5 months have passed and they still haven't fixed the error.\n\nhttps://answers.microsoft.com/zh-hans/microsoftedge/forum/all/microsoft/094485e4-235b-4b92-9816-e7a3a9f36ade", "ups": 3, "depth": 0, "author": "buzarini", "replies": []}, {"selftext": "same here, 6800 xt", "ups": 2, "depth": 0, "author": "Radeuz", "replies": [{"selftext": "rolled back to 23.9.1, everything ok", "ups": 1, "depth": 1, "author": "Radeuz", "replies": [{"selftext": "how do you roll back? is there a function in Adrenalin ?", "ups": 0, "depth": 2, "author": "JamesEdward34", "replies": [{"selftext": "Download driver, uninstall current driver, install old driver.", "ups": 1, "depth": 3, "author": "Jacket_22", "replies": [{"selftext": "Oh ok jusy basic DDU stuff then.", "ups": 0, "depth": 4, "author": "JamesEdward34", "replies": []}]}, {"selftext": "Uninstall with DDU, , then install the older driver?", "ups": 1, "depth": 3, "author": "Messenjah1", "replies": []}]}]}]}, {"selftext": "Love that my 7900xtx idle at 100 watts still.", "ups": 2, "depth": 0, "author": "MrEpic23", "replies": [{"selftext": "Enable freesync. My 7900xtx idles at 6w lol", "ups": 5, "depth": 1, "author": "Nord5555", "replies": [{"selftext": "That depends on the monitor setup, my SO XT idles at 90+w still too.   \n\n\nAnd VR isnt fixed either if u headset connects as secondary display (we have Pimax 8k and Reverb g2). \n\n&amp;#x200B;\n\n6000 series drivers were superior to this garbage.", "ups": 1, "depth": 2, "author": "Keldonv7", "replies": [{"selftext": "I use oculus quest 2 connectet to pc and a g7 1440p 240hz with freesync premium pro. \n\nMy 6900xt without freesync enabled was 84w idle. With freesync enabled 22w. \nMy 7900xtx 6w idle. Really dont have any issues \ud83e\udd37\u200d\u2642\ufe0f", "ups": 3, "depth": 3, "author": "Nord5555", "replies": [{"selftext": "Plenty of people dont, but that dosent mean some people may be less lucky. Anecdotal evidence is only evidence of anecdote.\n\nShes running kvm switch between macbook pro and desktop (but it dosent change anything, we tried it without kvm switch). 1440p 170hz panel + 1080p 60hz panel. 90+w idle. If we drop 1440p panel to 60hz idle drops to 14w\\~. Its just still not fixed, memory clocks often go full tilt for no reason in idle. Freesync dosent affect that in our case at all.", "ups": 1, "depth": 4, "author": "Keldonv7", "replies": [{"selftext": "Hmm maybe the panel aint able to drop refreshrate low enough even with freesync enabled ?  Both my brother and me runs 1440p 240hz g7 hes on 6900xt and idles at 16w where im at 6w. My Old 6900xt was at 22w.", "ups": 1, "depth": 5, "author": "Nord5555", "replies": [{"selftext": "It should just drop gpu mem clocks, i personally use 4080 (playing flightsims in VR so kinda forced to go green cards) and my same panel as my SO (Gigabyte M27Q) dosent drop refreshrate (u can easily test it by moving window between 170hz panel and 60hz one)  \n\n\n[https://imgur.com/a/GFEGCRO](https://imgur.com/a/GFEGCRO)  \n\n\nThat my idle draw now with youtube playing, discord open, slack open.   \nIt entirely possible to have saw draw on AMD gpu (had 6900xt before), it just depends on your monitor setup sadly, so kinda a gamble.", "ups": 1, "depth": 6, "author": "Keldonv7", "replies": [{"selftext": "But before enable freesync vram was stucked at full speed. Enabled it drops full Down for me", "ups": 1, "depth": 7, "author": "Nord5555", "replies": [{"selftext": "If you have two monitors the main monitor can be a high refresh rate monitor it doesn\u2019t matter anymore. The secondary monitor must be 60hz or your idle will be around 95-100watts. I have two 1440p; 170hz and 144hz monitors and need both refresh rates high. Both monitors have free sync enabled. It\u2019s a shame amd can\u2019t fix their most expensive/best gpus.", "ups": 1, "depth": 8, "author": "MrEpic23", "replies": [{"selftext": "Oh Well. If 2 gaming monitors like 50-100w each aint a problem why is 90w from the gpu such a problem then.. not that i actually measures much from having freesync on vs off. In terms of more electricity Bill. Ur gonna use Way more then 100w when gaming so its just when ur idling. Just make the pc go asleep when your away more then 10min then \ud83e\udd37\u200d\u2642\ufe0f", "ups": 1, "depth": 9, "author": "Nord5555", "replies": []}]}]}]}]}]}]}]}]}]}, {"selftext": "6700XT here. I thought it was just me, apparently for some reason it seems I practically solved it when I disabled ULPS through MSI Afterburner.", "ups": 1, "depth": 0, "author": "yendorion", "replies": []}, {"selftext": "Using igpu 5600g no problems\ud83d\ude02", "ups": 1, "depth": 0, "author": "MasterBlaster4949", "replies": []}, {"selftext": "Nothing with a 6800xt and a 7900xtx.", "ups": 1, "depth": 0, "author": "MrJimmytheweed", "replies": []}, {"selftext": "6800XT, all good, even got better performance in Plague Tale Requiem. I always DDU before updating too.", "ups": 1, "depth": 0, "author": "psykofreak87", "replies": []}, {"selftext": "6700 XT here, I'm not having any lag issues so far.", "ups": 1, "depth": 0, "author": "popop143", "replies": []}, {"selftext": "No lag here, 7900xt", "ups": 1, "depth": 0, "author": "THEPIGWHODIDIT", "replies": []}, {"selftext": "Did an in-place upgrade from 23.9.1 to 23.9.2\n\nPlayed a few games, watched some YouTube (hardware acceleration enabled, default ANGLE backend), fired up a blu-ray rip in VLC.\n\nAlso played around with Stable Diffusion for an hour or so (using DirectML onnxruntime) and noticed an improvement in performance from 2.83it/s to 2.9-3.0it/s with the 23.9.2 driver. Neat.\n\nNo apparent issues on my system with the driver, seems fine.", "ups": 1, "depth": 0, "author": "Rockstonicko", "replies": []}, {"selftext": "Getting stutters and weird stuff happening with a 7900XTX in rainbow six that absolutely wasn\u2019t happening before, this version is chalked, it doesn\u2019t feel like a stable build of a driver it feels the way an early iOS beta feels in hand", "ups": 0, "depth": 0, "author": "MorePropaganda", "replies": [{"selftext": "Flashing screen in apps like Outlook with FreeSync enabled and Edge would just freeze and make the whole system unresponsive (Ctrl+Alt+Del took over a minute to react o.O ) on 6700 XT. Reverted back to 23.9.1 (DDU uninstallation) and all issues gone.", "ups": 1, "depth": 1, "author": "flox1", "replies": []}]}, {"selftext": "Why they created different branch for Vega and Polarois?", "ups": 0, "depth": 0, "author": "rocketstopya", "replies": []}, {"selftext": "I had only played starfield with the new drivers, but the GPU seemed to be running hotter then the previous driver and I saw some graphic corruption a few times, at stock clocks.", "ups": 1, "depth": 0, "author": "zeus1911", "replies": []}, {"selftext": "6800XT here, no issues. On firefox. I dont watch videos on my PC so I cant comment on that.", "ups": 1, "depth": 0, "author": "JamesEdward34", "replies": []}, {"selftext": "6650XT here, no issues.", "ups": 1, "depth": 0, "author": "ihearturtits", "replies": []}, {"selftext": "5600G and 7900xtx no issue", "ups": 1, "depth": 0, "author": "CamelDismal6029", "replies": []}, {"selftext": "Frame rate is slow in mpc-hc even tho reported normally after opening browser. Enable n disable auto HDR fixes the issue, idk it's counted as reload the driver.", "ups": 1, "depth": 0, "author": "snorlaxgangs", "replies": []}, {"selftext": "12400F, RX6600, no problem here", "ups": 1, "depth": 0, "author": "solclearsky", "replies": []}, {"selftext": "exact same issue but cpu is a regular 2600. disabling hardware acceleration fixes some freezes", "ups": 1, "depth": 0, "author": "ThreeFourTwo", "replies": []}, {"selftext": "Have an R5 5600 and RX6700xt, no issues", "ups": 1, "depth": 0, "author": "John9023", "replies": []}, {"selftext": "Same here on an RX 580, updated driver through the Radeon Settings updater.\n\nBrowser: Firefox.", "ups": 1, "depth": 0, "author": "ZacXme", "replies": []}, {"selftext": "my vega 56 dosnt like either of the september drivers, i cant even change the frequency at all or it blue screens instantly after hitting apply. \n\nAnd i havnt been able to see the memory frequency values in about 6 months, they just get cut off when i select advanced control so its all guess work.\n\nWhat does work is importing settings from an already created profile though so any tuning i have to manually type it into an xml file\n\njust gonna roll back to the last driver of 2022\n\nEdit: can confirm 22.11.2 allows me to see memory frequency values again will test if it BSOD if i change gpu frequency later, i need sleep\n\nI can change the gpu frequency again without a BSOD so it was the drivers lmao", "ups": 1, "depth": 0, "author": "MundaneAlchs", "replies": []}, {"selftext": "haven't done much with it yet, but I always do a 3dmark bench when updating driver, lost 8% in comparison to 23.7.2\n\nforgot to add that I'm on 7900XT", "ups": 1, "depth": 0, "author": "ArtsM", "replies": []}, {"selftext": "Just tried 23.9.2 on my 7900XTX for a bit, simple game like league somehow \"locked\" at 80 or 90fps with 144fps settings and also video players on background freeze when tab in fullscreened games.  \n\n\nI am going back to 23.8.2 for now", "ups": 1, "depth": 0, "author": "Rootorii", "replies": []}, {"selftext": "No issues my end.", "ups": 1, "depth": 0, "author": "ATOJAR", "replies": []}, {"selftext": "I have similar issues I am on 6800 xt and 5800x3d", "ups": 1, "depth": 0, "author": "Expaw", "replies": []}, {"selftext": "6800 XT 5800X3D. This is a temporary workaround but if you're still on 23.9.2 and don't want to rollback, you can simply press windows key+ctrl+shift+B and it will restart the graphics driver. This fixes the lag for the current sesion but you'll need to do it once it starts acting up again (ie after waking pc from sleep or a system reboot). Hopefully there is a permanent hotfix soon.", "ups": 1, "depth": 0, "author": "rockycrab", "replies": []}, {"selftext": "Chrome edge or Firefox ? or all of the, ?", "ups": 1, "depth": 0, "author": "Melodias3", "replies": []}, {"selftext": "I've seen this issue only once. When i went to the youtube feed's page and hovered my mouse on a thumbnail to make the video start in it as a muted preview, it happened.\n\nBut again, only once. I was not able to reproduce it anymore after a windows restart. Weird indeed. Also it was only happening on firefox and not on chrome.", "ups": 1, "depth": 0, "author": "David0ne86", "replies": []}, {"selftext": "RDNA 3 here. Nothing of the sort !\n\nDDU in Safemode and then reinstall the driver. Also check your chipset drivers to make sure you are running the latest version.", "ups": 1, "depth": 0, "author": "ZeinThe44", "replies": []}, {"selftext": "Same issues here, I noticed it was dropping my vram clocks to 0 Mhz, turned off VRR and now works flawlessly", "ups": 1, "depth": 0, "author": "Sufficient_Employ_85", "replies": []}, {"selftext": "Mine was doing the same thing. I noticed that my SAM was disabled. Reinabled it and problem went away.", "ups": 1, "depth": 0, "author": "banajawaa", "replies": []}, {"selftext": "Same on my RX 6700 XT, the lag happens in Reddit video most often.", "ups": 1, "depth": 0, "author": "Psycho__Gamer", "replies": []}, {"selftext": "Hell, Im still on 23.7.1 drivers.  Meh....\n\nEdit-7900xtx", "ups": 1, "depth": 0, "author": "Koyote7676", "replies": []}, {"selftext": "23.9.2 caused MW2 on my 7900XT to crash after a few mins, downgraded back to 23.9.1 and everything's fine again.", "ups": 1, "depth": 0, "author": "Chanzy7", "replies": []}, {"selftext": "I am glad I looked on the subreddit before I decided to go ahead an update, I am gonna refrain from doing so until a more stable driver drops.", "ups": 1, "depth": 0, "author": "Twinkalicious", "replies": []}, {"selftext": "It flashes black screen for a second in browsing", "ups": 1, "depth": 0, "author": "Proper-Recipe-9169", "replies": []}, {"selftext": "I haven't experienced this exact problem, but when I'm running heavy Stable Diffusion renders &amp; maxing out VRAM, the entire frame rate of primary display will often take a dump and get a bit jumpy and laggy.  Haven't experienced that before and it seemed kinna strange.\n\n  Windows 10, 5800X, 7900XTX.", "ups": 1, "depth": 0, "author": "WubWubSleeze", "replies": []}, {"selftext": "5700 xt there, 23.9.2 seems stable for me", "ups": 1, "depth": 0, "author": "Krendrian", "replies": []}, {"selftext": "7900xtx and no issues with this driver at all. Had some stuttering / performance issues with 23.9.1 but this driver seems solid. I disable ULPS and MPO after every driver install though.", "ups": 1, "depth": 0, "author": "SlickShoesS", "replies": []}, {"selftext": "I am having the same exact problem.\n\nSame driver version: 23.9.2.\n\nRX6600XT and Ryzen 5 2600 \n\nI will try getting the previous drivers and downgrade and come back with results if that fixes it. \n\nCheers!", "ups": 1, "depth": 0, "author": "Jumpy_Ad_9206", "replies": [{"selftext": "Well well, reverted back to 23.9.1 and all is looking good know. \n\nInitially toggling between HDR enabled and disabled worked for me, but after couple mins it would start again. \n\nSo far downgrading to 23.9.1 is the steady solution, at least for me.", "ups": 1, "depth": 1, "author": "Jumpy_Ad_9206", "replies": []}]}, {"selftext": "Mine too until I turned off SAM.", "ups": 1, "depth": 0, "author": "mileskg21", "replies": []}, {"selftext": "Reading from the comments, it seems this is a problem that happen while watching a video or using your browser, related  just to 6xxx series cards, hope they will fix this in the next release", "ups": 1, "depth": 0, "author": "squadraRMN", "replies": []}, {"selftext": "Yep, same issue very annoying i need to turn off and on my monitor to fix the lockup, reinstalled windows no change, well the meme of amd drivers is really true lmao.", "ups": 1, "depth": 0, "author": "protonCocks", "replies": []}, {"selftext": "try to disable surface optimisation in global, that helps me when i had problems with 9.1 driver", "ups": 1, "depth": 0, "author": "Select_Truck3257", "replies": []}, {"selftext": "Had to go back to 23.9.1 my cpu usage was constantly 80 to 100% when gaming on i5 12400 and 6800xt", "ups": 1, "depth": 0, "author": "angdiddy67", "replies": []}, {"selftext": "Same thing here, just started searching for it.  Brave browser, but I've also had it happen in Starfield (admittedly while a youtube video was playing in the background) and Discord.", "ups": 1, "depth": 0, "author": "mcantrell", "replies": []}, {"selftext": "Same here 23.9.2 Caused Freezing, even tried on a Fresh Install, same outcome.... Downgraded Driver, good to go. \n\nRyzen 9 5900x\nAsus Tuf x570 Plus Wifi\n64GB DDR4 @ 3600MT/s\nMSI RX 6800xt\nAll NVMe Storgage", "ups": 1, "depth": 0, "author": "jholbrook514", "replies": []}, {"selftext": "That's something I hated on my 6800. Since moving to Nvidia I don't notice it anymore.", "ups": 1, "depth": 0, "author": "JAW_GE0777", "replies": []}, {"selftext": "If I run adrenaline at the same time with games it often causing my whole system to crash and reboot. 7800XT. But it was happening before the update as well. I just stopped opening it and have no issues.", "ups": 1, "depth": 0, "author": "cosmoinstant", "replies": []}, {"selftext": "Same problem on my 6900 XT nitro.\n\nStutter also while playing Starfield. Specially while switching to the scanner.\n\nI reinstalled 23.9.1 now. Problem is gone.", "ups": 1, "depth": 0, "author": "riOrizOr88", "replies": []}, {"selftext": "The same issue and similar ones have happened with previous driver releases on both my current 6700xt and previous 5700xt.  The answer in both cases has been to disable multipanel overlays.\n\nHere's a link to an nVidia support page with a registry edit to do so: [https://nvidia.custhelp.com/app/answers/detail/a\\_id/5157/\\~/after-updating-to-nvidia-game-ready-driver-461.09-or-newer%2C-some-desktop-apps](https://nvidia.custhelp.com/app/answers/detail/a_id/5157/~/after-updating-to-nvidia-game-ready-driver-461.09-or-newer%2C-some-desktop-apps)\n\nLinks to previous reddit posts addressing the issue: [https://www.reddit.com/r/Amd/comments/yvyqc7/disabling\\_multiplane\\_overlay\\_mpo\\_fixed\\_all/](https://www.reddit.com/r/Amd/comments/yvyqc7/disabling_multiplane_overlay_mpo_fixed_all/)\n\n[https://www.reddit.com/r/nvidia/comments/qffxcz/mpo\\_multiplaneoverlays\\_are\\_amazing\\_you\\_can\\_play/](https://www.reddit.com/r/nvidia/comments/qffxcz/mpo_multiplaneoverlays_are_amazing_you_can_play/)\n\nFYI - While a separate issue, disabling any in-game overlays like steam overlay and AMD Adrenalin overlay can often help in specific games as well.\n\nDisabling Steam Overlay  - [https://help.steampowered.com/en/faqs/view/3978-072C-18DF-FBF9](https://help.steampowered.com/en/faqs/view/3978-072C-18DF-FBF9)\n\nDisabling GOG Overlay - [https://docs.gog.com/gc-overlay/](https://docs.gog.com/gc-overlay/)\n\nDisabling Epic Game Store Overlay - [https://www.reddit.com/r/EpicGamesPC/comments/robfot/disabling\\_epic\\_games\\_social\\_panel\\_overlay/](https://www.reddit.com/r/EpicGamesPC/comments/robfot/disabling_epic_games_social_panel_overlay/)\n\nDisabling AMD Adrenalin Overlay - [https://www.amd.com/en/support/kb/faq/dh2-026](https://www.amd.com/en/support/kb/faq/dh2-026)\n\nHope this can help!", "ups": 1, "depth": 0, "author": "vh1atomicpunk5150", "replies": []}]}
{"post": {"title": "Should I undervolt my new 5800x3D using PBO2 or in ASUS ROG STRIX BIOS?", "subreddit": "Amd", "selftext": "Just bought a new Ryzen 7 5800x3D and wondering if I should use PBO2 to undervolt or the BIOS. Just a little worried about temps.", "ups": 13, "permalink": "/r/Amd/comments/16pv8ax/should_i_undervolt_my_new_5800x3d_using_pbo2_or/", "num_comments": 32}, "replies": [{"selftext": "If you can do it through bios I would suggest that.", "ups": 16, "depth": 0, "author": "Obvious_Drive_1506", "replies": [{"selftext": "Would I set the CPU offset to negative and then 0.05 like everyone says to be safe?", "ups": 2, "depth": 1, "author": "Weskerspp", "replies": [{"selftext": "You should see if you have a bios update. Most manufacturers have curve optimized enabled in bios, I would use that over regular voltage offset. Most people can just slap -25 all core on it in curve optimizer", "ups": 3, "depth": 2, "author": "Obvious_Drive_1506", "replies": []}, {"selftext": "Thats what I am doing to mine.  -0.05V offset. Some time ago I tried going further but if I remember correct I started seeing performance regression.\n\nEdit; I just did some quick tests in CB 24 comparing \"-0,05V undervolt\" to \"-30 all core curve optimizer\"\n\nThe undervolt has a lower power consumption (by about 7W) and temperature but also ran about 225 Mhz lower on average. \n\nThe curve optimizer result was about 5% better.", "ups": 2, "depth": 2, "author": "PantZerman85", "replies": []}]}]}, {"selftext": "i just did the bios -30 setting kombostrike thing, if its not stable go down to -20. also get a nice cooler like assasin", "ups": 4, "depth": 0, "author": "AppropriateMethod793", "replies": []}, {"selftext": "My 5800x3d runs cpu negative offset at 0.0500mv and pbo2 Co at -20 and bclk 102.5 (4560allcore) allday Long with Max 78 temp @ r23 30min test. Gameplay 60c", "ups": 2, "depth": 0, "author": "Nord5555", "replies": [{"selftext": "So you used both your BIOS and PBO2?", "ups": 1, "depth": 1, "author": "Weskerspp", "replies": [{"selftext": "Its in bios both of Them \ud83d\udc4c", "ups": 1, "depth": 2, "author": "Nord5555", "replies": [{"selftext": "Okay and so cpu offset voltage -0.05 or +0.05? And pbo2 and bclk where would those be? Ai tweaker? Sorry still new to undervolting. And what mobo are u using? Maybe it's a little easier to manage that", "ups": 1, "depth": 3, "author": "Weskerspp", "replies": [{"selftext": "I use msi b550 gaming edge wifi\nCpu negative offset -0.0500mv. In aitweaker u look after curve optimizer and put allcore -20. Should yield some good gains. If i want you Can slowly try increase bclk (base clockspeed) from 100 and up. Your cpu runs 44.5 multiplier so 100x44.5 equals 4450 allcore (Stock) different boards etc dont act all the same. So you might only be able to do 101x44,5 (4500 allcore) or you might be lucky to go all the Way up to 103.4x44,5 (4600allcore)\n\nI had to settle for 102.5x44,5 (4560 allcore) higher made unstability even with Stock voltage", "ups": 1, "depth": 4, "author": "Nord5555", "replies": []}]}]}]}]}, {"selftext": "Just do -30 all core and move on. No need to mess with voltage offsets", "ups": 5, "depth": 0, "author": "wewerecreaturres", "replies": []}, {"selftext": "[https://youtu.be/BOdolaIDADk](https://youtu.be/BOdolaIDADk)\n\ni copied this guys settings. been stable so far, several days.", "ups": 1, "depth": 0, "author": "lastxman", "replies": []}, {"selftext": "Do it in the bios please. It's very efficient for gaming. Mine never goes above 70w(max load) under gaming . Mostly ranges below 60w . I had set it to 1.025v and temps under 65c", "ups": 0, "depth": 0, "author": "GhostDNAs", "replies": [{"selftext": "Okay thanks, in the bios there is a Offset mode sign which I made nagtive, but for the cpu offset voltage what do I set that to?", "ups": 1, "depth": 1, "author": "Weskerspp", "replies": []}]}, {"selftext": "I do it in the BIOS.", "ups": 0, "depth": 0, "author": "ATOJAR", "replies": []}, {"selftext": "If your able, always BIOS.", "ups": 0, "depth": 0, "author": "SefDiHar", "replies": []}, {"selftext": "I advise you set Curve optimiser to negative 30 straight up. You can also put a thermal limit, which is what I have set on mine to 69C - nice!! \u263a\ufe0f", "ups": -3, "depth": 0, "author": "geko95gek", "replies": [{"selftext": "why that low when this CPU can go to 80 without throttling?", "ups": 1, "depth": 1, "author": "Prychacz", "replies": [{"selftext": "Yep I don't restrain my 7800x3d although I'm on -30 pbo and a 360m aio temps are great.", "ups": 2, "depth": 2, "author": "DiGzY_AU", "replies": []}]}, {"selftext": "As already mentioned there is no reason to limit it to 69 degrees if anything you're limiting the power of the chip quite severely. With a d15, good airflow I'm consistently seeing 70-72 in games, albeit hard on the CPU games, and can jump to 75-76 momentarily. If I had it limited to 69 that is probably a good 5-10% lower performance because of a random temp limit. Especially bad for peaks which is where you want the power. \n\nThis doesn't matter for most games as we mostly are GPU limited, but in starfield for an example this would degrade the FPS. And you're not gaining anything from it.", "ups": 1, "depth": 1, "author": "MrPapis", "replies": [{"selftext": "I quite understand the thermal limit option.\n\nI moved my PC to another room which is way hotter. \n\nThe 5800x3d is cooled by Dark Rock 4 (non-pro).\n\nWithout thermal limit I reach \\~85 easily, even with PBO -30 on all cores.\n\nI limited mine to \\~75c, I might lose performance but PC case is not feeling that hot now.", "ups": 1, "depth": 2, "author": "kepler2", "replies": [{"selftext": "The 5800x3d is incredibly efficient when gaming just because you're limiting the maximum temps might only save you 10-20W which is completely irrelevant in regards to temperature in the case. \n\nBut that CPU cooler isnt great for the CPU. It really does need a massive dual tower for maximum performance. Even then it needs undervolt.", "ups": 2, "depth": 3, "author": "MrPapis", "replies": [{"selftext": "I know but at the time I bought the CPU, I also bought an Arctic 280mm AIO but it didn't fit the case, so I got stuck with this.\n\n\\&gt; might only save you 10-20W\n\nI don't really care that much about W but about the temperatures. The cooler really struggles with this CPU already so limit the temp I guess is the only option for the moment.", "ups": 1, "depth": 4, "author": "kepler2", "replies": []}]}, {"selftext": "Exactly. I don't like heat, I don't like noise either. With my limits I get rid of both of those for the most part.\n\nI also loathe high energy consumption, so I wanted my 5800X3D to be as efficient as a 5600X and now it is!!\n\nImportant part is my 3D cache still works fine.", "ups": 2, "depth": 3, "author": "geko95gek", "replies": [{"selftext": "Yeah In the same boat but once you underclock using thermal limit you are basically limiting the CPU.\n\nBut as long as you get the performance you want, it's ok.", "ups": 1, "depth": 4, "author": "kepler2", "replies": [{"selftext": "Yes my performance isn't influenced at all.", "ups": 1, "depth": 5, "author": "geko95gek", "replies": []}]}]}]}]}, {"selftext": "Yep set it at 80", "ups": 1, "depth": 1, "author": "RedhawkAs", "replies": []}]}, {"selftext": "![gif](emote|free_emotes_pack|grimacing)", "ups": 1, "depth": 0, "author": "Ok-Visual7890", "replies": []}, {"selftext": "Oh yeah definitely. It'll help with both temp and all core boost clocks. Just enable pbo and under curve optimizer (co) set all core to - 30. Try for couple of hours/days if not stable drop it by 2 until its stable.", "ups": 1, "depth": 0, "author": "AciVici", "replies": [{"selftext": "What did you use? Did you set it in BIOS or in PBO2? And was it easy to do if you used your mobo?", "ups": 1, "depth": 1, "author": "Weskerspp", "replies": [{"selftext": "Ooo definelty do that in bios. Don't use any software. They're not consistent as bios and they can conflict and give all kinda weird issues.", "ups": 1, "depth": 2, "author": "AciVici", "replies": []}]}]}, {"selftext": "wut?\n\nPBO (is) in the BIOS.", "ups": 1, "depth": 0, "author": "Practical-Canary8782", "replies": []}]}
{"post": {"title": "My first AMD setup", "subreddit": "Amd", "selftext": "After 6 years I decided to change my Ryzen 5 1600X and my 1060, aiming for a full amd setup in order to take advantage of the free-sync of my monitors. I opted for the following components:\n\n- Ryzen 7 7700X\n- RX 7800XT\n- 2x16 DDR5 5200MHz with AMD Expo\n- ROG Strix B650E-F\n\nI am still in the assembly phase, but I am here to ask you for some advice on how to make the most of this build. Thank you!", "ups": 20, "permalink": "/r/Amd/comments/16pkf0a/my_first_amd_setup/", "num_comments": 47}, "replies": [{"selftext": "My sincere advice to you is to build the system, get it running and enjoy it. That's it. Don't fiddle with the BIOS, don't check temps, don't check frame rates. Just play. 3-6 months from now if you want to goof around then do it.", "ups": 8, "depth": 0, "author": "ruet_ahead", "replies": [{"selftext": "I am worried about temps, maybe I have not to", "ups": 3, "depth": 1, "author": "niccoborgio", "replies": [{"selftext": "\"Don't check temps\" probably should have read as don't obsess over temps. Get an initial reading to see if temps are within spec and have a blast after that.", "ups": 11, "depth": 2, "author": "ruet_ahead", "replies": [{"selftext": "Oki, I will try to not be obsessed", "ups": 3, "depth": 3, "author": "niccoborgio", "replies": [{"selftext": "Yeah 7700x is a hot cpu", "ups": 3, "depth": 4, "author": "DamagedJefff", "replies": [{"selftext": "So I should return it and take 7700?", "ups": 1, "depth": 5, "author": "niccoborgio", "replies": [{"selftext": "no, any am5 cpu is gonna be a bit hot", "ups": 2, "depth": 6, "author": "DamagedJefff", "replies": []}]}]}]}]}]}, {"selftext": "But power consumption adds up to your electricity bills. I do check my temps via Msi Afterburner + Rivaturner. A habit I've formed since I got into PC gaming in Dec of 2014. I'm still rocking the i7 4790k with a MSI RTX 3060 Ventus 3X 12G OC edition.", "ups": 2, "depth": 1, "author": "zulu970", "replies": []}]}, {"selftext": "Shouldn't FreeSync work with geforce as well? Even G-Sync works with amd cards now, no? \n\nHave fun with your new build!", "ups": 3, "depth": 0, "author": "Gameboy_One", "replies": []}, {"selftext": "I have the 7700X, Micro Center Bundle w/ MSI B650-P Wifi, 32GB GSkill 6000 cl 36 Ram, bought about 3 months ago.  \nYou will probably have to Update your BIOS, due to the possibility of CPU meltdown due to SoC Voltage.  \nMy 1st Bios flash was to cure that SoC Issue. I then flashed it 3 more times, mainly to cure Ram instabilities. I could not get Expo to run until the 3rd flash, the latest Bios v17 is pretty good, everything runs fine including Expo.  \nI prefer silent PC's (FD Define R5 Case)(Asus-Dual 4070), so I set my 7700X at 85c Package Thermal Throttle Limit. PBO &gt; Curve Optimizer -20 on the 2 Preferred Cores, -30 on the other 6. Cinebench 2024 1128 Multi, 117 Single core.  \n While gaming, Starfield, Cyberpunk, Star Citizen, Atlas (steam) plus others my CPU temps are 62 to 70c with occasional jumps to 75c (Cooler Phantom Spirit 120).  \nAM5 has had Ram Instabilities since the beginning, do not be surprised if your Ram will not run Expo on the Stock Bios, update to your motherboards most recent Bios then hopefully you'll be good to go.", "ups": 3, "depth": 0, "author": "Scanoe", "replies": []}, {"selftext": "Just enable expo, resize bar, and either limit temp or undervolt. I\u2019d chose undervolting first", "ups": 2, "depth": 0, "author": "Obvious_Drive_1506", "replies": []}, {"selftext": "You absolutely need to get that RAM to DDR5-6000 - Zen 4 loses a solid 10% of its gaming performance at DDR5-5200.", "ups": 2, "depth": 0, "author": "Tricky-Row-9699", "replies": [{"selftext": "But my cpu does not support 6000", "ups": 1, "depth": 1, "author": "niccoborgio", "replies": [{"selftext": "It does - \u201cofficial memory support\u201d is meaningless for CPUs because AMD and Intel consider enabling XMP/EXPO to be \u201coverclocking\u201d, when in reality any memory kit rated for a certain speed and set of timings has been tested exhaustively by its manufacturer and confirmed to be absolutely stable at those settings. It\u2019s not really overclocking to run your RAM at the manufacturer\u2019s rated speed.\n\nAMD themselves, actually, in the presentation where they revealed Zen 4, stated that DDR5-6000 is the sweet spot for this generation.", "ups": 6, "depth": 2, "author": "Tricky-Row-9699", "replies": []}]}, {"selftext": "It depends more on latencies, in many cases.", "ups": 1, "depth": 1, "author": "LongFluffyDragon", "replies": []}]}, {"selftext": "I would strongly recommend not getting an Asus board, they have been extremely issue-plagued. Faster RAM is also a good idea, even 5600 is similar price and plain better.\n\nYou also dont need an AMD card to use freesync, but the 7800 XT is not bad anyway.", "ups": 2, "depth": 0, "author": "LongFluffyDragon", "replies": [{"selftext": "I read that Asus has fixed the problem, no?\nAs you say it, seems that 7800XT is not so good xD", "ups": 2, "depth": 1, "author": "niccoborgio", "replies": [{"selftext": "They fixed the CPUs literally exploding due to wildly unsafe voltage levels, but the firmware and hardware are still pretty unreliable and poor quality. Bad RAM subtiming stability, lots of reports of exploding power phases and mysterious failures, generally buggy.\n\n&gt; seems that 7800XT is not so good xD\n\nWhat is wrong with it?", "ups": 1, "depth": 2, "author": "LongFluffyDragon", "replies": [{"selftext": "Before buying the mobo I even read up on it and the one from Asus was recommended by many without mentioning this problem at all, so I proceeded with the purchase without any thought. By now the pc is being assembled, I can only hope.", "ups": 1, "depth": 3, "author": "niccoborgio", "replies": [{"selftext": "I would recommend only getting hardware recommendations from hardware enthusiast circles, who tend to be aware of issues with products, and actual specifications, capabilities, ect.\n\nAnywhere full of gamers will typically give you random brand loyalty and half-understood misinformation. You still see some of that here, but it tends to be corrected.", "ups": 1, "depth": 4, "author": "LongFluffyDragon", "replies": [{"selftext": "What brand would you recommend? I've read that Gigabyte and AsRock aren't very performing. That leaves MSI for exclusion.", "ups": 1, "depth": 5, "author": "niccoborgio", "replies": [{"selftext": "Nonsense, basically. Motherboards have zero impact on performance unless they are malfunctioning or unable to deliver sufficient power. Most AM5 boards have ridiculous VRMs, so power delivery is not an issue aside from matching the cheapest boards with an R9.\n\nAsrock, then MSI seem to be most stable and bug-free for this gen (and the tail end of AM4).\n\nGigabyte is a bit janky and has a recent history of many extreme quality control issues (exploding components, weird bugs, missing parts, ect), but still better AM5 track record than Asus.", "ups": 1, "depth": 6, "author": "LongFluffyDragon", "replies": [{"selftext": "If possible, should I return the asus mobo and pick msi?", "ups": 1, "depth": 7, "author": "niccoborgio", "replies": [{"selftext": "If you intend to keep the system for years or upgrade it, and it is not too much hassle to return it, i would recommend it. One place Asus has persistently failed in past gens is timely long-term support/bugfixes, on top of any existing issues/shoddy components.\n\nMSI has a pretty shit past track record, but seems to be catching up to Asrock with their AMD boards now.", "ups": 1, "depth": 8, "author": "LongFluffyDragon", "replies": [{"selftext": "I am not directly assembling the PC, so I hope it is possible to step back.", "ups": 1, "depth": 9, "author": "niccoborgio", "replies": []}, {"selftext": "Anyway, reading here and there, I want to give AsRock a chance. Is the B650 Riptide good or do you recommend something else?", "ups": 1, "depth": 9, "author": "niccoborgio", "replies": []}]}]}]}]}]}]}]}]}]}, {"selftext": "In BIOS turn on the followings;\n1. 4G and Resize bar to enable SAM.\n2. XPM profiles \n3. Enable PBO\n\nIf you're worried about temp, fred not, AMD CPU tends to run hot even at idle, because the CPU will constantly try to maintain high cock speed. This is well known from ryzen 5000 series and above.", "ups": 2, "depth": 0, "author": "xNaRtyx", "replies": []}, {"selftext": "get a 7700 non x so you don't have to worry about CPU temps or mess with bios settings.", "ups": 2, "depth": 0, "author": "ofon", "replies": [{"selftext": "Eh bro, I have already bought everything. I am asking for help about settings", "ups": 1, "depth": 1, "author": "niccoborgio", "replies": [{"selftext": "Oh ok...in BIOS under PCIE you have to enable 4g encoding and resizable bar  \n\n\nWhat cooler did you get for the CPU?", "ups": 2, "depth": 2, "author": "ofon", "replies": [{"selftext": "Arctic Freezer 34 eSports Duo", "ups": 3, "depth": 3, "author": "niccoborgio", "replies": []}]}]}, {"selftext": "Hey can you let me know how it goes with the temps?  I am building with that cpu right now", "ups": 1, "depth": 1, "author": "youAtExample", "replies": [{"selftext": "it depends on your airflow...liquid vs air cooling, the size of your cooler, ambient temps and whether or not you're idle or at load", "ups": 0, "depth": 2, "author": "ofon", "replies": [{"selftext": "I\u2019ve got this cooler and ambient temp should be 70ish Fahrenheit.  https://www.amazon.com/dp/B08S77DVDS?tag=pcpapi-20&amp;linkCode=ogi&amp;th=1&amp;psc=1", "ups": 2, "depth": 3, "author": "youAtExample", "replies": [{"selftext": "that's an excellent cooler...esp for the price! You will be very happy with those 2 put together man. Enjoy!", "ups": 1, "depth": 4, "author": "ofon", "replies": []}, {"selftext": "maybe if you have PBO turned off and you\u2019re undervolted\u2026", "ups": 1, "depth": 4, "author": "LOOK_AT_DEEZ_RECIPES", "replies": [{"selftext": "he has the 7700 non x. It runs way cooler than the X version", "ups": 1, "depth": 5, "author": "ofon", "replies": []}]}]}]}]}, {"selftext": "You have to mess with bio settings to get expo enabled so that\u2019s kinda pointless", "ups": 0, "depth": 1, "author": "Obvious_Drive_1506", "replies": [{"selftext": "Excuse my ignorance what is expo and why do you need to limit the temps or undervolt? I purchased the 7700x a month  ago.", "ups": 1, "depth": 2, "author": "ltdanslegs87", "replies": [{"selftext": "They are run a bit hot", "ups": 0, "depth": 3, "author": "totallybag", "replies": [{"selftext": "I didn't see mine go over 92\u00b0C even during cpu benchmarks, that was on stock settings and peerless assassin. Should be totally fine in 99% of cases and the 1% left shouldn't be using 7700x for their needs.", "ups": 1, "depth": 4, "author": "icouldcarry", "replies": []}]}, {"selftext": "The Ryzen 7000 just run hot, and expo is basically amd\u2019s own xmp which you need to enable to actually get the advertised ram speed", "ups": 1, "depth": 3, "author": "Obvious_Drive_1506", "replies": []}, {"selftext": "Expo is just XMP on AMD motherboards.(overclocking ram to its listed specifications).  If it's not on, you won't be getting the speed you paid for on your ram.", "ups": 1, "depth": 3, "author": "Reklatzzzz", "replies": [{"selftext": "Thanks for the info everyone!", "ups": 1, "depth": 4, "author": "ltdanslegs87", "replies": []}]}]}]}]}, {"selftext": "Why is this my build \ud83d\ude02", "ups": 1, "depth": 0, "author": "xXanderman", "replies": [{"selftext": "Do you have the same components?", "ups": 1, "depth": 1, "author": "niccoborgio", "replies": []}]}, {"selftext": "Keep all software up to date. Good case flow with correct fan pushing &amp; pulling.", "ups": 1, "depth": 0, "author": "brocksuire75", "replies": []}]}
{"post": {"title": "Are We Getting a Game Ready Driver for Phantom Liberty ?", "subreddit": "Amd", "selftext": "Nvidia and Intel both released day 1 drivers for it, will AMD follow suit ?", "ups": 44, "permalink": "/r/Amd/comments/16panb0/are_we_getting_a_game_ready_driver_for_phantom/", "num_comments": 53}, "replies": [{"selftext": "Nvidia released one because dlss 3.5 was added. \n\nWhile gameplay systems were changed and there were a few tweaks under the hood I don\u2019t think there was really a need for a new driver.\n\n\nProbably when fsr3 is added.", "ups": 55, "depth": 0, "author": "dabocx", "replies": [{"selftext": "Do we have ETA on fsr3?", "ups": 9, "depth": 1, "author": "Warelllo", "replies": [{"selftext": "\"soon\"", "ups": 52, "depth": 2, "author": "2022HousingMarketlol", "replies": [{"selftext": "Sad.", "ups": 15, "depth": 3, "author": "vladi963", "replies": []}, {"selftext": "It's being pushed back so they can change the version number to 3.5.", "ups": 21, "depth": 3, "author": "BinaryJay", "replies": [{"selftext": "or FSR 3.51", "ups": 12, "depth": 4, "author": "countpuchi", "replies": [{"selftext": "3.69. Why not", "ups": 4, "depth": 5, "author": "GearGolemTMF", "replies": []}]}, {"selftext": "They will panic-announce FSR 3.5 Ray Rebuilding technology in 3 months with an estimated release date of soon (2 years)", "ups": 11, "depth": 4, "author": "Smackadummy", "replies": [{"selftext": "Ray REBUILDING is exactly what I would expect them to call it. Wow you got AMD marketing down man!", "ups": 4, "depth": 5, "author": "ZookeepergameBrief76", "replies": []}]}]}]}, {"selftext": "Eventually\u2122\ufe0f", "ups": 6, "depth": 2, "author": "dr1ppyblob", "replies": []}, {"selftext": "FSR3 is scheduled for Q1 2024, so don't hold your breath", "ups": 0, "depth": 2, "author": "xng", "replies": [{"selftext": "Fluid Motion Frames at the driver level is Q1 2024. FSR3 is this fall.", "ups": 10, "depth": 3, "author": "TheOctavariumTheory", "replies": [{"selftext": "According to AMD, FSR 3 is Q1 2024, https://www.ign.com/videos/fidelityfx-super-resolution-3-fsr3-amd-stage-presentation-gamescom-2023", "ups": -4, "depth": 4, "author": "xng", "replies": [{"selftext": "The man literally says \"early fall\".", "ups": 8, "depth": 5, "author": "TheOctavariumTheory", "replies": [{"selftext": "Ah, we're talking about different things. I'm talking about FSR3 for everyone in every game through adrenalin. You're talking about 2 specific games that gets to try it first, while others are going to have to wait.", "ups": -6, "depth": 6, "author": "xng", "replies": [{"selftext": "Because what you're talking about is not FSR3. It's AFMF at the driver level that supposedly works with all DX11/12 games, not every game. They're not the same thing in the same way RSR and FSR1 are not the same thing.", "ups": 9, "depth": 7, "author": "TheOctavariumTheory", "replies": [{"selftext": "Ah, thanks for the clarification!", "ups": 2, "depth": 8, "author": "xng", "replies": []}]}]}]}]}, {"selftext": "AMD FMF will be frame generation at home.\n\nWe're so spoiled \ud83e\udd34", "ups": 1, "depth": 4, "author": "chapstickbomber", "replies": []}]}]}, {"selftext": "AMD said this year. There have been claimed [leaks](https://gamerant.com/amds-fsr-3-with-ai-frame-generation-expected-as-early-as-next-month/) (speculation) saying it'll be at end of this month but I wouldn't read too much into that.", "ups": 1, "depth": 2, "author": "CatalyticDragon", "replies": []}]}]}, {"selftext": "Isn\u2019t really needed. Games running better than before (mostly due to the ryzen smt bug fix) but still. Not much more performance to be milked via driver. Nvidia added 3.5 and intel added Xess. Nothing new for amd was done outside of fixing the ryzen bug", "ups": 7, "depth": 0, "author": "LordTism", "replies": [{"selftext": "What setting are you supposed to use for that? Auto, on or off?", "ups": 2, "depth": 1, "author": "sijedevos", "replies": []}]}, {"selftext": "people that don't understand the marketing bullshit behind game ready driver nonsense.\n\nWhat is ... is either a patch/hotfix driver for known issues... OR in the case of nvidia's driver... makes it so the new dlss is functional with it.\n\nAMD doesn't need to always have a \"game ready\" driver available for every little game release or dlc or whatever.", "ups": 7, "depth": 0, "author": "DHJudas", "replies": []}, {"selftext": "I hope they will in time for the DLC. \n\nI saw some benchmarks and the 4070ti was now faster than 7900xt in Raster. Where the XT was beating it with 15-20% before. So unless Nvidia had some thing they could have done all along the XT in raster should be the faster card. And better CPU optimization shouldn't destroy that difference.", "ups": 3, "depth": 0, "author": "MrPapis", "replies": [{"selftext": "&gt;Where the XT was beating it with 15-20% before\n\nAre u sure u were looking at same resolution settings? In 1440p XT never had such a gap, unless u are talking about 4k.", "ups": 17, "depth": 1, "author": "Keldonv7", "replies": [{"selftext": "At 4k the gap is 35%.   \n[https://www.techspot.com/review/2642-radeon-7900-xt-vs-geforce-rtx-4070-ti/#:\\~:text=Overall%20they%20are%20very%20close,could%20be%20a%20lot%20more](https://www.techspot.com/review/2642-radeon-7900-xt-vs-geforce-rtx-4070-ti/#:~:text=Overall%20they%20are%20very%20close,could%20be%20a%20lot%20more)", "ups": -1, "depth": 2, "author": "MrPapis", "replies": [{"selftext": "Depends on the source i guess, i usually listen to videos in the background at work, so i was reffering to hardware unboxed benchmarks   \nhttps://youtu.be/1mE5aveN4Bo?t=591", "ups": 4, "depth": 3, "author": "Keldonv7", "replies": [{"selftext": "It shouldn't depend on this source, because it's the same one. Steve from HWUB is also Steve from Techspot. Sometimes the Techspot articles are literally the script from a HWUB video.", "ups": 5, "depth": 4, "author": "itch-", "replies": []}, {"selftext": "Gamers Nexus was using an 5800x3d and the techspot using 7800x3d. That is definitely a high chance of CPU bottleneck.", "ups": 1, "depth": 4, "author": "MrPapis", "replies": [{"selftext": "Why does benchmarkers not benchmark with more common cpus whether it is bottlenecked or not.", "ups": 0, "depth": 5, "author": "Suspicious-Sink-4940", "replies": [{"selftext": "They do? They make CPU reviews which benchmarks the specific CPU's. When testing a GPU its not relevant to test CPU/CPU bottleneck.", "ups": 2, "depth": 6, "author": "MrPapis", "replies": []}]}]}]}]}]}]}, {"selftext": "its a DLC not a whole ass new game there is no reason to throw a driver at that", "ups": 4, "depth": 0, "author": "Ninjathelittleshit", "replies": [{"selftext": "Well they did do an overhaul to systems and graphics. But I would agree, why would you need a new driver if its just an update. Dont think ive seen those before", "ups": 14, "depth": 1, "author": "hardlyreadit", "replies": []}, {"selftext": "Nvidia and Intel seemed to think that, and update 2.0, was enough to release a driver for", "ups": 10, "depth": 1, "author": "RockyXvII", "replies": [{"selftext": "Nvidia actually has a reason, dlss 3.5 debuted with this. So that makes sense to add a new driver. Intel idk, but if I had to guess I think they need like new drivers once a month since theyre new in the gpu market and are always trying to make improvements to their drivers", "ups": 16, "depth": 2, "author": "hardlyreadit", "replies": [{"selftext": "Intel releases a new driver every two weeks for their GPUs.  I've actually been very impressed with their efforts around GPU drivers.", "ups": 15, "depth": 3, "author": "brumsky1", "replies": []}]}, {"selftext": "Nvidia enabled DLSS 3.5 and Ray Reconstruction with the 2.0 update so a driver was needed.\n\nIntel is just updating their drivers every few weeks and now was a good time for Cyberpunk.", "ups": 3, "depth": 2, "author": "RedChaos92", "replies": []}]}, {"selftext": "DLC itself isn't a whole ass new game, but Patch 2.0 is.\n\nIt is straight up what the game should have been at launch. It's also a lot heavier on both CPU and GPU, but it looks the part.", "ups": 6, "depth": 1, "author": "ms--lane", "replies": []}]}, {"selftext": "Probably next week couple of hours before release probably or a day", "ups": 1, "depth": 0, "author": "Melodias3", "replies": []}, {"selftext": "Post release there will be tweaks, but nothing day 1. The same way Nvidia got shafted with Starfield being an AMD exclusive - CP2077 is for Nvidia.", "ups": -2, "depth": 0, "author": "2022HousingMarketlol", "replies": [{"selftext": "The game is pretty optimized for AMD. With Raster, AMD is equal or better compared it's Nvidia counterpart. \n\nIt's at RT, especially heavier RT loads that AMD just falls apart simply because it doesn't have the hardware, nor the software to take advantage of visual features of the game.", "ups": 6, "depth": 1, "author": "n19htmare", "replies": []}, {"selftext": "&gt;The same way Nvidia got shafted with Starfield being an AMD exclusive - CP2077 is for Nvidia.\n\nAnd that's exactly why Cyberpunk 2077 will be one of the very first games to ever add AMD's FSR3 in the next few months.\n\nAnd that's why Cyberpunk has an often updated AMD dev branch on Steam, updated a few times in the last month.\n\nRight? Right?!", "ups": 7, "depth": 1, "author": "heartbroken_nerd", "replies": [{"selftext": "Starfield launched without basics like brightness adjustment, let alone HDR or FOV, but we're still under the pretense it's a conspiracy?\n\nMeanwhile, Cyberpunk's 2.0 update is using outdated versions of both FSR *and* DLSS.  How that is even possible, especially when they're adding RR with PT, is beyond me.", "ups": 0, "depth": 2, "author": "Shidell", "replies": [{"selftext": "Starfield simply has a very limited game engine, they only implement what they need and nothing that raises the game price like dlss.", "ups": 0, "depth": 3, "author": "xng", "replies": []}]}]}]}, {"selftext": "Imo there should be no game-specific drivers.", "ups": 0, "depth": 0, "author": "SupinePandora43", "replies": [{"selftext": "It isn\u2019t a game specific driver, just some optimizations to make these  big AAA titles run smoother than they otherwise would.", "ups": 1, "depth": 1, "author": "Astrikal", "replies": []}]}, {"selftext": "AMD been ready since ATI", "ups": -2, "depth": 0, "author": "imabeach47", "replies": []}, {"selftext": "How can they have already released drivers for a DLC that isn't even out for a few more days?", "ups": -14, "depth": 0, "author": "TheLazyGamerAU", "replies": [{"selftext": "Erm... You really think these giants (nVidia, Intel, etc.) get stuff that's about to come out same time as we \"mortals\" do? Lol", "ups": 11, "depth": 1, "author": "dubtrainz-next", "replies": []}, {"selftext": "Not necessarily for the DLC but for the 2.0 update as well.", "ups": 3, "depth": 1, "author": "Astrikal", "replies": []}]}, {"selftext": "AMD only releases day one drivers if there is actually something busted that needs to be fixed due to the game not working correctly with the current driver (starfield, anyone?). The driver takes a more agnostic approach, which *usually* works.\n\nNvidia tends to do a lot of tuning of their massive per-game-optimized spaghetti driver to play well with new games, often doing unique codepaths or tweaks for individual games.\n\nIn both cases it depends on them getting early cooperation from the developers to find and fix issues, or do special optimizations for big games, in nvidia's case.", "ups": 1, "depth": 0, "author": "LongFluffyDragon", "replies": []}, {"selftext": "We aren't, because Game Ready Driver is Nvidia branding.", "ups": 1, "depth": 0, "author": "Paganigsegg", "replies": []}, {"selftext": "You typically do not need a new driver for any new game or application.", "ups": 1, "depth": 0, "author": "CatalyticDragon", "replies": []}, {"selftext": "I know I'm late to the party, but they did, it's just a beta driver for now I think.\nHere's the link: https://www.amd.com/en/support/kb/release-notes/rn-rad-win-23-9-3-cyberpunk2077-payday3", "ups": 1, "depth": 0, "author": "JRiiice", "replies": []}]}
{"post": {"title": "Delta between core and hotspot temp is widening with time", "subreddit": "Amd", "selftext": "I have a XFX merc 310 7900XTX despite the issues I had, one positive thing about it was the low temps at 63c at core and 82c at hotspot. Now the delta recently became 63 to 90 under load, while I know it's perfectly normal to have these temps, the fact that it's gradually widening worries me a bit. Would XFX RMA service repaste it within the warranty? or should I do the deed when it becomes higher?\n\nIs anyone else with a similar experience?\n\nEdit: I found a helpful clause in XFX warranty, leaving it here for people like me to get help if they face a similar issue. Good to know, now I'll focus on my work instead!\n\n\" **Can I service my graphics card myself without VOIDING Warranty?**\n\nFor USA and Canada, you can service the graphics card yourself, piercing warranty stickers will not void the limited warranty. This includes replacing thermal paste, and cleaning out the fan/heastinks assembly of dust and debris. **IMPORTANT:** Any accidental physical damage to components on the card however will not be covered by the limited warranty, servicing the card is only recommended for PC Technicians and other IT professionals. Please take care when servicing a card, components can be sensitive and delicate.**\u200d**\\*\\****XFX has carefully selected the optimal thermal or fansink component for your graphics card model. \u00a0We do not encourage the removal of components due to damage that may result in the process. XFX understands that some enthusiasts may choose to replace the original component with their own cooling solution. To support the gaming community, it is required that you contact XFX prior to any modifications so that we can update your profile and product registration to avoid potential issues with warranty support. \u00a0In addition, XFX support will be able to walk through the installation with you or provide feedback and pointers on available options for your specific product. \u00a0You may even consider shipping your components to XFX and allow the technicians at XFX to perform the modification for you (shipping charges to XFX apply). Failure to contact XFX could result in termination of the limited warranty. For VEGA class products, it is recommended not to touch the cooling solution or thermal paste, the VEGA GPU and HBM memory are very sensitive and can be easily damaged compared to previous GPUs.***\n\n***ANY Modifications done to a card must be reversible. Any card that needs to be sent in for warranty must be returned to 100% factory stock condition, including original cooling components.*** \"  \n\n\n  \nSecond and last edit: Apparently delta can differ with the application too, Cyberpunk 2077 gave me the delta highest, and now somehow I'm back to the usual 23c delta on usual benchmarks. I don't know if it's possible to paste fixes itself, or if the results I got were an outlier. Wanted to add this for transparency. It feels really weird to see delta drop so suddenly but here we are.", "ups": 11, "permalink": "/r/Amd/comments/16pdii7/delta_between_core_and_hotspot_temp_is_widening/", "num_comments": 42}, "replies": [{"selftext": "I have the same card and I've noticed the same thing.  If you live in the United States, we have laws that allow you to open products and still be covered under the warranty.\n\nI've been considering buying a Thermal Grizzly Kryosheet to install on my GPU.  Frankly, I haven't because I've switched to a mild OC while UV.  2750Mhz @ 1115mv results in 60c ish GPU temp with hot spot in the low 80s.  I did this primarily because of the amount of heat produced is noticeable less and lower power draw.", "ups": 5, "depth": 0, "author": "brumsky1", "replies": [{"selftext": "If your delta is under 20 degrees and you're not seeing more than low 80s on the hot spot the Kryosheet won't help much. Those are normal/acceptable temps.", "ups": 1, "depth": 1, "author": "Mercennarius", "replies": [{"selftext": "Exactly. They stated they haven\u2019t gotten it because their UV resulted in those temps meaning the kryosheet is unnecessary.", "ups": 3, "depth": 2, "author": "Paccypacpac", "replies": []}, {"selftext": "Yeah I agree.  When I was going for max OC without UV.  I did see hot spot temps 25-30c over avg GPU temp.  Something like mid 60s avg to 90ish hot spot.\n\nThe truth is, for me, the extra heat and power draw wasn't worth 1-4 FPS on average.", "ups": 1, "depth": 2, "author": "brumsky1", "replies": []}]}, {"selftext": "That's a good law.", "ups": 1, "depth": 1, "author": "bobalazs69", "replies": []}]}, {"selftext": "That\u2019s still within spec so I doubt they\u2019d do anything. If you want to repaste just grab the ptm7950 thermal pad and throw it on.", "ups": 4, "depth": 0, "author": "Obvious_Drive_1506", "replies": [{"selftext": "Servicing by yourself voids your warranty here so I guess I'll wait until it reaches 100c territory before asking the RMA.", "ups": 2, "depth": 1, "author": "sad7900xtxuser", "replies": [{"selftext": "The limit is 110c so if it isn\u2019t throttling they won\u2019t take it", "ups": -1, "depth": 2, "author": "Obvious_Drive_1506", "replies": [{"selftext": "I won't ask them to replace it, but to service it, I think it's totally fine if I pay for the service fee.", "ups": 1, "depth": 3, "author": "sad7900xtxuser", "replies": [{"selftext": "Again, they may not service it since it\u2019s not outside of normal operating range.", "ups": -2, "depth": 4, "author": "Obvious_Drive_1506", "replies": []}]}, {"selftext": "Technically it will start throttling well before 110....110 is just when the card is designed to reduce power and clock speeds at all costs to avoid exceeding that temperature. But cards will start losing performance well before 110.", "ups": 1, "depth": 3, "author": "Mercennarius", "replies": []}, {"selftext": "You're getting down voted, but you're absolutely correct.  It's within operating tolerances, it won't be handled via an RMA.", "ups": 2, "depth": 3, "author": "looncraz", "replies": []}]}]}]}, {"selftext": "Yup, had the same issue on my Powercolor 7900 XTX. You're experiencing paste pump out with the thermal cycling. Will likely get worse. I solved this by going with a Thermal Grizzly Kryosheet pad...repasting it won't fix this. The Kryosheet made a massive difference in my hot spot delta and it won't experience pump out either...core temps were relatively unchanged.", "ups": 3, "depth": 0, "author": "Mercennarius", "replies": [{"selftext": "yeah sadly I'm way too clumsy for this. glad(!) to know I'm not alone though! thanks!", "ups": 1, "depth": 1, "author": "sad7900xtxuser", "replies": [{"selftext": "It's not hard...I did it in 15 minutes. Just place the pad on the die and then tighten the cooler back onto the card. Only tedious/delicate part of the process is just making sure the pad stays centered on the die before you tighten the cooler back down.", "ups": 1, "depth": 2, "author": "Mercennarius", "replies": [{"selftext": "Yeah, shaky hands and tech don't go well together haha", "ups": 2, "depth": 3, "author": "sad7900xtxuser", "replies": [{"selftext": "Sure...don't over estimate how hard it is though. It's easy. Also, even if you got a replacement card, the rate at which pump out seems to be a problem this generation means you likely may end up in the same situation. You are likely better off just putting a good $20 pad like the Kryosheet on yourself, i'm sure you can do it just fine.", "ups": 1, "depth": 4, "author": "Mercennarius", "replies": [{"selftext": "yeah I'll just update my post, I found a fun clause in xfx warranty that they claim they'll service the card with new components in the case user wants to, and it'll be within warranty. Guess that shows a path to me.", "ups": 1, "depth": 5, "author": "sad7900xtxuser", "replies": []}]}]}]}]}]}, {"selftext": "Try PTM7950 if you going to repaste but be aware if you have anything that looks like user damage you may be out of luck for warranty.\n\nAs for void warranty sticker its not enforceable in US or EU \n\n[https://www.ifixit.com/News/74736/warranty-void-stickers-are-illegal-in-the-us-what-about-elsewhere](https://www.ifixit.com/News/74736/warranty-void-stickers-are-illegal-in-the-us-what-about-elsewhere) explains EU and elsewhere.\n\nIf you ever repasted before i i recommend getting PTM7950 and trying this, has given many good results.\n\nmy liquid devil 7900 XTX had delta go from 30 to 42c with hotspot of 92c even, dropped back down to 60-70 range my liquid devil pumped out in 1-2 weeks both repastes before going PTM7950 that is slightly older then a month now with no sign of degradation so far.\n\nThe way you can tell you have pumpout is you used to hit lower temps at lower case or room temp and now even with open case fans at 100% it makes no difference as if there no contact with hotspot at all with cooler.\n\nPTM7950 is also on the 4090 FE and Lenovo also uses it in their laptops, linus tested it in 1 of he's videos comparing it to liquid metal testing it on CPU.", "ups": 2, "depth": 0, "author": "Melodias3", "replies": [{"selftext": "yeah, it feels the same, the edge temp is the same, yet the hotspot keeps increasing.  I know about the PTM, but I had a bad experience with it, and maybe it was a fake. I have no good source to buy it as I'm not from US. could try kryosheet though. I wonder if I asked the xfx service would they put it in the card without voiding warranty lol.", "ups": 1, "depth": 1, "author": "sad7900xtxuser", "replies": [{"selftext": "I got mine from ebuy7 but moddiy seems to be safest option.", "ups": 1, "depth": 2, "author": "Melodias3", "replies": []}]}]}, {"selftext": "I own the same card, saw the same behavior. The thermal paste starts pumping out. XFX will probably not RMA since it's still within spec. If you have the courage, you should replace the stock paste with a Kryosheet or PTM7950 as suggested by others. I did and it completely rectified the issue.", "ups": 2, "depth": 0, "author": "TheAlcolawl", "replies": [{"selftext": "How is your delta after changing it with a pad?", "ups": 1, "depth": 1, "author": "VinumNoctua", "replies": [{"selftext": "During normal gaming sessions it hovers around a 15\u00b0C delta. During a few Time Spy loops, 20\u00b0C. The hot spot rarely hits 80\u00b0C.", "ups": 2, "depth": 2, "author": "TheAlcolawl", "replies": [{"selftext": "I have XT, and delta went from 20-25 to 40 since January. Opening the card voids the warranty in my country, but I guess it'll be worth it since I'm going to use it for a long time.", "ups": 1, "depth": 3, "author": "VinumNoctua", "replies": [{"selftext": "Same, got 7900xt in january. Delta went from 18 to 36*c. They dont allow me to repaste :(", "ups": 1, "depth": 4, "author": "thic_pug", "replies": []}, {"selftext": "Idk where you live but the warranty sticker doesn't mean a damn thing in the US and EU. I'd say it's worth the shot.", "ups": 1, "depth": 4, "author": "TheAlcolawl", "replies": []}]}]}]}]}, {"selftext": "Have you been tweaking the OC on the card? The hotspot temp delta increases significantly the more you OC the card. I have the PowerColor Red Devil and at high OC I was getting a 25 core-hotspot delta as well but when I undervolted to 1120 + capped max core freq to 2500 it goes down to a 10-15 delta. From what I've seen it is not worth it to OC this card as both the temps and TDP increase significantly for only very marginal gains.", "ups": 2, "depth": 0, "author": "Skyro620", "replies": [{"selftext": "tried the OC before, and it did improve performance up to 5 percent which is not bad but no, the temps are on stock, it's near 97c hotspot when it's +15% PL, I know the differences because I love to tweak the card. at first, it was 88c on +15%, the hotspot gap is widening while the edge temp is the same.", "ups": 1, "depth": 1, "author": "sad7900xtxuser", "replies": []}]}, {"selftext": "This is pretty normal on new GPUs without pad-based cooling. On repaste replace it with kryosheet/PTM/some other pad-based TIM", "ups": 2, "depth": 0, "author": "Curious-Thanks4620", "replies": []}, {"selftext": "Unfortunately it's pretty common with these cards (not just your model)\n\nThey're very sensitive to pump out as others have said\n\nIf you go the \"repaste\" route don't waste your time with thermal paste and buy a PTM7950 or kryosheet pad\n\nResults with thermal paste will look good for some days and then the delta will widen again...", "ups": 3, "depth": 0, "author": "Confitur3", "replies": [{"selftext": "This is what I ended up doing with my Merc 7900XTX. No issues ever since.", "ups": 2, "depth": 1, "author": "layydback", "replies": []}]}, {"selftext": "Were the test conducted in the same environment? Same room temp, benchmarks, etc?", "ups": 1, "depth": 0, "author": "Mysteoa", "replies": [{"selftext": "more or less the same, it's not a lab environment ofc, it shouldn't matter much cuz the difference is not the delta between room temperature but between edge and hotspot on the card.", "ups": 1, "depth": 1, "author": "sad7900xtxuser", "replies": []}]}, {"selftext": "My 6900XT hits around 20~25\u00b0C hotspot delta with original air cooler and replaced thermal pads and paste. I believe the delta was around 15C with a waterblock. The included pads were terrible and were destroyed when originally removing the heatsink, so had no choice but to replace them after changing back to air cooling.", "ups": 1, "depth": 0, "author": "L1191", "replies": []}, {"selftext": "I have a 7900 XT and am experiencing the exact same issue. I created a few posts about it here on Reddit. I bought this card in January, and everything was fine, like a 20\u00b0C-ish delta, but now I see a 40\u00b0C with default settings.\n\nRecently, I had a conversation with the support and I'm convinced it's 99% thermal paste issue. Opening the card voids my warranty in my country, but I'm going to do it anyway. Everyone suggests PTM7950. I'll try to use normal thermal paste first and look for the changes, then I'll buy this pad and change it for good.\n\nEdit: I should also say that if you send the card back, they won't do anything unless the hotspot reaches above 110\u00b0C.", "ups": 1, "depth": 0, "author": "VinumNoctua", "replies": [{"selftext": "Exact same problem bro.", "ups": 1, "depth": 1, "author": "thic_pug", "replies": []}]}, {"selftext": "Had a 7900XTX from Gigabyte, Gaming OC model. Exactly same behaviour. Good at first 60c on the edge and 85-87 on hotspot. Then it got worse and finally after 6 months temps went to over 100 on the hotspot, in some games even reaching 110. RMAed it, but since its Gigabyte they didnt give a damn, card was in service for a month, they did not respond either to me or the shop that I bought it from. Finally seller decided to step up and refunded me.\n\nGot Nitro+ from Sapphire and so far its night and day difference. Delta never exceeds 20-25 degrees (hot spot usually at around 60 and edge at 80) even though its set at 400W.\n\nSeems its a common issue, too bad not much talk about it. It should not be acceptable that the temps degrade so much and you shouldnt be required to fix it by yourself with repasting.", "ups": 1, "depth": 0, "author": "Mundane-Ad7202", "replies": [{"selftext": "How long have you been using the Nitro+?", "ups": 1, "depth": 1, "author": "VinumNoctua", "replies": [{"selftext": "2 months so far. No degrade in temps as I am monitoring it all the time anyway, but who knows what will happen in 6 months. But one difference is that no matter how much power you throw at it even max so 460W, hotspot never exceeds 90 and delta is still at around 20-25. Amazing card so far.", "ups": 1, "depth": 2, "author": "Mundane-Ad7202", "replies": []}, {"selftext": "I have had the nitro since launch, no issues here. Still 15-17c delta at 460w load.", "ups": 1, "depth": 2, "author": "Consistent_War_4703", "replies": []}]}]}, {"selftext": "Try running a few loops of Port Royal if you can, see what the delta is (it's the worst I've seen on mine).", "ups": 1, "depth": 0, "author": "JediF999", "replies": []}]}
{"post": {"title": "I have put ryzen 7600x on cheapest a620 board", "subreddit": "Amd", "selftext": "And it's not as bad as people claims it .\n\nI bought MSI pro a620-e motherboard and put a 7600x inside.\nReviewers says it's fine for non x processors because they have lower TDP and the power delivery is limited on the board and I consent about the limitations of the power delivery. But reviewers also says it will massively hinders 7600X processors which I found is not true. If the processor might be a little hindered it is not by much and I found I have a better R23 score than some other guys that kept their processor stock on a b650  board.\n\nHowever on the 100PPT limit I see in ryzen master while using cinebench r23( which is at 100% all the time using that app btw). I am able to keep a clock of 5.2 GHz in boost for a multicore result of 15083 with a temp in the vicinity of 85C.\n\nAll in all, I would say, people on a budget can thus use the cheapest DDR5 motherboard and get 5800X performance on a 7600X, just keep in mind that the mofsets are not cooled with a heatsink on that board so you need airflow on the motherboard chipsets and also that a 7600 non X version is probably a smarter choice there.\n\nObviously, that board does not have much in term of overclocking capability, but if you ended up with a ryzen 5 X on this motherboard, don't worry too much about it and keep using it.", "ups": 86, "permalink": "/r/Amd/comments/16oicsd/i_have_put_ryzen_7600x_on_cheapest_a620_board/", "num_comments": 136}, "replies": [{"selftext": "I had someone tell my friend as Asrock b550m steel legend couldn't handle a 5700x. People massively underestimate motherboards. The difference between a flagship EVGA board and $200 board isn't that much from a practical application standpoint. You can push a 5950x to the limit in both boards without the boards themselves overheating.\nBasically any board can handle any CPU with the right socket as long as it's bios compatible and you aren't overclocking. And most mid-tier boards can handle any compatible processor except in the absolute most extreme CPU overclocking. \n\nMost of what you pay for in a high end board is aesthetics and features. Unless blowing up CPUs for leaderboard scores is your hobby", "ups": 53, "depth": 0, "author": "Deliriousdrifter", "replies": [{"selftext": "\n&gt;The difference between a flagship EVGA board and $200 board\n\nIs $200 mid range for motherboards? The fuck happened?", "ups": 14, "depth": 1, "author": "I9Qnl", "replies": [{"selftext": "Well motherboards for DDR5 memories are expensive. I personally use a MSI PRO B650-P WIFI (not their most expensive product), which costed me about \u20ac179,00. It's now increased to &gt;\u20ac200 here in the Netherlands.", "ups": 4, "depth": 2, "author": "Suikerspin_Ei", "replies": []}, {"selftext": "Top end Boards are $600-900 $200 boards like the mag tomahawk and Asus Rog Strix are the mid tier.\n\nJayzTwoCents has a really good video explaining the insane inflation of motherboards", "ups": 7, "depth": 2, "author": "Deliriousdrifter", "replies": [{"selftext": "those are not \"top end\", they are  \"scam end\" for whales and influencers. a good top end board shouldnt cost more than $200.", "ups": 12, "depth": 3, "author": "mkdr", "replies": [{"selftext": "I agree about the price, however those \"top end\" has better passive cooling via heatsinks, better quality capacitors etc.", "ups": 1, "depth": 4, "author": "Suikerspin_Ei", "replies": [{"selftext": "They don't have better capacitors, they might have more, but no MOBO company uses anything better than nicion 10k I don't think.", "ups": 1, "depth": 5, "author": "Assationater", "replies": []}]}]}, {"selftext": "I bought a B450 for like $80 albeit it was last gen when i bought it but even brand new B550s could be had for only a little more at the time, this is ridiculous.", "ups": 6, "depth": 3, "author": "I9Qnl", "replies": []}, {"selftext": "Maybe I\u2019ve misread, but the rog strix z790-e is nearly $700 Canadian. I have one. Is it worth it for the average user? Hell no. Does it have some comfy ram/cpu oc ability and a lovely bios? Sure does. Is it better than a board half the price? Likely not by much if at all. \n\nI think the only actual advantage I\u2019ve got from this board outside aesthetics, is the ability to OC ram beyond 8000mhz and surely there are cheaper boards that can do that.\n\nGood chance you\u2019re paying for a bunch of m2 heat sinks you likely won\u2019t use and some fancy letters. If I didn\u2019t get it for half price, I\u2019d have returned it for the tuf.", "ups": 2, "depth": 3, "author": "NotsoSmokeytheBear", "replies": [{"selftext": "Yeah, I paid $350 for a good x570 board at launch.  Then I learned what a crapfest RGB support is on Linux and that bleeding edge hardware (new Ethernet and WiFi chipsets) isn't much fun either.\n\nWill never do hardware at launch or RGB again.", "ups": 1, "depth": 4, "author": "aztracker1", "replies": [{"selftext": "I hadn\u2019t planned on rgb but decided to make a nice aesthetic last minute. Fortunately using windows it was easy to get everything in sync but all I really did was use the bios aura sync and Lian li l connect to sync. Once synced I uninstalled l connect and they remain in sync. I control them all with signalrgb - if you can get that on Linux it\u2019s amazing. No need for armory crate etc. \n\nAs for lan I really don\u2019t understand why these high end boards use flawed intel nics. It\u2019s much like how even a $10,000 tv will have a 100mbps lan port. I grabbed a Realtek 2.5gb for $40 and it works flawlessly.", "ups": 1, "depth": 5, "author": "NotsoSmokeytheBear", "replies": []}]}]}]}, {"selftext": "$200+ yeah... you can get decent boards for less though. B series in AMD is generally fine for most people.  Will mostly come down to nvme slot count, front usb3, wifi and network chipsets. Higher end Wii add feedback leds and on board power and reset.  BIOS flashback is getting more common too.  All depends on your needs.", "ups": 1, "depth": 2, "author": "aztracker1", "replies": []}]}, {"selftext": "Most board at around 150$ price point does enough for 99% user base that is for sure. All these gaming yada yada branding is bullshit. Expansive boards are made for overclockers. Features like wifi and Bluetooth are nice features. A little bit of heatsink can be a nice touch. Reinforcement of the PCI slots for GPU is ok. But users do not need  huge chunks of metal nor leds and fans on the board itself.", "ups": 15, "depth": 1, "author": "SuccotashAdditional", "replies": [{"selftext": "It's the same thing with everyone slapping massive water coolers on their completely tame CPU.", "ups": 11, "depth": 2, "author": "xXDamonLordXx", "replies": [{"selftext": "Not mentionned in the thread but I do use a 35USD air cooler.", "ups": 6, "depth": 3, "author": "SuccotashAdditional", "replies": []}, {"selftext": "My buddy bought 360 AiO Galahad for his 5600 just for looks ;)", "ups": 2, "depth": 3, "author": "BOLOYOO", "replies": [{"selftext": "Fractal Design solid panel cases FTW. No RGB. No window.", "ups": 1, "depth": 4, "author": "aztracker1", "replies": []}]}]}, {"selftext": "For me I tend to look for at least 2 nvme and front USBC, Bluetooth, WiFi and BIOS flash back.  Which many current boards have. Will do pcie5 nvme for my next build as well.", "ups": 1, "depth": 2, "author": "aztracker1", "replies": []}]}, {"selftext": "&gt; I had someone tell my friend as Asrock b550m steel legend couldn't handle a 5700x. People massively underestimate motherboards. \n\nSome people are also just completely ignorant and unable to stop themselves. If it cant handle that, every other CPU is also out!", "ups": 4, "depth": 1, "author": "LongFluffyDragon", "replies": []}, {"selftext": "that asrock b550m will run a 5950x all day long... so whomever told your friend is totally full of shit...", "ups": 4, "depth": 1, "author": "DHJudas", "replies": []}, {"selftext": "A620 are officially allowed to not provide enough power and before there were some mITX boards limited to 65W TDP chips while the top ones were 95W\n\nbut outside of these all boards will support all processors on stock, that's the main reason of ever increasing prices of entry level boards though, if you know you'll never use the high power chip why pay for extra power delivery?", "ups": 2, "depth": 1, "author": "mornaq", "replies": []}, {"selftext": "Here I am running my 5700x in an ASRock AB350 pro4 lol.", "ups": 2, "depth": 1, "author": "WillTheThrill86", "replies": [{"selftext": "I bumped a b350 BIOS going from 2600 to 5700 (non X) ... ran great, lower power and the 3200 ram could go at the full speed.", "ups": 1, "depth": 2, "author": "aztracker1", "replies": [{"selftext": "Yeah that's been my experience as well with the ram. Running 2x16GB at full speed with it.", "ups": 1, "depth": 3, "author": "WillTheThrill86", "replies": []}]}]}, {"selftext": "Maybe it goes back to bulldozer days where people would try to run a 125 TDP fx 8350 on a board with 3 crappy phases. CPUs generally have lower TDPs now. 7600x is 105W TDP. And VRM are generally better up and down the stack. I wouldn't run a 7950X on that board, but you could if you limited CPU wattage to 105W or something, if that's something you can do with that board. You wouldn't even lose that much performance.", "ups": 1, "depth": 1, "author": "tamarockstar", "replies": [{"selftext": "This right here is kinda my point. Nowadays mid-tier boards have at minimum a 8 phase VRM that's literally 3x as good as anything even 5 years ago. A $200 board can easily run any CPU. And overclock all of them as well. Last gen you could do the kinda overclocking that kills CPUs for leaderboards on a sub $200 B550 board. And VRMs are only getting better on average over time", "ups": 3, "depth": 2, "author": "Deliriousdrifter", "replies": []}, {"selftext": "There were AMD first Gen ryzen boards that didn't have very good VRM configs.", "ups": 1, "depth": 2, "author": "aztracker1", "replies": []}]}]}, {"selftext": "I currently run the cheapest b350 motherboard of the time (60 euros, how is that possible?) with a 5800x and it runs beautifully. What more is somehow SAM is enabled even though it says you need a 400 series chipset at least!\n\nDo consider what the vrm heatsinks are if any are present, then reviewers will get what are the best budget mobo with good performance and longevity.", "ups": 11, "depth": 0, "author": "AccroG33K", "replies": [{"selftext": "I am using a 40e b450m msi gaming plus\nBought It used ran on it a r5 1600/5600 and now a  5800x3d", "ups": 4, "depth": 1, "author": "Mario2x2SK", "replies": []}, {"selftext": "Same but with a 5800X3D. I\u2019ve been running it since that chip came out.", "ups": 2, "depth": 1, "author": "invictus81", "replies": [{"selftext": "Honestly I REALLY wanted that CPU back then, the ULTIMATE upgrade for a PC that even the old parts still get used to this day (NAS). But the price difference... I went for the 5800x standard since it was less than 300 euros when the 3d variant cost was 50% more! Nowadays with the arrival of Ryzen 7000, it is now at the 280 to 330 euro mark, which is a much better deal.", "ups": 3, "depth": 2, "author": "AccroG33K", "replies": [{"selftext": "For sure. It was quite pricy during release, I think I paid roughly $550 CAD. But I upgraded from R5 1600 to 5800X3D. I reckon because I waited since 2017 it was worthwhile.", "ups": 2, "depth": 3, "author": "invictus81", "replies": [{"selftext": "Yeah I was on a Ryzen 3 1200 so even a 5600x would have led to a 4x performance increase lol \ud83d\ude02", "ups": 1, "depth": 4, "author": "AccroG33K", "replies": []}]}]}, {"selftext": "Much harder justifying an upgrade away from the 5800xd to 7000 series. Pretty big chunk of change.", "ups": 2, "depth": 2, "author": "bat-fink", "replies": [{"selftext": "If you also need new ram and motherboard to upgrade to 5000 series. Otherwise, it's pretty easy to justify.", "ups": 1, "depth": 3, "author": "AccroG33K", "replies": [{"selftext": "I went from 5600x to 7600x so I'm aware\n\nMonetarily, he'd probably need to shoot for a 7800x3d to make it worth his while, [7600x and 5800x3d trade blows] and with a whole new motherboard/ram spending - $650+ is hard to justify to me, considering how well the 5800x3d is holding up.", "ups": 1, "depth": 4, "author": "bat-fink", "replies": [{"selftext": "You already had a 5000 series CPU, that's completely different from our case! I won't comment on the why you would want to upgrade from a 5600 to a 7600.\n\nWe were all using a 1st gen Ryzen CPU with a first gen AM4 mobo, building an entire CPU + ram + mobo + CPU cooler combo is WAY more expensive than just slapping a 5800x and doing bios update. Would we get the same performance as zen 4? No! But we would be close enough to get another 3 to 5 years of gaming with existing machine!", "ups": 0, "depth": 5, "author": "AccroG33K", "replies": [{"selftext": "Not sure you're meant to be replying to me as none of what you said made sense in context. \n\nHave a good weekend though, dude.", "ups": 0, "depth": 6, "author": "bat-fink", "replies": [{"selftext": "You may need to read one more time previous comments, I can't understand how my reply was out of context since it's in the continuity of the root comment...\n\nOr maybe I offended you on the one generation upgrade thing. I mean, why wouldn't we upgrade from 1st gen Ryzen to 5th gen ryzen while also going one tier up, when you upgrade only one generation, and stay in the same product tier?\n\nOtherwise, have a good weekend too, dude", "ups": 0, "depth": 7, "author": "AccroG33K", "replies": []}]}]}]}]}]}]}]}, {"selftext": "I used to have an A320m-k and I ran a 5600x for a long time with no issues", "ups": 10, "depth": 0, "author": "Psilogamide", "replies": [{"selftext": "People have been using r9 5950x with a320 motherboards lot of times due to oems", "ups": 8, "depth": 1, "author": "Camilo_D2005", "replies": [{"selftext": "You got a source backing this claim?\n\n&amp;#x200B;\n\nI'm in massive doubt people would use 5950Xs and not notice the power drop. Unless they buy prebuilts and don't know anything about PCs.", "ups": 2, "depth": 2, "author": "fogoticus", "replies": []}]}]}, {"selftext": "What motherboard has to do with cpu temps? I would be more concerned about motherboard vrm temps and longevity, even if you can check that on A620, as most of the time these cheap boards don't even bother to put a temp sensor.", "ups": 11, "depth": 0, "author": "batvinis", "replies": [{"selftext": "As I said the non X version is more suited for the motherboard, but it does not hinder 7600x as much as reviewers tell it does.\n\nI pointed out that the mofsets have no cooling whatsoever and that for this reason you would want to use smaller tdp processor like 7600.\n\nAny way the PPT is locked at 100 so at the same time it is not like MSI teams did not knew what they were doing about the VRM not having heatsinks. (It's the marketing team that is the issue for this motherboard)\n\nI would not put any stronger than ryzen 5 on this board.", "ups": -1, "depth": 1, "author": "SuccotashAdditional", "replies": []}]}, {"selftext": "I use an a620m and 7600x and use eco mode. I\u2019ve never had the cpu run over 80\u2022 and it runs at 100% on benchmarks.", "ups": 4, "depth": 0, "author": "WonderfulJerk", "replies": [{"selftext": "Does eco mode hinder score and boost?", "ups": 2, "depth": 1, "author": "SuccotashAdditional", "replies": [{"selftext": "it does a little but with not much work done you can equal or even beat stock performance at slightly above eco TDP", "ups": 3, "depth": 2, "author": "Cnudstonk", "replies": []}, {"selftext": "I haven\u2019t spent a ton of time doing benchmarking other than to stress test, so I couldn\u2019t say. What I can say is for my uses, I have no bottleneck in anything I do. I\u2019m usually gaming at 2160p with my 6700xt and everything runs smooth and pretty cool, even in a small case.", "ups": 1, "depth": 2, "author": "WonderfulJerk", "replies": []}]}]}, {"selftext": "Ppl are so overkill with boards in their builds in this subreddit. Psus too (wattage wise). psus is more polarizng. I'll see like the cheapest psus possible with high wattage, or ppl just go completely overkill with 1000w psus to run like a 7600 + 6800 xt, where an a tier 650/750w would have been more than fine.", "ups": 3, "depth": 0, "author": "lemon07r", "replies": []}, {"selftext": "I organized an r5 7600x and an asrock a620m motherboard for a colleague. The bucket just runs smoothly.", "ups": 5, "depth": 0, "author": "HerrrHerrmann", "replies": []}, {"selftext": "I see people talking about undervolting, but if I recall correctly a-series chipsets don't support overclocking, which might mean undervolting is also out of the question in that case. If anyone knows something I don't, please feel free to reply with the added knowledge.", "ups": 2, "depth": 0, "author": "Thesadisticinventor", "replies": [{"selftext": "You are right from. The voltage options in bios are locked a chipset boards. TechYesCity did a video on this saying how a 7800X3D has equivalent performance compared to X series but voltage locked settings prevented undervolting, which means it will run at default, so toasty temps if there isn't adequate cooling.", "ups": 2, "depth": 1, "author": "Saffy_7", "replies": []}]}, {"selftext": "The only real impact is lack of CPU overclocking. All the other benefits of upgrading the chipset are for more/ faster USB ports and additional/faster PCIE lanes, which very few solutions exist and/or are reasonable.", "ups": 2, "depth": 0, "author": "aplethoraofpinatas", "replies": []}, {"selftext": "Mobo's are super Over-spec right now. \n\nGo watch some Actually hardcore overclocking videos on YouTube. \n\nHe breaks it down well. \n\n8 layer pcb? 12+ phases? none of it is necessary even no heatsink those phases don't get hot they are just that much better than they used to be.. now its just inflating the build price to give them better margins on the high end stuff.. a bit extra part for a huge price.", "ups": 2, "depth": 0, "author": "Solarflareqq", "replies": [{"selftext": "When did he ever mention that no heatsink on phases is ok?", "ups": 1, "depth": 1, "author": "fogoticus", "replies": [{"selftext": "Some of his early AM5 testing he was running no heatsinks and found the load was so low spread across so many phases that they never really got warm let alone hot. \n\nand in load testing he was basically saying that half could be removed and still even oc the cpu with some heatsinks on the remaining and be ok. \n\nSo basically over spec by quite alot and that can drive prices up for not much gains. \n\nSo when you see cheap mobos with 10 phases its likely just fine and still perhaps overspec even then.", "ups": 1, "depth": 2, "author": "Solarflareqq", "replies": []}]}, {"selftext": "8 layer pcb's should be the standard for EVERY board now, layers matter for everything, phases not so much..", "ups": 1, "depth": 1, "author": "Assationater", "replies": [{"selftext": "I agree but it does drive up costs because people forget high end mobos from the Z77 etc era were like 4 layer pcbs so when you see price creep alot of people dont understand why they could by a premium mobo for 250-300$ in 2013 but now they are 300-500$ for a decent mobo and the 250-300 range is considered low/mid now. \n\nThings like this are why. and the components they are using are alot better , reinforced pci-e slots etc everything is higher spec. \n\nDoes it account for all the cost markup maybe not but alot of it is actually from just better parts and spec.", "ups": 1, "depth": 2, "author": "Solarflareqq", "replies": []}]}]}, {"selftext": "Well done.\n\nI'm honestly thinking of putting the 5800X3D on my B350 strix board I've had since 1600 (currently it has the 3600X).", "ups": 2, "depth": 0, "author": "SUNTZU_JoJo", "replies": [{"selftext": "I got a 5600X in my MSI Tomahawk B350m that I bought for $80 in 2017. Fucking amazing value.", "ups": 1, "depth": 1, "author": "dizector", "replies": []}, {"selftext": "It\u2019s not really worth it tbh unless you get it for cheap or 2nd hand\u2026but I do get the hype being able to put a 5000 series into a 300 series !", "ups": 0, "depth": 1, "author": "Best_Nida_EU", "replies": [{"selftext": "I would argue it's probably a better option to only fork out for a 5800X3D than to have to get a 7000 series chip, new motherboard and 32GB of DDR5..", "ups": 8, "depth": 2, "author": "SUNTZU_JoJo", "replies": [{"selftext": "Like I said it really depends on the price a brand new one cost way too much and investing in am4 is kinda meh.  \nFor example if you have an expensive build (crosshair, low latency ram etc...) 5800X3D make sense, but if not it's not worth it imo I would rather invest in something else like a 7800XT and upgrade the whole kit later and if I'm cpu bound I will just use a 5600 it's only 140$ brand new and if I'm getting a 2nd hand it's even better.", "ups": 1, "depth": 3, "author": "Best_Nida_EU", "replies": [{"selftext": "I get you but it's not 'investing' in AM4..I already earned my 'investment' 2x fold cuz I upgraded from a 1600 to 3600X, so moving to a 5800X3D just makes that deal even sweeter.\n\nIt's a case of (do I spend \u00a3300-350 or do I spend double that.\n\n&amp;#x200B;\n\nI have a 6800XT in my current build so the 5800X3D will fit that nicely.", "ups": 6, "depth": 4, "author": "SUNTZU_JoJo", "replies": [{"selftext": "I'm running a 5800x3d on my x370-i strix board from 2017. Have been using it for nearly 6 years. It has no curve optimizer but I have tuned the voltage and power limits so it wouldn't run so hot. I will run this board to the ground and then get the next gen ryzen upgrade, whatever the name may be. No more asus boards though, fk that company.", "ups": 3, "depth": 5, "author": "dethica", "replies": [{"selftext": "I know what you mean by the ASUS boards, lot of drama but this B350 has been solid throughout tbf", "ups": 1, "depth": 6, "author": "SUNTZU_JoJo", "replies": [{"selftext": "I also had problems with their warranty service when returning a brand new graphics card that was defective. Twice. Took half a year to get a GPU from them that would work. Also them not providing bios updates for relatively new pre built strix gaming desktops - it all adds up.", "ups": 2, "depth": 7, "author": "dethica", "replies": []}]}]}]}, {"selftext": "You clearly don't understand. The less you shelled out for your am4 build, and the older it is, the more 5800x3d makes sense. Sticking to the old platform was never before as beneficial, but right now it happens to be. Wake up.", "ups": 2, "depth": 4, "author": "Cnudstonk", "replies": []}, {"selftext": "You do not need a high end build for the 5800x3d what nonsense is this clearly you have no idea what youre talking about. \n\nI have a old x370 prime pro and my old trusty 16gb 3200mhz RipjawsV that is now running 3600mhz. I literally get to keep my 2017 build and just pay 300 for a CPU that rivals the latest and greatest? \n\nThis vandetta against the 5800x3d interest me based on what do you have this opinion? Clearly you havnt owned it so why the negative view? Is it not fast enough you think or?", "ups": 0, "depth": 4, "author": "MrPapis", "replies": []}]}]}, {"selftext": "What do you mean? The gaming perf will be increased by ~30% for $300, that will literally last another 2-3 years at top tier perf", "ups": 4, "depth": 2, "author": "minhquan3105", "replies": [{"selftext": "I did this upgrade recently (3600 non X however) and I think it's more like 50%, could be wrong though", "ups": 4, "depth": 3, "author": "Donlad8", "replies": [{"selftext": "Exactly, I have no idea what they meant by \"not worth it\"", "ups": 2, "depth": 4, "author": "minhquan3105", "replies": []}, {"selftext": "It depends greatly on the game. I have seen everything from 10-20% to 100% with a similar upgrade. 50-75% in a few of my main programs.", "ups": 1, "depth": 4, "author": "LongFluffyDragon", "replies": [{"selftext": "Oh ok, that's interesting to know thanks. I think I'm fairly GPU limited with a 6800xt at 1440p which is probably impacting my perception of the performance upgrade too", "ups": 1, "depth": 5, "author": "Donlad8", "replies": [{"selftext": "You should definitely see some CPU performance improvements/bottlenecks in many games, by the time you are reaching framerates high enough for a 6800 XT to be fully utilized at 1440p. Depends on the games, a small number just dont care in the slightest about the bigger cache, but Zen3 is still faster overall.", "ups": 1, "depth": 6, "author": "LongFluffyDragon", "replies": []}]}]}]}, {"selftext": "300 dollars for a cpu on a dead platform is pure insanity lol. A ryzen 7600 is 220 dollars and runs neck and neck with it.. pulls less watts while doing it, and smacks it in productivity lol. I do understand that going 7600 and a board and preferably ddr5 is more expensive that slapping a 5800x3d in\u2026 but 300 dollars and still being on an old board with old i/o old pcie gen, and ddr4\u2026 no thanks. Save up and actually upgrade", "ups": -2, "depth": 3, "author": "PenguinsRcool2", "replies": [{"selftext": "Your argument makes no sense so you can get 7600 performance, which is among the best gaming CPU's and you get to keep mobo and RAM saving yourself 400 at the minimum, for equal performance? \n\nYou are totally forgetting that while 5800x3d isnt the fastest CPU its DEFINITELY good enough for a high end system with a high resolution display. No it wont be a 240-360-500hz monster for AAA titles with a 4090 but clearly you dont have experience with the cpu and i can say its FINE for gaming still even wit hmy 175hz 34\". Really only something like Starfield that its showing its age and im still getting 120 when not in New atlantis.", "ups": 2, "depth": 4, "author": "MrPapis", "replies": [{"selftext": "No way its worth it", "ups": -4, "depth": 5, "author": "PenguinsRcool2", "replies": [{"selftext": "I think you have some bias against it and where is it from? Some benchmarks you think are unacceptable? Someones opinion? I mean besides you who doesnt own the CPU and honestly doesnt seem to have any good reason at all?", "ups": 2, "depth": 6, "author": "MrPapis", "replies": [{"selftext": "I own a 5800x3d, and a 7700x, intel 13700k few xeons, 2 epyc cpus. All kinda cpus lol. I\u2019m just saying the price on the 5800x3d is truly insane and makes no sense", "ups": -2, "depth": 7, "author": "PenguinsRcool2", "replies": [{"selftext": "Why? Whats minimum price of an am5 board? And 16Gb of ddr5? Im a 16gb is enough for gaming kinda guy so i will let let that be to your advantage. add to that the 220? of the 7600x or maybe the more comparable 7700x you know being 8 core parts an all that.\n\n[https://www.thefpsreview.com/2022/12/02/amd-ryzen-7-7700x-cpu-review/6/#ftoc-heading-24](https://www.thefpsreview.com/2022/12/02/amd-ryzen-7-7700x-cpu-review/6/#ftoc-heading-24)\n\nAnd then of course we have the best argument of them all in actual gaming, you know not some CPU test to check their ultimate potential under unrealistic conditions, but you know how we use a high end CPU with a powerful GPU and a high resolution display. \n\nAnd what do we see? the 7700x can certainly in some games run away from the 5800x3d. But the games it runs away in makes hundreds of FPS on the 5800x3d anyways and in more normal titles and normal conditions you're under a GPU limit and what we actually see is the 5800x3d even pulls some wins because the GPU is the limiting factor in AAA titles and then the x3d parts has well 3d cache which does have an actual advantage in realistic usecases, so it will end up even coming out on top at times. And seeing as 7600x and 7700x performs incredibly similarly for normal gaming scenarios its more fitting to compare it to the 7700x as it too is an 8 core part. So its really not worse than 7600x or 7700x for gaming but you save 400-500 dollars? \n\nI mean the numbers speak for themselves. It costs less, performs very similarly and its a simple drop in upgrade? Please you need to explain what is insane and makes no sense? Because you are just saying that but its literally meaningless unless you can show me some normal conditions where the 7600x+7700x actually performs twice as good, because thats the price difference we are talking about.", "ups": 1, "depth": 8, "author": "MrPapis", "replies": [{"selftext": "450 usd, 150 for a nice mb, 220 for cpu, 90 for 32 gb ddr5. A 5800x3d is 300 dollars\u2026", "ups": 1, "depth": 9, "author": "PenguinsRcool2", "replies": []}]}, {"selftext": "I agree that it's still pricey. I've built 5800x3d late last year because I've found b550 board for $75 and I already had 48GB of ram (so I've split it between two PCs, 32GB for a new one).  I'd spend a little more for AM5 system today, but I understand everyone who just wants to swap the CPU and keep the rest (because they play cpu-demanding games and don't want to upgrade gpu).", "ups": 1, "depth": 8, "author": "Vonsoo", "replies": []}]}]}]}]}, {"selftext": "and what do you need in addition to those 220? a new motherboard and new memory.\n\nAll to get neck and neck? Not to mention if you play ACC, FS2020, unity games in general, modded games, virtual reality, the 5800x3d is the safer bet. No one cares about pci e gen, that is the least relevant factor of them all.", "ups": 1, "depth": 4, "author": "Cnudstonk", "replies": [{"selftext": "Going forward id say its a factor, for only 150 bucks more you can have 32 gb ddr5, and a new platform, and a better cpu\u2026 vs 300 dollars invested into an ancient board expecting to happily get 6 years out of it, plus in 5 years when you want to upgrade, you\u2019ll have a good ram kit and a good board to do so, and the 7600 cpu will be worth something", "ups": -1, "depth": 5, "author": "PenguinsRcool2", "replies": [{"selftext": "The cpu isn't necessarily even better man. if you're not buying a 7800x3d you have nothing.", "ups": 1, "depth": 6, "author": "Cnudstonk", "replies": [{"selftext": "You have a platform that isn\u2019t dead and ddr5, and a more efficient cpu that is considerably better in most use cases. But you go ahead and spend 300 dollars on an old cpu lol", "ups": 0, "depth": 7, "author": "PenguinsRcool2", "replies": [{"selftext": "but you've invested into what is basically the same thing. i'm done with this stupidity.\nIf you're on zen 1, zen1+, or zen2, it's a slamdunk across the board in every single situation.\n\nIf you play certain games, you can go from zen 3 to 5800x3d and get the same god damn uplift, sometimes even more, but it's not always guaranteed but here is where you might want to do your research.\n\nPlatform being dead means literally nothing when you invest more money into what is slower in MT, and basically on par in games. How in the fuck is that smart? You invest in nothing! Goodbye.", "ups": 3, "depth": 8, "author": "Cnudstonk", "replies": []}]}]}, {"selftext": "We get it; you paid more, got less, and are desperate to justify it by dragging other people down behind you.\n\nSee how nobody is going for it?", "ups": 1, "depth": 6, "author": "LongFluffyDragon", "replies": []}]}]}]}]}, {"selftext": "What? Do you see the prices of AM5 motherboard today? They are crazy, not to forget you need new ram and new cooler to go along with it?\n\nWith only a zen 3 CPU, you get near top chart performance with only a new CPU, granted you already have a good cooler, 5800x3d would give a great performance boost!", "ups": 2, "depth": 2, "author": "AccroG33K", "replies": []}, {"selftext": "Wtf? i have a 5800x3d with a 7900xtx in my system. Very few games need more CPU for 170 FPS. Its definitely a \"fine\" paring for most games. Starfield is not great i will admit, but still 120+ average isnt bad either. Atlantis definitely showing a particular heavy CPU bottleneck.", "ups": 1, "depth": 2, "author": "MrPapis", "replies": []}]}, {"selftext": "Not a bad idea, the R7 5800X3D comsumes almost 100W at 100% and much less while gaming, the only issue would be getting a proper cooler for it unless you invest some time undervolting it.\n\nAlso the performance gain will be closer to 50% depending on the games you play so it's very worth it if you want to keep the build for longer.", "ups": 1, "depth": 1, "author": "Pl4y3rSn4rk", "replies": [{"selftext": "I've currently got a DH-14 with the dual fans, so I think that's plenty cooling TBH.\n\nIt's mostly for Tarkov TBH..fps gains are insane for that.", "ups": 2, "depth": 2, "author": "SUNTZU_JoJo", "replies": [{"selftext": "Yep it's plenty, still you could try using PBO Curve Optimizer to reduce the voltage a lil bit, it'll make the R7 clock a tad higher.\n\nAnd most games that scale well with faster RAM will see a pretty good uplift with the massive L3 Cache from the X3D. I intend to get one because I play Rust and it also loves the extra cache just like Tarkov.", "ups": 1, "depth": 3, "author": "Pl4y3rSn4rk", "replies": []}]}]}, {"selftext": "My board, my first cpu, my second cpu, and my last cpu! Has been a great run.", "ups": 1, "depth": 1, "author": "Formi", "replies": []}, {"selftext": "The VRMs on it are decidedly bad, but it should be ok with a good air cooler to keep them from heating up.", "ups": 1, "depth": 1, "author": "LongFluffyDragon", "replies": []}]}, {"selftext": "Who claims it's bad? Thing only 100w, and if you power limited to 80w you could probably still push clocks a little bit.", "ups": 0, "depth": 0, "author": "bubblesort33", "replies": [{"selftext": "Reviewers on the net and youtube. They might make users regret their choices or not consider these boards.", "ups": 1, "depth": 1, "author": "SuccotashAdditional", "replies": []}]}, {"selftext": "You might think it\u2019s fine but your motherboard mosfets and inductors are having a meltdown. I would recommend turning on Eco Mode at least.", "ups": 0, "depth": 0, "author": "Depth386", "replies": [{"selftext": "By the inductor do you mean the choke? Because they should be fine, it's really the mofsets that are more concerning.", "ups": 1, "depth": 1, "author": "SuccotashAdditional", "replies": []}]}, {"selftext": "Let's see how long that board lasts.", "ups": 0, "depth": 0, "author": "FcoEnriquePerez", "replies": []}, {"selftext": "you might be fine for now...but 85C is pretty toasty. I would look to reduce the PPT a bit more and you'll be surprised at how little performance you lose. It probably won't even be noticeable. Try an undervolt as well...negative offset in curve optimizer in PBO or AMD overclocking", "ups": -8, "depth": 0, "author": "ofon", "replies": [{"selftext": "85C is fine for CB.", "ups": 12, "depth": 1, "author": "gusthenewkid", "replies": []}, {"selftext": "Cheapest board equals, these option does not exist. And 85C is fine. By design they support 95C", "ups": 9, "depth": 1, "author": "SuccotashAdditional", "replies": [{"selftext": "Mine ( non x) hits 97c under full load in games like starfield I\u2019m ordering a AIO soon", "ups": 0, "depth": 2, "author": "awed7447", "replies": [{"selftext": "I am on air cooler.", "ups": 1, "depth": 3, "author": "SuccotashAdditional", "replies": [{"selftext": "Anything significant would make the experiment a bit moot anyway lol.\n\nAppreciate the look into this. I've got a ton of old chip heatsinks around. Not sure how to attach them mind lol", "ups": 1, "depth": 4, "author": "TT_207", "replies": []}]}]}]}]}, {"selftext": "My 7600x with a 90w ppt, -35 curve optimizer offset beats the stock clocks by 200mhz while running 20\u00b0C cooler and drawing 25w less power.", "ups": 1, "depth": 0, "author": "Pro4791", "replies": []}, {"selftext": "The key is to get an itx board which has the better cooler and signaling required for that small board. The A520i AC I have also had the Wi-Fi updated to 6e via M.2 card. I run a 5800x3D now, but the 5950x had no issues. I can adjust TDC and other levels, but no - offsets. \n\nThe current B620 version is the UD A620I-AX. Not sure if there are downgrades vs 500 series, but VRM seems ok.  I\u2019ll go this route again for mid-late zen4 upgrade.", "ups": 1, "depth": 0, "author": "jassco2", "replies": []}, {"selftext": "who ever claimed it would be bad? you even can put a 7800x3d on the cheapest am5 board and it will run fine.", "ups": 1, "depth": 0, "author": "mkdr", "replies": [{"selftext": "It won't have enaugh PPT to go to intended power thought.", "ups": 1, "depth": 1, "author": "SuccotashAdditional", "replies": [{"selftext": "nonsense. you can run a 7800x3d with ppt 75w and pbo -20 and it will run full power and get same if not better results, for example I get 18400 in cb r23 with just 75w ppt. a 7800x3d wont use more than 85w ppt which is totally fine for the lowest boards you can buy. for gaming you can even lower it to around 45w.", "ups": 1, "depth": 2, "author": "mkdr", "replies": []}]}]}, {"selftext": "tbh its a 6 core that doesnt consume much, 6 core will work on shitty A620 (every reputable A620 brand should meet a minimum power delivery spec, and throttle the CPU if it cant handle it at full power)", "ups": 1, "depth": 0, "author": "maze100X", "replies": []}, {"selftext": "I've always been a fan of cheapest possible boards. Audio is crap, but I prefer to use &gt;$100 external DAC/AMP which can still be used after upgrading PC, sound quality matches most expensive motherboards (or you can go crazy and get really good one if you have FLAC music collection).\n\nRecent experience:\n\n1. 2017 intel i7-7700k on a cheap Z-270 board. It's still running at 4.9Ghz (4.2 stock) on air cooler (reported temps are 90-100C but so what, it's like that for last 5 years).\n2. 2022 amd 5800x3d on an asrock b550 for $75 (inside a case from 2011). It runs PBO2 -30 all cores and I have not seen any games crashing. Air cooler, super quiet PC (I let it heat up to 70C and ramp up fans later).\n\nOnly drawback of a cheap board is poor audio quality, but just buy nice USB DAC (or internal sound card) and you are fine.\n\nYou may also want to have two nvme slots running full speed, but I usually play 1-2 games and I just keep them on the fastest system nvme, second one is for files, photos etc.", "ups": 1, "depth": 0, "author": "Vonsoo", "replies": []}, {"selftext": "I'm using the same motherboard with 7800x3d. No issues at all.", "ups": 1, "depth": 0, "author": "Mansour449", "replies": []}, {"selftext": "And you can always undervolt. I can\u2019t remember how many watts my 5950x was using before/after undervolting but I do remember it running significantly cooler. My 13700k pc that I use daily is dropped thirty watts at full load with a quick and dirty undervolt and is stable. It should be noted that I only undervolted the 13700 because I thought I still had a noctua d15 in a box somewhere but turns out it\u2019s in my sons pc and all I had laying around was a be quiet shadow rock 3 that is just barely keeping temps under control", "ups": 1, "depth": 0, "author": "khristopkel", "replies": [{"selftext": "Undervolting on a board is good practice when you know how. \nI did it with an offset since PBO is unavailable.", "ups": 1, "depth": 1, "author": "SuccotashAdditional", "replies": []}]}, {"selftext": "I'm using a Asus A320(with BIOS UPDATED) with R5 5600 stock + 32gb ram 3200mhz with no issues for a long time now. I dont plan to overclock anything, so no problems whatsoever.", "ups": 1, "depth": 0, "author": "PsYcoot1c", "replies": []}, {"selftext": "I'd stir away from motherboards that have no VRM cooling. I'd also heavily recommend against them for a multitude of reasons. Even if you're saving just 20 bucks, it's not worth it for the long run. Not everyone has open bench setups or open case setups or PCs placed in ideal conditions with good cooling. And while this may function well for a while, it most likely will not be the same a year later. (I'm talking about the average user here)\n\n&amp;#x200B;\n\nFor short test bursts the motherboard could be fine but if you're playing any game that puts proper load on the CPU and you play it for more than 20 minutes, that CPU's performance will drop and those VRM's will have shorter life time.", "ups": 1, "depth": 0, "author": "fogoticus", "replies": []}, {"selftext": "And here I am with my R5 4500 rocking solid. I can\u2019t imagine a regular user nowadays would need so much performance\u2026", "ups": 1, "depth": 0, "author": "Confident_Visit1358", "replies": []}, {"selftext": "A cheap motherboard li9ke that will likely start throttling in long workloads.\n\n&gt; I am able to keep a clock of 5.2 GHz in boost for a multicore result of 15083 with a temp in the vicinity of 85C.\n\nCPU temps don't matter when it comes to the motherboard, the real issue is the motherboard's VRMs, do a loop of cinebench for a long period of time to stress the VRMs, because of no heatsinks on the VRMs you'll likely see them throttle.", "ups": 1, "depth": 0, "author": "tpf92", "replies": []}, {"selftext": "I think you meant \"concede,\" not \"consent.\"", "ups": 1, "depth": 0, "author": "PM_ME_YOUR_SSN_CC", "replies": []}, {"selftext": "Afaik there has been a lot of talk about how great disparity there is between motherboards regarding memory timings. Have you also checked/compared ram timings / performance with other better boards? [https://www.youtube.com/watch?v=UN2gkbMQ2fs&amp;t=892s](https://www.youtube.com/watch?v=UN2gkbMQ2fs&amp;t=892s)\n\nWhile i don't think there is anything  wrong with going with budget cpu/mobo, i believe its also worth considering the potential of that choice limiting your future upgradability since many people choose amd (also) for socket longevity.", "ups": 1, "depth": 0, "author": "No_Guarantee7841", "replies": [{"selftext": "64MB / s read on aida this is 15% increase over my last DDR4 kit but I did not compare that corsair 6000mhz kit with other system.", "ups": 1, "depth": 1, "author": "SuccotashAdditional", "replies": []}]}, {"selftext": "I\u2019d say the only problem with it is that most a series boards are seriously lacking in VRMs. While this obviously doesn\u2019t matter for a 7600x, if you wanted to upgrade to an X3D processor in the next generations it may be problematic. Similar to some a320 boards struggling to run 5800x3Ds.", "ups": 1, "depth": 0, "author": "X_SkillCraft20_X", "replies": [{"selftext": "No need to upgrade to x3d they are overpriced and overhyped.", "ups": 0, "depth": 1, "author": "SuccotashAdditional", "replies": []}]}, {"selftext": "I was raving about a520 being all cool for a year or so, now I can't overclock my 5600x and it makes me tepid about 5800x3d so I might be stuck here.", "ups": 1, "depth": 0, "author": "byteframe", "replies": [{"selftext": "5800x3d too expansive for gaming only. No point.", "ups": 1, "depth": 1, "author": "SuccotashAdditional", "replies": []}]}, {"selftext": "Can you throw ecomode on the 7600x on the A620?", "ups": 1, "depth": 0, "author": "TheK1NGT", "replies": []}, {"selftext": "Mind-blowing. The 7600X is compatible with a motherboard that says it'll work with the 7600X?  Never would have thought.", "ups": 1, "depth": 0, "author": "tfwtaken", "replies": []}]}
{"post": {"title": "My experience with 7800x3d so far", "subreddit": "Amd", "selftext": " Posting this here with the hopes someone has a micracle fix for this, because im honestly starting to deeply regret buying it, intermitent micro stuttering on every single game, tried everything to fix but nothing worked so far, the 0.1% lows are extremelly low in 90% of games, theres always what looks like a timed frame pace spike and i cannot ignore it. Tried everything i could, formating, disabling ftpm, DDU stuff, BIOS updated, expo enabled, temps are fine.\n\nIm able to return it, and im seriously thinking of doing it and go intel, unless someone gives me a miracle solution to my problem.\n\nW11\n\nr7 7800x3d\n\ntuf b650 gaming plus wifi\n\ntuf 4090\n\n2x16 corsair vengeance expo 6000mhz cl30\n\n2x nvme drives\n\n1200w gold corsair shift", "ups": 57, "permalink": "/r/Amd/comments/16obb13/my_experience_with_7800x3d_so_far/", "num_comments": 249}, "replies": [{"selftext": "Did you do a fresh install of windows when you moved to this CPU?", "ups": 56, "depth": 0, "author": "dirthurts", "replies": [{"selftext": "To expand on this, and to repeat anecdotal information from this and other subs, doing a fresh install after upgrading one's license from 10 to 11 seems beneficial in many situations.\n\nAlso, if OP was moving *from* Intel to AMD and is using the *same installation* of Windows without reinstalling Windows (regardless of installing AMD chipset drivers) then that's almost always a no-no.\n\nLastly, and I don't know if MS has fixed this with Windows 11 yet, but there were supposedly issues with core/thread assignment if one went from a non-X3D AMD CPU to a new X3D one leading to, again, a re-install of Windows 11.\n\nAs for my own situation(s) with Windows 11, and on mobile Intel *10th* gen, I have a laptop that go updated to 11 against my wishes (***because MS can't help but misrpresent the update process of 10 to 11 in Windwos Update these days...***). I did end up not having any current issues with that system as far as gaming performance (*or perf. in general*) so, again, I don't know if MS has made some improvements to the process of going from 10 to 11 or not as I only have a sample of one to reference.", "ups": 28, "depth": 1, "author": "HateToShave", "replies": [{"selftext": "Caveat to \"Almost Always\".\n\nI had a i5 3470k and a 1060 on a freshly installed Windows 10 (was deperate trying to get more performance cus it was showing its age quite hard).\n\nI upgraded to a whole new setup with a Ryzen 7600 and a 6750XT, and just plugged in all my old SSDs.\n\nI fully expected to have to install Windows again, but lo and behind the PC booted before I could enter the Bios and \"Just Worked\".\n\nSooo...  I ran DDU to uninstall the old drivers, updated all the AMD drivers for mobo/gpu, turned on XMP and the CPU overclock profile and even updated windows to 11 (again, I was fully expecting to have to install windows again, so why not?).\n\nIt has been running for a few months now, everything runs great within the expected performance, boots in less than 20s, and has been overall very stable (only thing that gave me trouble was Metro Exodus really not liking DirectX12 for some reason).\n\nCould be that the intall itself was less than a month old when I changed PCs, or I have been very lucky.", "ups": 6, "depth": 2, "author": "DomeShapedDom", "replies": [{"selftext": "Many reviewer saw performance losses if not doing a fresh install between systems.  It wasn't noticeable in real world but fairly significant in benchmark results. I don't remember specifics, but a fresh install always gives best results.", "ups": 8, "depth": 3, "author": "farmeunit", "replies": []}]}]}, {"selftext": "There's a windows feature under backup &amp; restore that allows windows to do a clean install for you while preserving your files. I went with that option and had it pull the latest windows version from the magical cloud.", "ups": 3, "depth": 1, "author": "segfaultsarecool", "replies": []}, {"selftext": "&gt;Did you do a fresh install of windows when you moved to this CPU?\n\nYes", "ups": 3, "depth": 1, "author": "ldontgeit", "replies": []}, {"selftext": "Is it a good or bad thing, doing a fresh install?", "ups": 2, "depth": 1, "author": "Chakluxe", "replies": [{"selftext": "Very good. If you change your CPU you have to. Generally.", "ups": 2, "depth": 2, "author": "dirthurts", "replies": [{"selftext": "I had blue screens but they stopped after I removed the Intel drivers.", "ups": 2, "depth": 3, "author": "Liam2349", "replies": []}, {"selftext": "Even from, say, Ryzen 2400g to say 5700g ?", "ups": 1, "depth": 3, "author": "mediandude", "replies": []}, {"selftext": "Generally speaking yes. If You change chipsets or cpu of the same vendor then it may not be needed. I used 1920x then switched to 3800x on windows 10 and it worked fine. Of course chipset drivers update is a must. \nIf You encounter any problems as said above, fresh install may be the only way to go.", "ups": 1, "depth": 3, "author": "BoskiCezar", "replies": []}]}]}, {"selftext": "This! What about clock speeds? What are those 8 cores running at while gaming?", "ups": 4, "depth": 1, "author": "Jetcat11", "replies": []}, {"selftext": "I recently moved from Intel to amd 7800x3d and didn't need to reinstall windows.\nJust updated the bios got the new amd chip set drivers and everything was fine", "ups": 2, "depth": 1, "author": "GPopovich", "replies": []}]}, {"selftext": "This is mostly useful for audio issues (such as popping, crackling, etc) but maybe it'll help. Download and run Latencymon:\n\nPress the Play button to start monitoring your system\n\nSwitch to the Drivers tab and sort by Highest execution (ms)\n\n&amp;#x200B;\n\nNow just use your system until the microstutters occur again. After that, check the list of drivers for those with the greatest Highest execution time. Maybe that'll point you in the right direction.", "ups": 62, "depth": 0, "author": "knexfan0011", "replies": [{"selftext": "I had to do this one time and found out it was some RGB software setting. I hate rgb", "ups": 13, "depth": 1, "author": "HomerPimpson304", "replies": [{"selftext": "What RGB software was causing this issues?", "ups": 1, "depth": 2, "author": "ldontgeit", "replies": [{"selftext": "I would not be surprised if the answer was all of them did this", "ups": 7, "depth": 3, "author": "ExtremeDude2", "replies": []}]}, {"selftext": "What rgb app?", "ups": 1, "depth": 2, "author": "cdodge18", "replies": []}]}, {"selftext": "Thats a really smart idea, will try thank you", "ups": 9, "depth": 1, "author": "ldontgeit", "replies": [{"selftext": "I wish I had an approach as well thought out as the comment above yours when I was trying to figure out similar issues, OP.\n\nI was having the same symptoms as you with a 5600/6800xt combo. It was actually the lesser issue so I can't say i have a troubleshooting guide for you. My main issue with just catastrophic driver crashes when I first put everything together. I took the nuclear approach and just wiped all the drivers. What solved my main issue was disabling MPO in windows. I'm not sure if that's what also solved my stuttering issues but come to think of it, I didn't really encounter them after that fix either. Give it a try, friend.", "ups": 4, "depth": 2, "author": "Inside-Line", "replies": []}, {"selftext": "Uninstall and reinstall your chipset driver", "ups": 3, "depth": 2, "author": "raidechomi", "replies": []}]}, {"selftext": "Thank you so much. My 5800x3d woes hope this will help.", "ups": 2, "depth": 1, "author": "Gallieg444", "replies": [{"selftext": "so you did the MPO disable? I commented on this thread before with same stutter on a 5800x3d system no matter the video card.", "ups": 1, "depth": 2, "author": "natethegreat_ttv", "replies": []}]}, {"selftext": "I have the 7800X3D and I'm having issues when I tab in and out of a game .. I get crackling sounds and after a few times it will spass out! \n\nThe only way to fix it is when I reboot my pc.", "ups": 3, "depth": 1, "author": "Iddqd84", "replies": [{"selftext": "Highly doubt it\u2019s the cpu\u2026", "ups": 3, "depth": 2, "author": "AWeisen1", "replies": []}, {"selftext": "did you try DDU?", "ups": 1, "depth": 2, "author": "Academic_Addition_96", "replies": []}, {"selftext": "I saw a youtube video of this guy that pushes amd cards so i dont know how accurate it is but that is what he was stating was something with nividia causes a delay in atl tab out of full scren games, apparently it doesnt do that in windowed games. so you could try that and see if it stops.", "ups": 1, "depth": 2, "author": "Eagle1967", "replies": []}]}, {"selftext": "This is excellent, my 5800X3D machine had microstutters. Turns out I cannot use 2x 8000Hz polling rate USB devices at the same time.", "ups": 3, "depth": 1, "author": "Alauzhen", "replies": []}, {"selftext": "I\u2019m bookmarking this. Keep getting microstutters in Forza. May have to see what\u2019s going on.", "ups": 1, "depth": 1, "author": "EnteiCosmos", "replies": [{"selftext": "I think the microstutters are Windows related because I get them on an i7 10700k and my 7800x3d in Warzone and MSFS 2020", "ups": 2, "depth": 2, "author": "CherokeeCruiser", "replies": []}]}]}, {"selftext": "Did you try stress testing the rig (core cycler, occt, memtest, memtest vulcan) ? Sounds like something might not be completely stable.", "ups": 20, "depth": 0, "author": "SeveralMight7560", "replies": [{"selftext": "Yeah, if memory serves all ddr5 is ecc right? so if an error did happen it could cause an issue like this but keep the PC from crashing...", "ups": 1, "depth": 1, "author": "Mauicez", "replies": [{"selftext": "Do not confuse the on-stick ECC that most consumer ddr5 has with full path ecc where ECC data makes it all the way to the CPU. :x", "ups": 3, "depth": 2, "author": "Kiseido", "replies": []}]}]}, {"selftext": "Are you using MSI afterburner for OSD ? That was the cause for me.\n\nI now use CapframeX for OSD, and I no longer get stuttering.\n\nOr you can try with nvidia GFE or intel presentmon if you don't want RTSS.", "ups": 20, "depth": 0, "author": "lexsanders", "replies": [{"selftext": "&gt;MSI afterburner for OSD\n\nYes, but the thing is, even with afterburner closed, the same issue happens", "ups": 1, "depth": 1, "author": "ldontgeit", "replies": [{"selftext": "well if you have a spare ssd, just install a fresh windows trial for 4 hours.\n\nIf it's good on a fresh windows, it means your current windows has some bad setting.\n\nWhich one, couldn't tell you, but at least you will know the truth that the hardware is fine and the software is bad. Or the opposite could happen and you will still get stutter.\n\nJust make sure to no install too much crap on the fresh OS, jsut drivers and GFE or capframex.", "ups": 10, "depth": 2, "author": "lexsanders", "replies": [{"selftext": "Yeah. I recently had a Windows update which introduced weird lag and stuttering. I reverted, and it was fine. I don't know if this is the case here, but it would be a good idea to make sure.  I would also run dism /online /cleanup-image /restorehealth, and sfc /scannow, just to see what happens too.", "ups": 1, "depth": 3, "author": "RippiHunti", "replies": [{"selftext": "I am speaking from experience because I bought over 1000$ worth of new components and reinstalled windows twice, and it was MSI afterburner stuttering all along....", "ups": 9, "depth": 4, "author": "lexsanders", "replies": []}]}]}, {"selftext": "Afterburner really screwed with my 7800xt drivers. Even after uninstalling and reinstalling/repairing the original ones I've had issues", "ups": 3, "depth": 2, "author": "madrussianx", "replies": []}, {"selftext": "How about resetting windows to its basic factory standards? No reinstall needed. This feature got a lot better since Win10, no experience with Win11 though.", "ups": 1, "depth": 2, "author": "domzen", "replies": []}, {"selftext": "Do you have any RGB control software running? Like L-Connect? Anything at all running in the background?", "ups": 1, "depth": 2, "author": "ZaiCCe", "replies": []}]}, {"selftext": "I\u2019ve been wanting to try PresentMon or CapFrameX for a few months now, haven\u2019t had time though.\n\nDo either of these allow you to set up simple text overlays like RTSS? I know you can customize RTSS but I\u2019ve always liked the simple and clean look. Basically no graphs or backgrounds.\n\nMy RTSS looks something like this:\n\nGPU: (temp\u00b0) (% utilized) (# watts)\n\nCPU: (temp\u00b0) (% utilized) (# watts)\n\n(# FPS) (# 1% low)", "ups": 1, "depth": 1, "author": "w5vaBDrBK2wryfX9", "replies": []}]}, {"selftext": "Have you tried running memory with EXPO/DOCP off (essentially at base settings?) \n\nThat sounds like an issue I had with bad ram OC on my 7800x3d\n\nTry running a few memory tests.", "ups": 8, "depth": 0, "author": "eudisld15", "replies": []}, {"selftext": "I had the same issue right after upgrading to AM5 with a 7800X3D. Tried everything you mentioned, and reinstalled Windows twice. I then noticed the stuttering only happened when my AMD metrics overlay was on. Turn the overlay off, the stuttering was gone. I wondered why it was doing that, but at least I had a temporary fix. I figured it was an Adrenalin bug and would be patched.\n\nTurns out it was my L-Connect software to control my Lian Li Strimer RGB extensions for my PSU cables. It would only cause an issue if I had the AMD metrics overlay on. Uninstalled L-Connect, problem went away. I just use SignalRGB now to control all my RGB peripherals lol", "ups": 9, "depth": 0, "author": "RedChaos92", "replies": [{"selftext": "&gt;Turns out it was my L-Connect software \n\nI have L-connect with a controller plugged into mb usb header to control my unifans, think its that?", "ups": 4, "depth": 1, "author": "ldontgeit", "replies": [{"selftext": "You seem to have literally every single piece of software and hardware known to cause stuttering...  like...  an insane library of a million different things that are all known to destroy your system...  (I'm exaggerating...  actually only 999,998)\n\nWhy not start with a clean install, clean everything.  Maybe a version or two back on gpu drivers but that shouldn't be too much of a problem, but get rid of everything but bare windows.", "ups": 11, "depth": 2, "author": "cp5184", "replies": [{"selftext": "This is good advice. It's how I figured out L-Connect was causing my stuttering. Had no stuttering with just barebones windows and my graphics driver. Installed my software and drivers one by one, restarting each time and testing games. Stuttering reappeared after L-Connect was installed.", "ups": 2, "depth": 3, "author": "RedChaos92", "replies": []}]}, {"selftext": "Couldn't hurt to try just to be sure. I know you're using an Nvidia GPU whereas my problem only happened with AMD overlay on, but who's to say L-Connect doesn't have issues with other programs as well?\n\nWorth a shot!", "ups": 1, "depth": 2, "author": "RedChaos92", "replies": [{"selftext": "Dont have to unistal just close the app right?", "ups": -2, "depth": 3, "author": "ldontgeit", "replies": [{"selftext": "Usually those programs have a background service that keeps running even if you close the program. You could find and disable that service, uninstall is more thorough though", "ups": 8, "depth": 4, "author": "NewestAccount2023", "replies": []}, {"selftext": "Just close it.  Change to not start with windows, it doesn't need to always be running.", "ups": -1, "depth": 4, "author": "BinaryJay", "replies": []}]}]}]}, {"selftext": "FYI the controller will remember all your settings from L-connect you only have to run it once, or once again after the PC completely loses power due to unplugging, switching PSU off or power failure.  I don't know if this is the case if you use all the stupid animations but works for me with static lights.", "ups": 2, "depth": 1, "author": "BinaryJay", "replies": [{"selftext": "I manually added my Strimers into SignalRGB so it recognizes them as soon as SignalRGB starts. Works for static or animated lighting. Haven't had any issues with recognition even after unplugging my PC to replace my GPU thermal paste with a thermal pad.", "ups": 1, "depth": 2, "author": "RedChaos92", "replies": []}]}]}, {"selftext": "What mouse are you using out of curiosity?", "ups": 5, "depth": 0, "author": "neueziel1", "replies": [{"selftext": "two weeks ago my PC was \"lagging\", more like micro stutters every like ... 5 sec or so. Nothing was helping, i restarted whole PC like 15 times, checked if there is not an windows update going on, antivirus etc... after 2 days i realized that my wife was cleaning my desk and moved that bluetooth thing from my pro superlight mouse far away from mouse. It was not disconnecting mouse but did those weird micro stutters.\n\nMoved it closer again and bang, PC working smooth again.", "ups": 9, "depth": 1, "author": "sulowitch", "replies": [{"selftext": "For me I have a razer with 8k polling. Some games don\u2019t like that and cause stutters to happen. I lowered it to like 2k and it\u2019s perfect.", "ups": 6, "depth": 2, "author": "neueziel1", "replies": [{"selftext": "even 1000 polling rate was doing problems to CPU if i remember. it was stressing CPU too much back like... 3-4 years?", "ups": 4, "depth": 3, "author": "sulowitch", "replies": [{"selftext": "Nah more like 10 years ago. A lot of older games from early 2000s don't like more than 125/500Hz rates.", "ups": 3, "depth": 4, "author": "Fezzy976", "replies": []}, {"selftext": "Yeah, I had that issue and was confused because it persisted on both Windows and Linux. Only after googling, I found out that 500 Hz is the optimal value for a 6-core CPU if you want to avoid stutters. That was 5-6 years ago, though.", "ups": 2, "depth": 4, "author": "LoafyLemon", "replies": []}]}]}]}]}, {"selftext": "Had the same with my 7800x3d and 3090. The fix was plugging my 2nd and 3rd Monitor into the iGPU of the Ryzen.\nOn my side it was a GPU issue", "ups": 5, "depth": 0, "author": "Noah0302kek", "replies": [{"selftext": "thanks for the tip, i actually have 3 monitors, gona check this once im back from work", "ups": 2, "depth": 1, "author": "ldontgeit", "replies": [{"selftext": "Check Nvidia setting panel for what your power profile is set to. `Multi-monitor performance` is i think what you want. \n\nBut being as this is an amd forum, you may not find that much useful help if this is caused by Nvidia software / hardware", "ups": 2, "depth": 2, "author": "Kiseido", "replies": []}, {"selftext": "Stripe the system down to bare bones to troubleshoot.  All USB, monitors, hdd etc", "ups": 1, "depth": 2, "author": "chemie99", "replies": []}]}]}, {"selftext": "You try aida64 to check if you have any throttling?", "ups": 2, "depth": 0, "author": "aaadmiral", "replies": []}, {"selftext": "Try this: https://www.igorslab.de/en/interrupt-problems-read-message-signaled-interrupts-msi/", "ups": 3, "depth": 0, "author": "wertzius", "replies": [{"selftext": "tried msi v3 ON and OFF, literally zero diff", "ups": 3, "depth": 1, "author": "ldontgeit", "replies": [{"selftext": "Was worth a try... I do not encounter your problems with my 7800X3D.", "ups": 3, "depth": 2, "author": "wertzius", "replies": []}]}]}, {"selftext": "I would try booting into Linux, just to rule out a potential driver issue on Windows' side. If the issue turns out isolated to just Windows, then I would try reinstalling the system.", "ups": 3, "depth": 0, "author": "LoafyLemon", "replies": []}, {"selftext": "Turn down the polling rate of your mouse to 250hz and see if that helps.", "ups": 3, "depth": 0, "author": "hanbaoquan", "replies": []}, {"selftext": "I've had my own similar annoyance although it's a bit stranger. I have 7800X 3D with a 4090 (Strix X670E A mobo) and it works great  no complaints but I also have a Nitro 7900XTX and whenever I put the 7900XTX in my 7800X 3D system and record gameplay footage using Relive theres a very annoying frequent stutter every 10 seconds. It happens if I record and disabling SAM fixes it.\n\nI have a Ryzen 5900X system and it works just fine on that system.", "ups": 6, "depth": 0, "author": "Loku184", "replies": [{"selftext": "a lot of people report the same issues..RDNA 3 is a step in the right direction, but typically with more sudden complexity, more issues arise. Radeon had a lot of ambitions for this generation of GPUs and while we saw huge performance increases, that also came with a price hike and problems such as idle power high video playback power usage etc etc.  \n\n\nStuttering while recording is one of the few things quite a few people report. Are you by any chance not using an SSD to record your gameplay?", "ups": 2, "depth": 1, "author": "ofon", "replies": [{"selftext": "My 7800X 3D is all SSD. 2TB Samsung 980 Pro gen 4 nvme for OS (Windows 11 Pro), another 2TB 980 Pro for games and recording footage and I have another 2TB sata ssd as well.\n\nI tried changing recording location from the OS drive to my secondary and it's the same thing.", "ups": 1, "depth": 2, "author": "Loku184", "replies": []}]}]}, {"selftext": "Try to monitor ram usage and check if there is hardware reserved. There is a bug where bios just reserved half your ram for iGPU.", "ups": 2, "depth": 0, "author": "zatagi", "replies": []}, {"selftext": "Try enabling or disabling SAM", "ups": 2, "depth": 0, "author": "OptionPleasant7133", "replies": []}, {"selftext": "Do you have some weird stuff installed that runs on startup?\n\nAnd I have to ask, do you have the RAM sticks installed in correct slots, #2 and #4?", "ups": 2, "depth": 0, "author": "Mungojerrie86", "replies": []}, {"selftext": "I had similar issues for ages on my old cpu - (5800x3d - yes it\u2019s a proper version but these are relevant) what worked for me:\n- install the motherboard drivers from the motherboard website ( NOT the default windows ones). This includes Bluetooth, network, onboard sound etc\n-disable all hardware monitoring software ( for me it was GPU monitoring by hwinfo that was also causing it).", "ups": 2, "depth": 0, "author": "dUcKy1010", "replies": []}, {"selftext": "Is this with any kind of undervolt? What you are feeling as stutter might be clock stretching, which is more of a total system unresponsiveness (even freezes the mouse cursor)\n\nWith my 5800X3D I had clock stretching at -30 all-core, I had to go to -30, -30 and -25 on the other 6 cores.", "ups": 2, "depth": 0, "author": "RedTuesdayMusic", "replies": []}, {"selftext": "There is something very wrong here and it isn't your cpu", "ups": 2, "depth": 0, "author": "R1Type", "replies": []}, {"selftext": "Man, just turn off pc, cut power to it, disconnect all componets,  all cables, levebit for like 5 minutes be, put them back in the case, do a clean windows 10 install via USB thru bios. After, only do Windows updates, don't install any thing else. Once all windows Updates  finish, restart pc, install gpu driver, restart PC, Once that's done, install newest chipset, then manage your own bios settings (xmp, fan curve, resizable bar...) and download your fav game and try it again. \n\nMonitor temps and all (msi afterburner, open Hardware monitor). For perfect in detail fan curve, there are dozens of so called \"system\" or \"fan\" controllers. But malt are paid, suck ass, don't work with windows 10, or are just for the gpu like MSI afterburner. For cpu and case fans, use Argus Monitor. That app let's you set fan curves even if your mobo has faulty cpu temp sensor like mine. That app saved me big time!\n\nAnyway. All do clean, try.... And if you still have issues, it's something in one of the components or the game, or drive nor internet or idk.", "ups": 2, "depth": 0, "author": "Killua_Zaeldyeck", "replies": []}, {"selftext": "My setup only differs that o choose to use 4x8gb 6000mhz. I have some issues at 6000mhz on memory, so i lowered to 5800 and its rock solid, and im using pbo that keep 5ghz and undervolted.\nThe memory is the problem with the X3d, we have to find the spot.", "ups": 2, "depth": 0, "author": "TopCell8018", "replies": []}, {"selftext": "Hi, I have a 5800x3D and before I used to get consistent split second freezes, hitches, what have you. Not on every game, but it was enough to bother the shit out of me.\n\nI fixed it by disabling hardware acceleration in chrome and turning off SAM in bios.", "ups": 2, "depth": 0, "author": "Holesnifferboy", "replies": []}, {"selftext": "**This is due to a bug somewhere in either the AMD chipset drivers and/or Windows related to high USB mouse polling rates.**\n\nI have run into this problem on my setup in the past. No idea what triggers it, but it started after installing the latest chipset drivers available at the time.\n\nI could induce or prevent the poor framepacing by going between chipset driver versions. I then realized that the framepacing was good *as long as I wasn't moving my mouse.* As soon as I was moving my mouse, the framepacing went to shit.\n\nIf your system acts the same way, that's likely what you have going on.\n\nI used Process Hacker to narrow the problem executable down to csrss.exe, specifically the winsrvext.dll!UserServerDllInitializationExt thread, and at that point determined it was high USB mouse polling rate causing huge I/O spikes, and subsequently, frametime spikes.\n\nReduce your mouse polling rate, and the problem *should* go away.\n\nIf that doesn't immediately solve the problem, also try uninstalling your AMD chipset drivers and reverting to older chipset drivers. I also have an ASUS board, and I found that the older chipset drivers from the support page for my board on ASUS's website did not have the bug, whereas the latest drivers from AMD did have the bug.", "ups": 2, "depth": 0, "author": "Rockstonicko", "replies": [{"selftext": "Thank you for your feedback, i have 2 mouses, g305 and roccat kone air pro, i will try process hacker and see if i can find the culprit.", "ups": 2, "depth": 1, "author": "ldontgeit", "replies": [{"selftext": "Hopefully that's all it is, and if that is the problem, I know how frustrating it is. It followed me across Windows installs as well, because I had the USB polling rate saved to the profile on my mouse's onboard memory.\n\nNot sure if it's ASUS specific or what, but I made a Reddit thread asking if anyone else was having the issue with poor framepacing with the newest chipset driver and I got a huge nada.\n\nIf you start up Process Hacker, on the main \"processes\" tab, add the column \"I/O total rate\". \n\nSort your processes by \"I/O total rate,\" then start furiously moving your mouse around. If you see csrss.exe spiking up to \\~70-100KB/s, and moving up the list, then you have the polling rate bug, and you need to reduce the USB polling rate on one or both of your mice.\n\nIf csrss.exe is functioning normally, you should only see something like 5-15KB/s max of I/O activity regardless of how much you fling the mouse around.", "ups": 2, "depth": 2, "author": "Rockstonicko", "replies": []}]}]}, {"selftext": "Take a look at these [spikes](https://youtu.be/B31PwSpClk8?t=716).  The 7800x3d is not new to this type of behaviour based on the benchmarks in this videos.  Everyone is so focused on AVG FPS, and not looking at the frame consistency or 1% lows, and just all the little nuances that an AMD system brings.  I actually own a 7950x3d, and I have followed windows setup tutorials to the \"T\", verified core parking is working correctly, minimal changes in the BIOS (enable XMP only), and I am still never confident that my system is running the way it is suppose to be.  \n\nThere is something to be said for the fact that Intel uses a hardware based scheduler (Intel Thread Director) vs AMDs software based (Windows Game Mode w/ Xbox Game Bar or Process Lasso).  I know the 7800x3d doesn't need a scheduler with its architecture, but it certainly is not immune to some inconsistencies or inefficiencies with the platform.  I don't want this to turn into a flaming thread on AMD vs Intel, but rather my observations from owning both platforms in the past.  I actually started out on the 13900KS side of things, and drank the AMD hype kool-aid back when these x3d processors were announced.  The biggest drawback the Intel platform has is heat and power draw, but the frame consistency and 1% lows were never in question with Intel.  AMD is a new platform, and while 3d cache stacking is not new to AMD (5800x3d) it still has some things that need to be worked out.  However, based on historical data AMD is not known for their quick fixes, just look at the USB issues that plagued x570.  \n\nI have noticed fps spikes and inconsitencies with my 7950x3d setup, while others I game with on Intel are not noticing the same thing, and quite frankly, are tired of hearing me complain about it while I game.  I am going back to Intel for this very reason.  I know this will draw more power, but for gaming it is only going to use a few more Watts anyways, who cares.  The cost is so small over time.  If you are able to return it, and want things to just work, Intel is probably going to be your best bet.", "ups": 4, "depth": 0, "author": "thee_zoologist", "replies": []}, {"selftext": "Honestly, going with Intel 12 or 13th gen is not the right move if you're sensitive to stuttering.  The e-cores are known to introduce both micro and regular stuttering in the gameplay, because the games/Windows scheduler sometimes fail at delegating the games tasks to the P cores instead of the e-cores. Some people have to disable the e-cores entirely in the BIOS to fix it.", "ups": 5, "depth": 0, "author": "ConsistencyWelder", "replies": [{"selftext": "Lol you are so full of \ud83d\udca9 I own both 12 and 13gen and there are 0 stutters. The idiots of the Intel world are disabling ecores for the reason of added performance that has only been seen on a handful of games and at that not even competitive ones so it\u2019s pointless. As far as scheduler that was happening in windows 10 because 12th gen and on was designed with windows 11 in mind but regardless in under 6 months they patched windows 10. It\u2019s no longer an issue on either platform.", "ups": 5, "depth": 1, "author": "SnooKiwis7177", "replies": []}, {"selftext": "&gt;Honestly, going with Intel 12 or 13th gen is not the right move if you're sensitive to stuttering. The e-cores are known to introduce both micro and regular stuttering in the gameplay,\n\nBS", "ups": 2, "depth": 1, "author": "LickingMySistersFeet", "replies": []}, {"selftext": "I don't understand why anyone would use a high end Intel  cpu(i9) in a new build right now. The power draw and heat issues alone = No thanks.", "ups": 0, "depth": 1, "author": "ShortHandz", "replies": [{"selftext": "Power draw under 100% usage is different than light usage like gaming where 70w is typical.", "ups": 2, "depth": 2, "author": "SnooKiwis7177", "replies": [{"selftext": "Until your room turns into Dante's inferno mid summer during a gaming session.", "ups": 0, "depth": 3, "author": "ShortHandz", "replies": [{"selftext": "Dude it\u2019s been 100+ here in Florida and I haven\u2019t had an issue. Maybe get your ac checked out?", "ups": 1, "depth": 4, "author": "SnooKiwis7177", "replies": []}]}]}]}, {"selftext": "damn that's annoying...so intel has problems too with the added complexity. Hopefully they introduce a seperate lineup with straight cores and no e-cores after this CPU socket is done with.", "ups": 1, "depth": 1, "author": "ofon", "replies": [{"selftext": "It's windows fault, its thread scheduler doesn't know the difference between p cores and e cores or amd's frequency cores and cache cores", "ups": 1, "depth": 2, "author": "NewestAccount2023", "replies": [{"selftext": "Its intel fault, they should sell normal cpus, not cpus with garbage cores.", "ups": -1, "depth": 3, "author": "sanjozko", "replies": []}, {"selftext": "7800x3d has all cache cores though so that shouldn't be a problem at all here", "ups": 1, "depth": 3, "author": "ofon", "replies": []}]}]}, {"selftext": "Moved from 13900k 5.4 p 4.3 e i had stuttering in some games, now i on 7800x3d x670e hero pbo auto, virtualization off, ftm on, docp 6000 cl32 no issues,stuttering and e.t.c my 7800x3d run on agesa 7c and cpu boosting to 5.4 - 5.6 mhz soc 1.245 all good now", "ups": 1, "depth": 1, "author": "AeroBravo", "replies": [{"selftext": "Lol bro what cooler did you use? Sounds like your cpu was thermal throttling the entire time based off your stated clock speeds", "ups": 0, "depth": 2, "author": "SnooKiwis7177", "replies": [{"selftext": "I fixed the frequency and voltage at 13900k, the temperature in games was no higher than 75, r23 - 87c 40500\nThe disadvantage of Intel is its hybridity, which works differently in different projects. Cooling was asus rog strix lc ii 360\np.s of course I tested hour-long tests in OCCT, without any errors", "ups": 1, "depth": 3, "author": "AeroBravo", "replies": []}]}]}, {"selftext": "He's literally not using intel lmao", "ups": -6, "depth": 1, "author": "Dabs4Daze0", "replies": [{"selftext": "Yeah, but OP is considering returning the AMD CPU, so he might get a 13th gen Intel in place of it.", "ups": 4, "depth": 2, "author": "AccroG33K", "replies": []}, {"selftext": "I think he's just giving pre-emptive knowledge for those that are thinking of going Intel as a solution to this problem...I'm glad to know personally.", "ups": 1, "depth": 2, "author": "ofon", "replies": []}]}]}, {"selftext": "Two things left to try. Maximum performance mode in the nvidia control panel. Then try PCIe Gen3 in Bios.", "ups": 4, "depth": 0, "author": "Puffdotbusiness", "replies": [{"selftext": "Its max perfomance on control panel, GPU gen 3 why tho? i can try", "ups": 2, "depth": 1, "author": "ldontgeit", "replies": [{"selftext": "Newer gens of PCIe have more bandwidth but are also more susceptible to interference, so maybe if your motherboard is kinda borderline in terms of signal integrity it could help to lower the bandwidth.\n\nIf that does help, reseating the graphics card or CPU might help to make gen 4 PCIe stable, but the real world performance difference is usually minute.", "ups": 2, "depth": 2, "author": "knexfan0011", "replies": []}]}]}, {"selftext": "Sounds like a ram instability issue. AMD 7000 series is notoriously bad dealing with heavily overclocked RAM, especially on 4 DIMM boards.\n\nTry lowering the clock speed on your ram to like 5600mhz or 5200mhz. It sucks but if it solves your issue then it's worth it.", "ups": 2, "depth": 0, "author": "Dabs4Daze0", "replies": [{"selftext": "I think so too. Memory is the number 1 culprit for instability on AM5 from the research that I've done thus far.", "ups": 1, "depth": 1, "author": "Saffy_7", "replies": []}, {"selftext": "Check the official support page. 4 dimms means a max supported speed of like 3800mt/s.", "ups": 1, "depth": 1, "author": "Kiseido", "replies": [{"selftext": "On DDR5? No way lol", "ups": 1, "depth": 2, "author": "Dabs4Daze0", "replies": []}]}]}, {"selftext": "In general, Windows 10 is better for gaming.\n\nWindows 10 has better low 0.1, 1%.\n\n[https://www.youtube.com/watch?v=ZEns1LTicXI&amp;pp=ygUbd2luZG93cyAxMCBsYXRlbmN5IHRlY2ggeWVz](https://www.youtube.com/watch?v=ZEns1LTicXI&amp;pp=ygUbd2luZG93cyAxMCBsYXRlbmN5IHRlY2ggeWVz) \\- latency tests\n\n[https://www.youtube.com/watch?v=L9J9I1pCBtw&amp;pp=ygUXd2luZG93cyAxMCB2cyAxMSBnYW1pbmc%3D](https://www.youtube.com/watch?v=L9J9I1pCBtw&amp;pp=ygUXd2luZG93cyAxMCB2cyAxMSBnYW1pbmc%3D) \\- gaming\n\nAlso disable memory integrity in W11 if you really want to stick to this.", "ups": 2, "depth": 0, "author": "kepler2", "replies": []}, {"selftext": "Chipset drivers up to date?", "ups": 1, "depth": 0, "author": "LiquidMantis144", "replies": [{"selftext": "yes", "ups": 2, "depth": 1, "author": "ldontgeit", "replies": []}]}, {"selftext": "Are you sure the GPU is fine?", "ups": 1, "depth": 0, "author": "_xXU5ernameXx_", "replies": [{"selftext": "it is, it was tested in other system with 13900k and it did not replicate the problem.", "ups": 4, "depth": 1, "author": "ldontgeit", "replies": [{"selftext": "Dude I have a 5800x3d and I've been having legit the same issues as you. I get micro stuttering at random times and it's full system stutters not just in games", "ups": 3, "depth": 2, "author": "natethegreat_ttv", "replies": [{"selftext": "FTPM, turn that crap off and see if it's that...", "ups": 3, "depth": 3, "author": "v3rninater", "replies": [{"selftext": "&gt;FTPM, turn that crap off and see if it's that...\n\nYeah, ftpm off issue persists", "ups": 1, "depth": 4, "author": "ldontgeit", "replies": []}]}]}, {"selftext": "Why did you change CPUs lol ? Both are very good cpu as if one wasn\u2019t good enough.", "ups": 1, "depth": 2, "author": "AssFuckTwinsGbanger", "replies": [{"selftext": "i changed from 5800x3d to 7800x3d", "ups": 2, "depth": 3, "author": "ldontgeit", "replies": [{"selftext": "Oh I miss read , I thought you said a 13900k", "ups": -2, "depth": 4, "author": "AssFuckTwinsGbanger", "replies": []}]}]}, {"selftext": "Just return it then and go intel. Who cares which brand it is and the performance difference is negligible across a broad range of games. You spend good money you expect things to work properly thats the most important thing.", "ups": 0, "depth": 2, "author": "randysailer", "replies": []}, {"selftext": "Then probably faulty CPU, did you test the Mainboard and ram?", "ups": 1, "depth": 2, "author": "_xXU5ernameXx_", "replies": []}]}]}, {"selftext": "Capture traces with perfmon for cpu and afterburner for gpu\n\nReproduce the issue, check traces\n\nI bet on mem issues, though.. tried manually @ 5600 or something?  Passes Aida stability test?", "ups": 1, "depth": 0, "author": "ssuper2k", "replies": []}, {"selftext": "Maybe it's a GPU issue? Is your gpu plugged into the monitor instead of the motherboard?\n\nIs your g-sync on? Try turning that off in nvidia control panel. Post some screenshots with the temps while a game is running. U can use the hwinfo software.", "ups": 0, "depth": 0, "author": "rainmakesthedaygood", "replies": []}, {"selftext": "Might be the RAM disable expo and run stock.", "ups": 0, "depth": 0, "author": "Sujilia", "replies": []}, {"selftext": "Just built a new rig with 7800X3D and 4090FE, I am not experiencing the issues you are experiencing.\n\n* ASRock X670E Taichi\n* 2x16 G.Skill Trident Z5 Neo CL30\n* NVME drive\n* 850W Corsair RM850x PSU\n\nI highly doubt your issue has to do with the CPU. Have you considered the mobo being an issue?\n\nEDIT: I am also using L-Connect controlling x10 Lian Li Uni SL120 V2 fans via a single controller that is connected to one of my USB2.0 headers on the mobo. No issues.", "ups": 0, "depth": 0, "author": "katalysis", "replies": []}, {"selftext": "Yep, as i said several times, 99% of tech YouTubers are highly biased towards AMD, there are many of us that were mislead into buying and inferior platform. I\u2019m not throwing it away, but I\u2019m not buying AMD anytime soon. I recommend Frame Chasers and Tech Yes City, of the big ones, obviously GN, and stop counting.\n\nEdit: [The Infamous AMDip](https://youtu.be/JZfTER8x1M0)", "ups": -3, "depth": 0, "author": "Essential2643", "replies": [{"selftext": "CPUPro, is this you?", "ups": 1, "depth": 1, "author": "Mungojerrie86", "replies": []}]}, {"selftext": "You bought an Asus MB to go with the X3D, we know you don't know what your doing, take it to a PC store and get them to fix it.", "ups": -1, "depth": 0, "author": "Death2RNGesus", "replies": []}, {"selftext": "I've heard of reviewers getting defective CPUs before I believe. I think it was Linus Tech Tips that got a bad CPU with weird behavior like this from AMD. They returned it and got a properly working one. \n\nThe crappy thing about a lot of AMD CPUs is that we often get the silicon that didn't make the cut to be server grade. Which I think might result in more defective parts coming our way.", "ups": -6, "depth": 0, "author": "bubblesort33", "replies": []}, {"selftext": "I am using a 7800X3d and a 7900XTX nitro+ and had to disable SAM. It was not playing nice in some games with micro stutter as well. Once disabled i\u2019ve had 0 issues.", "ups": 1, "depth": 0, "author": "Klorrode", "replies": []}, {"selftext": "Like someone else said, you could try expo off, I have mine on expo 1 but manually set to 5200mhz.  I never even tried running 6000, I know most people do tho.  Did you change any pbo settings or anything?", "ups": 1, "depth": 0, "author": "Gorphie", "replies": []}, {"selftext": "Just had an issue with mouse stuttering, random audio dropouts, and sporadic fps. Fixed the issue by raising trfc ram timing by 100. DOCP out of the box may not be stable. Definitely suggest trying with DOCP off and see if the issue persists. Also running testmem5 with 1usmusV3 config and see if you get errors with DOCP on.", "ups": 1, "depth": 0, "author": "hellegaard1", "replies": []}, {"selftext": "Did you try a windows reinstall? Or was it a clean install already?", "ups": 1, "depth": 0, "author": "Boogertwilliams", "replies": []}, {"selftext": "Did you update your chipset drivers?", "ups": 1, "depth": 0, "author": "Meth_Lord", "replies": []}, {"selftext": "Have you run a memory test, the more detailed ones that can take an hour or more but will find any problems deep within the stick?   I have had bad ram sticks (new and one that failed after about a year) and it caused stuttering that was short but noticeable across games, software drivers, settings, you name it until I discovered it was a ram stick that had failed (one part of it so it always showed up at the correct speed and capacity but had a flaw that caused the stutters)", "ups": 1, "depth": 0, "author": "TempestTornado23", "replies": []}, {"selftext": "Are you using gsync + vsync? Also whats your monitor refresh rate?", "ups": 1, "depth": 0, "author": "AfraidNerve6973", "replies": [{"selftext": "Why use GSync+Vsync?\nIIRC, they shouldn't be running together.", "ups": 2, "depth": 1, "author": "Core2008", "replies": [{"selftext": "Vsync in game or in control panel + gsync + setting low latency mode to ultra works flawlessly if you want to have the smoothest gaming experience. You can use a frame limiter rather than vsync, however the combo above works just as well.", "ups": 1, "depth": 2, "author": "AfraidNerve6973", "replies": []}]}, {"selftext": "Yes im using gsync+vsync, but i also tried with vsync off and tried locking fps to lower levels, problem persist. My monitor is 165hz but i lock at 158fps to stay inside gsync range of my monitor", "ups": 1, "depth": 1, "author": "ldontgeit", "replies": [{"selftext": "Fixing micro stutters is a pain in the ass from my own experience. In my situation, It was mostly caused by improper power delivery into the system. If other things dont work , you could try running the system on another PSU and see how it goes.", "ups": 1, "depth": 2, "author": "AfraidNerve6973", "replies": []}]}]}, {"selftext": "7800x3d, asus b650 tuf gaming, no issue for 6months with a 6700xt  \ntroubleshooting an issue may take time", "ups": 1, "depth": 0, "author": "RBImGuy", "replies": []}, {"selftext": "I've got a 5800x3d and play things in VR so stutters are not only annoying but can make you sick...\n\nI found that having things like Dropbox, Adobe Creative Cloud, MSI Afterburner, or Logitech G Hub in the background was the source of my problem.", "ups": 1, "depth": 0, "author": "disgruntledempanada", "replies": []}, {"selftext": "Make sure the Chipset driver is up to date or try reinstalling it.\n\nYou also could try if disabling \u201eMemory integrity\u201c in windows \u201eCore isolation\u201c settings helps.", "ups": 1, "depth": 0, "author": "Ballerfreund", "replies": []}, {"selftext": "Sounds like a ram issue", "ups": 1, "depth": 0, "author": "stilliffex", "replies": []}, {"selftext": "Am really wondering If you can take a look at the Power consumption profiles and ensure they aren't capped?", "ups": 1, "depth": 0, "author": "DarthMinMax", "replies": []}, {"selftext": "I want to say it's a software issue, but probably is bios related. Remember the FTPM issue AMD took a year or longer to fix? It fits your description of the frame pacing issues. It's excellent hardware if you can wait around for a fix.\n\nNow I'm not saying FTPM is the issue, but AMD makes mistakes in their AGESA code every now and then that causes these symptoms, it was one of the examples. We had similar performance drops for security patches.\n\nI'd try older BIOS versions to see if any of them fixes it. Perhaps beta version. Maybe try turning off more stuff like HPET, hyperthreading, all kinds of virtualization and any feature you don't use that might put a load on the SOC.", "ups": 1, "depth": 0, "author": "jonathanx37", "replies": []}, {"selftext": "Do you have a multiple monitor setup with your monitors at different refresh rates? I do, and it causes some micro stuttering in some games when both monitors are on, like Total War Warhammer. Turning off my secondary display fixes 80% of stuttering in those games for me.", "ups": 1, "depth": 0, "author": "No_Satisfaction1284", "replies": []}, {"selftext": "Is the memory being used on the QVL list? I was reading last night that sometimes memory that hasn't been tested on the board can behave funny..", "ups": 1, "depth": 0, "author": "Saffy_7", "replies": [{"selftext": "yes it is on the QVL", "ups": 1, "depth": 1, "author": "ldontgeit", "replies": [{"selftext": "Try setting RAM at 5200mhz and see how the system responds.", "ups": 1, "depth": 2, "author": "Saffy_7", "replies": []}]}]}, {"selftext": "Make sure bios is updated. I have 7800x3D and 6700XT and it\u2019s super smooth.", "ups": 1, "depth": 0, "author": "miked3", "replies": []}, {"selftext": "I had a similar problem with my 7950x/4090 setup. Try disabling \u201cUSB selective suspend\u201d in the Windows power settings. That worked for me.", "ups": 1, "depth": 0, "author": "rippy77", "replies": []}, {"selftext": "And just to be clear, you've not touched the infinity fabric clock (FCLK)?\n\nBecause, while there could be a plethora of things causing this issue, unstable infinity fabric would allow the CPU to work but nuke it into a stuttery mess.", "ups": 1, "depth": 0, "author": "raysin_bisket", "replies": []}, {"selftext": "install chipset drivers for your board and uninstall MSI afterburner completely, I had microstutters with it installed even if it wasn't on with previous CPU, until uninstalling.\n\nFor chipset drivers visit https://www.amd.com/en/support and select Chipsets -&gt; AMD Socket AM5 -&gt; B650 (if that is your board, and not B650E), press Submit, download and install the Win11 \"AMD Chipset drivers\", then reboot.\n\nIf that doesn't help I'd question if you got a lemon.", "ups": 1, "depth": 0, "author": "ArtsM", "replies": []}, {"selftext": "Did u install the AMD 3D CACHE drivers?", "ups": 1, "depth": 0, "author": "Different-Ostrich692", "replies": []}, {"selftext": "Try reseting bios and see if you get stutter if you don't turn expo / xmp on", "ups": 1, "depth": 0, "author": "sudo-rm-r", "replies": []}, {"selftext": "Exclude your game folders in windows virus and threat protections", "ups": 1, "depth": 0, "author": "slicky13", "replies": []}, {"selftext": "Do a fresh install \n\ninstall the latest b650 chipset driver\n\nand for someone like me who had a similar issue with low 0.1 and micro stuttering it was my Samsung NVME check another NVME with 1tb or more because it mostly performs better\n\n&amp;#x200B;\n\nif that doesn't help i would probably check voltages in HWINFO when stressing Cinebench and FURMARK at the same time and keep a close eye on the PSU", "ups": 1, "depth": 0, "author": "Modey2222", "replies": []}, {"selftext": "Why would this be cpu related?  This is far more likely to be a gpu issue.", "ups": 1, "depth": 0, "author": "XXNameAlreadyTakenXX", "replies": []}, {"selftext": "Are you doing a PBO undervolt?", "ups": 1, "depth": 0, "author": "EolasDK", "replies": []}, {"selftext": "Sometimes ReLive is bugged and causes stuttering,  so if you have that on it's worth disabling to see if it helps. If this is the case you will get stuttering or see your frame time have huge spikes when you move your camera around fast.\n\nAlso a shot in the dark but for some games in some resolutions some combination of vsync/radeon chill/radeon antilag can help with stuttering.", "ups": 1, "depth": 0, "author": "ragged-robin", "replies": []}, {"selftext": "Do you have a dedicated TPM built on the mother board, or fTPM enabled? \n\nI had to buy a hardware module because windows11 kept shitting the bed with the fTPM enabled in bios. Basically every 10-30 seconds I'd have a slow dkow down for a half second that felt like a hard stall. Bought a hardware TPM and my 5950x flies.\n\nEdit: I missed the part where you said you already disabled it. God speed and good luck.", "ups": 1, "depth": 0, "author": "beaverbait", "replies": []}, {"selftext": "Is your RAM in the correct slots? Should be slot 2 and 4 away from the CPU", "ups": 1, "depth": 0, "author": "Asgardianking", "replies": []}, {"selftext": "It's not the CPU, I'll bet you it's software", "ups": 1, "depth": 0, "author": "the_wanginator", "replies": []}, {"selftext": "If you're not happy, just return it, no sense wasting your time.", "ups": 1, "depth": 0, "author": "quotemycode", "replies": []}, {"selftext": "Try closing all backround programs and try game then.\nI had either lian li or corsair rbg programs that was causing heavy stuttering.", "ups": 1, "depth": 0, "author": "v4rjo", "replies": []}, {"selftext": "I switch from 8700k  win 11 to 7800x3d  G.Skill Flare X5 32GB (CL): 30-38-38-96 , Asus Prime X670E-PRO WIFI  319 merc 6800xt card i previous had \n\ni turn of the pc without new installation just to check ,  everything was working  the felling was that it was bit slow /lag (remember didn't change  graphic  card)  no other issue then i did new **format** **everything fresh all programs** and made the necessary fix for fast boot to the memory.\n\nAfter that everything is day and night than before  not just in gaming but in everyday feeling,  i hope for you to find a solution its a same for such a high end system to have issues just do  a fresh install .", "ups": 1, "depth": 0, "author": "Ritzasone", "replies": []}, {"selftext": "Have you kept the CPU voltage and other BIOS settings related to the CPU on default and still faced the micro stutter?\n\nIf yes and if BIOS and Windows are fresh updated then there should not be a software related issue. Also if the Ryzen master is the latest installed.\n\nHowever, BIOSes can also introduce micro stutters via poor management of the CPU voltage.\n\nTherefore, try to go back 1 version on the BIOS and give that a shot.", "ups": 1, "depth": 0, "author": "MilkSheikh007", "replies": []}, {"selftext": "Weird! I just bought myself a ryzen 7 5800x3d with an asus tuf b50 plus with 32gb of ripjaws [6000hz] and I love it! I can play borderless games with my msi 3060 ti at 100+ fps at 2k with no issues. Framedrops down to 60-80fps occasionally on certain heavier games.\n\nI did a clean windows install though.", "ups": 1, "depth": 0, "author": "TaiyouOkami", "replies": []}, {"selftext": "Do you have Armory Crate installed? If so, get rid of it. That was the culprit for stutters in my games.", "ups": 1, "depth": 0, "author": "daparky", "replies": []}, {"selftext": "I had the EXACT same problem....and it magically went away.\n\nI am not joking you.\n\nI even made some threads about it.\n\nAfter a lot of BIOS updates, chipset driver updates, and just general usage....the problem slowly went away after a few weeks.\n\nI don't know what else to tell you.", "ups": 1, "depth": 0, "author": "AncientKroak", "replies": []}, {"selftext": "I recently moved to a 7800x3d and a 4090 but did a fresh install. Everything runs smoothly. No stutters on any games.", "ups": 1, "depth": 0, "author": "Polaris1983", "replies": []}, {"selftext": "Format hard drive and fresh Windows install.\n\nIf it doesnt help![gif](emote|free_emotes_pack|downvote)\n\nGet rid of Windows 11 and install Windows 10. I had lots of issues while I went from 10 to 11 with my 5800x3D AM4 PC.", "ups": 1, "depth": 0, "author": "WaveJust6431", "replies": []}, {"selftext": "My friend had stuttering in certain games and just overall instability. Changed out his Asus X670 for a Asrock X670 and the problem went away completely.\n\nAsus still has issues with their AM5 boards.", "ups": 1, "depth": 0, "author": "-peas-", "replies": []}, {"selftext": "Have you disabled Aura in bios? It's known to cause issues like what you describe. It's Asus rgb thing. Also, tried without expo? Had  a million issues with expo myself. \n\nSame motherboard and 7800X3D here, even same gpu model.", "ups": 1, "depth": 0, "author": "ridebird", "replies": []}, {"selftext": "Did you do fresh install of windows? Few people asked but I didn't see the response \n\nChanging cpu and especially motherboard you have to install new windows, then doownload all the drivers for motherboard from website, chipset, Bluetooth, etc.", "ups": 1, "depth": 0, "author": "Prophet_NY", "replies": []}, {"selftext": "Before reinstalling, try clearing your old hidden drivers out.\n\nYou can manually do it under device manager (view &gt; show hidden devices) or just use, Device Cleanup Tool. It does the same thing but it's faster.\n\nEdit: Also uninstall every single app you don't need anymore (if you don't know what it is, do a quick search). There's so much crap that runs in the background, and it's easier to just remove what you don't need.", "ups": 1, "depth": 0, "author": "OrdyNZ", "replies": []}, {"selftext": "That's why I got a 13600K. Pairing that beast with 7200Mhz Tuned RAM everything runs buttery smooth with amazing 1% lows. (Obviously ignoring shader compilation stutter at the beginning)", "ups": 1, "depth": 0, "author": "LickingMySistersFeet", "replies": []}, {"selftext": "I\u2019m a 13900k owner and this week tried a 7800X3D and was not impressed and returned it the next day.  I\u2019m mainly a Warzone player and the 0.1% low average was much worse than my 13900k on DDR4 4000 14-15-15-35\n\nThe average FPS was 30 fps higher while the 0.1 low average was 30 fps lower which resulted in an overall less smooth experience as the highest were higher but the lows were lower.\n\n\nI\u2019m pretty sure this is what people are referring to when the say the AMDip.", "ups": 1, "depth": 0, "author": "Rbk_3", "replies": []}, {"selftext": "Since I changed to full AMD (7900XTX and 7800X3D), I also have more problems with stuttering. My 7800X3D has massive problems with my RAM operating over 5200MHz.", "ups": 1, "depth": 0, "author": "ToniWarfare", "replies": []}, {"selftext": "More likely either windows or the asus board, not the CPU directly. Stuttering is hardly an AMD feature.", "ups": 1, "depth": 0, "author": "LongFluffyDragon", "replies": []}, {"selftext": "What games are you playing?  \n\n\nIt's incredibly smooth for me in Overwatch 2 and Apex Legends.  \n\n\nThere's a chance that it's not related to the CPU itself. Have you checked whether your RAM's OC is stable? Generally I've gotten microstutters in my older build when the RAM wasn't stable.", "ups": 1, "depth": 0, "author": "williamthebastardd", "replies": []}, {"selftext": "I had this with every AMD CPU. The fix was getting an external tpm and boom 0 micro stutters. $10K in gpu, monitor, ram, etc but a $20 tpm fixed it. Don't trust the Agenda \"fix\" it makes it better but never gets rid of it entirely.", "ups": 1, "depth": 0, "author": "live2dye", "replies": [{"selftext": "im gona get one of those, they around 12euros in my country", "ups": 1, "depth": 1, "author": "ldontgeit", "replies": []}, {"selftext": "&gt;$20 tpm fixed it\n\nDisabling TPM in BIOS isnt supposed to fix it? i got mine disabled", "ups": 1, "depth": 1, "author": "ldontgeit", "replies": []}]}, {"selftext": "maybe it's the monitor? is it 60hz or better?", "ups": 1, "depth": 0, "author": "IpPoster", "replies": []}, {"selftext": "Early in my build(first ever), I switched out the motherboard. Didnt reinstall. Then I upgraded the cpu. Didnt reinstall. Just upgraded the gpu. Didnt reinstall but you shouldnt have to. I dont have any problems that would make me consider reinstalling windows. And I wanted to avoid doing so because of all the random crap I've downloaded that I'd have to dig deep down to make backups for. What am I missing?", "ups": 1, "depth": 0, "author": "Cantcookeggs", "replies": []}, {"selftext": "One more thing, some devices share same pipeline. Make sure you are not overlapping them. It could be cutting your graphic from 16 to 8 or even down to 4.", "ups": 1, "depth": 0, "author": "Competitive_Roll466", "replies": []}, {"selftext": "I've had micro stuttering with amd with 2 pc. All the things you tried, I tried. A few frame stuttering, every 4 or 5 seconds. So annoying. \n\nFirst pc fix was incredibly dumb. It appears that my mouse have a weak signal (corsair sabre wireless). Having the usb receiver on the motherboard was not ok, probably because of the case metal. Now I use an extender to have the receiver closer. Don't ask why but it was not just the pointer being unresponsive, every time the signal reconnected, it dropped a few frame. As if a huge buffer was unloading to much movement instructions.\n\nSecond pc fix was a faulty motherboard. I bought it used (now I can guess why). But this was on am5.", "ups": 1, "depth": 0, "author": "Mostrapotski", "replies": []}, {"selftext": "well i too run into a bunch of problem with my 5600x, cauz it constantly crashes Overwatch midgame, it also pisses me off. my fix is dat i lower the cpu core clock, n it fixes it! its indeed a cpu issue here", "ups": 1, "depth": 0, "author": "jakecen", "replies": []}, {"selftext": "Reinstall windows, my 7800x3d has been running great.", "ups": 1, "depth": 0, "author": "ascufgewogf", "replies": []}, {"selftext": "AMD lists the max memory speed for the 7800X3D as 5200, might be worth looking into that first", "ups": 1, "depth": 0, "author": "biggestLOUser", "replies": []}, {"selftext": "How sure are you it is the CPU? Do you have g-sync enabled on your monitor? Have you tried disabling expo and also 1 stick of RAM to test out if the RAM cause issue?", "ups": 1, "depth": 0, "author": "Glum-Bottle-1540", "replies": []}, {"selftext": "having a pure amd build myself (with a 7800x3d), i wish more ppl would go team red. i had a few issues with my motherboards (had to return 2 times since quality control is somewhat non existing nowadays)... also had to ask for a replacement cpu after all of that, since one of the old motherboards fried it to a crisp... but i dont think amd is the one to blame here, but more the individual board partners which supplied shitty motherboards. one thing that stood out to me on one of my shitty motherboards was that it showed 'cpu error' while failing to post, even though the cpu was fine (the motherboard was actually the component with a problem... what an irony... so thats maybe something to consider? (bad motherboard or ram?) jumping forward a bit... i m a happy customer and would pick the 7800x3d over any current intel (or even other amd) offerings. you should consider returning the cpu and asking for a new one. if that doesnt help, send back all esential components and ask them to fugure it out themselfs (thats also what i did in the end).", "ups": 1, "depth": 0, "author": "newroth", "replies": []}]}
{"post": {"title": "Overclocking my AMD Ryzen 5 4500 CPU", "subreddit": "Amd", "selftext": "I currently have my CPU running at 4000 MHz with core voltage of 1.408 is this too high for this clock speed which is close to its boost speed", "ups": 0, "permalink": "/r/Amd/comments/16oic4w/overclocking_my_amd_ryzen_5_4500_cpu/", "num_comments": 22}, "replies": [{"selftext": "Why the hell do you have the voltage set so high?", "ups": 3, "depth": 0, "author": "handymanshandle", "replies": [{"selftext": "What should I have it at", "ups": 1, "depth": 1, "author": "UnionEducational1140", "replies": [{"selftext": "Please don't go messing with these things if you don't know what you're doing. It's an expensive mistake waiting to happen.", "ups": 9, "depth": 2, "author": "Draykez", "replies": [{"selftext": "Like what voltage should it be at", "ups": 1, "depth": 3, "author": "UnionEducational1140", "replies": [{"selftext": "Not really sure but I Guess for you at like 1.3v the lower the better.\nIf you ll go too low you might not be able to even boot to bios.\n\nYou ll need to stress test it watch the temps and hope you won t break it.\n\nYou do this on your own risk", "ups": 1, "depth": 4, "author": "Mario2x2SK", "replies": [{"selftext": "Risk I'm willing to take this cou wasn't expensive at all so I could just get myself a newer one if needed but thanks, and also to the other guy who commented sorry for getting a little heated from that", "ups": 1, "depth": 5, "author": "UnionEducational1140", "replies": []}]}, {"selftext": "On auto in bios!", "ups": 1, "depth": 4, "author": "nomzo257", "replies": []}]}, {"selftext": "Well then tell me something that is helpful for my situation", "ups": 0, "depth": 3, "author": "UnionEducational1140", "replies": [{"selftext": "keep everything at auto\n\nif you dont know how to OC by yourself, then dont because you will damage your CPU", "ups": 5, "depth": 4, "author": "maze100X", "replies": [{"selftext": "They say the voltage is already on auto", "ups": 0, "depth": 5, "author": "AreYouEvenReal2", "replies": [{"selftext": "on auto the cpu will decide what voltage it needs\n\nin idle states Zen 2 cpus will usually see 1.4 - 1.5V and its fine (its low current state)\n\nin loads that use multiple cores it will drop to 1.2v\\~", "ups": 2, "depth": 6, "author": "maze100X", "replies": [{"selftext": "Ahh ok tbh I might not even try to have this OC might just buy me a better CPU", "ups": 1, "depth": 7, "author": "UnionEducational1140", "replies": [{"selftext": "there isnt much point to OC today anyway, most CPUs will get you only 5%\\~ better performance by overclocking\n\n&amp;#x200B;\n\nif you are not happy with the 4500, you might want to take a look at the 5700X/5800X3D as 2 good options for gaming (5700x is great price/perf, 5800x3d is the fastest gaming cpu on AM4)", "ups": 1, "depth": 8, "author": "maze100X", "replies": []}]}]}]}]}]}]}]}, {"selftext": "It's on auto", "ups": 0, "depth": 1, "author": "UnionEducational1140", "replies": []}]}, {"selftext": "That voltage, if it is fixed, is outrageous and will kill the CPU in likely weeks. Please clear CMOS to factory reset, and stop messing with stuff without doing any research at all.\n\nVoltage should be left on automatic control, which will range between about 1.1 and 1.5, based on what is currently safe. sustained load voltage will never be much over 1.2-1.3v\n\nLikewise, clockspeed should not be fixed to a specific value. 4000 is actually lower than or about equal to the speed it should boost to. Use PBO to add offsets to the speed target, or leave it alone. Overclocking modern components tends to have little benefit.\n\nHopefully your CPU is not (too) permanently damaged already.", "ups": 3, "depth": 0, "author": "LongFluffyDragon", "replies": [{"selftext": "I changed the voltage to 1.25", "ups": 1, "depth": 1, "author": "UnionEducational1140", "replies": [{"selftext": "That will break the boost algorithm and still be high for full load. Just reset the board and leave it alone. you should never be setting a fixed voltage for normal use.", "ups": 2, "depth": 2, "author": "LongFluffyDragon", "replies": [{"selftext": "Well it's not fixed that's it's capped value it can reach", "ups": 0, "depth": 3, "author": "UnionEducational1140", "replies": [{"selftext": "That is not how it works. Just dont mess with it unless you understand how the boosting algorithm works, thoroughly. All you will do is worsen performance, heat, or cause damage.", "ups": 2, "depth": 4, "author": "LongFluffyDragon", "replies": [{"selftext": "Ok not a problem i can just get a new CPU", "ups": 0, "depth": 5, "author": "UnionEducational1140", "replies": []}]}]}, {"selftext": "It's gaming use", "ups": 0, "depth": 3, "author": "UnionEducational1140", "replies": []}]}]}]}, {"selftext": "At 1.408V, I'd kind of expect you to be pushing a more impressive clock speed, like 4500 MHz or 4600 MHz.\n\nFor daily use, I wouldn't bother overclocking Zen 2/3/4 core frequency, Precision Boost is already close to the limit out of the box, and the only thing you can feasibly accomplish is a small boost in Cinebench", "ups": 1, "depth": 0, "author": "Noreng", "replies": []}]}
{"post": {"title": "23.9.2 without release notes", "subreddit": "Amd", "selftext": "Yesterday we got 23.9.2 but still there is no release notes page.\n\nDoes anybody know what they fixed?", "ups": 63, "permalink": "/r/Amd/comments/16njdgy/2392_without_release_notes/", "num_comments": 91}, "replies": [{"selftext": "New Game Support  \n\n* Lies of P  \n* Party Animals  \n*  The Crew\u2122 Motorfest\n\nKnown Issues  \n\n* Performance Metrics Overlay may report N/A for FPS on various games.  \nhttps://www.amd.com/en/support/kb/release-notes/rn-rad-win-23-9-2-polaris-vega", "ups": 29, "depth": 0, "author": "vBDKv", "replies": [{"selftext": "So is:\n\n* Audio may intermittently become out of sync with video when recording from AMD Software: Adrenalin Edition with AV1 codec.\n\nResolved?", "ups": 7, "depth": 1, "author": "Colest", "replies": [{"selftext": "This specific driver is for Polaris/Vega which doesn't have AV1 encode so ofc it's not going to be listed as an issue.", "ups": 10, "depth": 2, "author": "AreYouAWiiizard", "replies": [{"selftext": "Yes I see that now that the linked has been edited into the post.", "ups": 1, "depth": 3, "author": "Colest", "replies": []}]}]}, {"selftext": "Where did you read them?  \nHere I still get a 404 on their page.", "ups": 1, "depth": 1, "author": "OSSLover", "replies": [{"selftext": "Just added link, took some time to figure out reddit formatting lol", "ups": 8, "depth": 2, "author": "vBDKv", "replies": [{"selftext": "Ahhhh, they fucked up their own release notes link in adrenalin.\n\nBut where is RDNA?\n\nThank you and yes, their markdown implementation is weird xD", "ups": 1, "depth": 3, "author": "OSSLover", "replies": [{"selftext": "There was this whole fud about AMD cutting driver support for the Vega / Polaris GPUs. Just because they weren't included in the Lies of Pi hotfix.\n\nThis version seems to be for those gpus.", "ups": 5, "depth": 4, "author": "SolarianStrike", "replies": [{"selftext": "If they splitted the drivers I hope for RDNA specific fixes.", "ups": 0, "depth": 5, "author": "OSSLover", "replies": [{"selftext": "That's actually not how it works :) Even if listed under VEGA/Polaris, the longer you go back, the more details you get. These drivers are not specifically made for either VEGA or Polaris (which are pretty old by now), but for all of them, even the new RDNA3's.", "ups": 3, "depth": 6, "author": "vBDKv", "replies": [{"selftext": "Polaris has gotten 8 years of driver support!", "ups": 1, "depth": 7, "author": "raidechomi", "replies": [{"selftext": "FineWine ;)", "ups": 2, "depth": 8, "author": "vBDKv", "replies": [{"selftext": "kinda a sad thing to be proud of, do it right the first time and there is no need for that FineWine.. Then again I guess when your the multi billion dollar underdog you need that time to work out the kinks..", "ups": -2, "depth": 9, "author": "HotRoderX", "replies": []}]}, {"selftext": "7*", "ups": 0, "depth": 8, "author": "Cryio", "replies": []}]}]}]}]}]}]}, {"selftext": "They finally uploaded the RDNA branch of the driver - [https://www.amd.com/en/support/kb/release-notes/rn-rad-win-23-9-2](https://www.amd.com/en/support/kb/release-notes/rn-rad-win-23-9-2) Link now works :D", "ups": 3, "depth": 2, "author": "Omegachai", "replies": []}]}]}, {"selftext": "After installing the new driver, my frequency and voltage settings in amd adrenalin broke, as soon as I switched to manual control, the video card (rx 580 8gb) produces a gray screen and artifacts, but the settings in msi afterburner work stably, I advise polaris owners to wait with the update", "ups": 13, "depth": 0, "author": "hackerass", "replies": [{"selftext": "Thanks for the heads up!", "ups": 3, "depth": 1, "author": "Southcoastolder", "replies": []}, {"selftext": "i had the same problem on 23.9.1 with a 7800xtx", "ups": 2, "depth": 1, "author": "RazerPSN", "replies": []}, {"selftext": "Exactly the same", "ups": 2, "depth": 1, "author": "AccordingGanache9813", "replies": []}]}, {"selftext": "Still no hypr-rx or anti-lag+ for rdna2?\n\nSigh...", "ups": 7, "depth": 0, "author": "diegodamohill", "replies": [{"selftext": "wait, anti-lag+ is not available for rdna2?", "ups": 2, "depth": 1, "author": "wolnee", "replies": [{"selftext": "Nope", "ups": 3, "depth": 2, "author": "diegodamohill", "replies": []}, {"selftext": "Only normal anti-lag is available right now but I doubt they will make it rdna3 exclusive for long.", "ups": 1, "depth": 2, "author": "raifusarewaifus", "replies": []}, {"selftext": "https://youtu.be/E1kX6f-mzsU?si=AzmqQpTHLIahvpe6", "ups": 1, "depth": 2, "author": "ZAR1FF", "replies": []}]}, {"selftext": "Use modded drivers, I've been enjoying it for a while.\n\nAfter owning RDNA1 since 2019, I found out about modded drivers and never looked back to official ones.\n\nMore stable for me and lots of fixes that AMD doesn't bother to.\n\nEven after upgrading to an 6950XT, I still use modded drivers.", "ups": 1, "depth": 1, "author": "Anthonymvpr", "replies": []}]}, {"selftext": "I try to Install and got error 182 AMD Software installer detected AMD Graphics hardware in your system configuration that is not supported. Guess they messed it up so back to 23.9.1 for me.", "ups": 6, "depth": 0, "author": "HawkM1", "replies": [{"selftext": "It's Polaris and Vega only\n\nGet proper driver here: https://www.amd.com/en/support/kb/release-notes/rn-rad-win-23-9-2", "ups": 3, "depth": 1, "author": "CAP_NET_ADMIN", "replies": [{"selftext": "Well at the time [https://www.amd.com/en/support](https://www.amd.com/en/support) was pointing me to the wrong file they did not even have release notes up when i downloaded. The file size is 624mb like pretty much any other driver released in the last 6 months anyways the site is pointing to the right driver now its called 23.9.2-win10-win11-sep19-combined and has a file size 1.2gb. So what is going on here did they combined Polaris and vega with RDNA drivers?", "ups": 1, "depth": 2, "author": "HawkM1", "replies": []}]}, {"selftext": "Same. I had to use the auto detect installer thing to get 23.9.2 on my 6750xt.", "ups": 2, "depth": 1, "author": "epyon9283", "replies": []}, {"selftext": "Because your system configuration is not supported.  They didn\u2019t mess it up.\n\nIt\u2019s for Polaris and Vega.", "ups": 2, "depth": 1, "author": "ATrayYou", "replies": []}, {"selftext": "Same here on my 6700xt", "ups": 1, "depth": 1, "author": "RinMaru30", "replies": []}, {"selftext": "exact same thing happened to me on my 6700xt after i DDU'd. Did a clean install of 23.9.1 again then it allowed me to install 23.9.2 from within adrenalin by checking for update and installing from there", "ups": 1, "depth": 1, "author": "MrHatchh", "replies": []}, {"selftext": "Same here on my 7900xt even after passing ddu to it", "ups": 1, "depth": 1, "author": "Intrepid_Excuse9616", "replies": []}, {"selftext": "I'm getting this issue as well and can't seem to resolve it.", "ups": 1, "depth": 1, "author": "Bug5577", "replies": [{"selftext": "Try using the AMD driver selector tool, I was having the same issue and doing this worked perfect first time.", "ups": 1, "depth": 2, "author": "ATOJAR", "replies": []}]}, {"selftext": "Yeah it was doing that to me so I just used the AMD driver selector tool, installed without issue.", "ups": 1, "depth": 1, "author": "ATOJAR", "replies": []}]}, {"selftext": "My screen sometimes flashes white during chrome? any fix for this?\n\nsapphire 6700 xt (sadly with coil whine btw :P, but that shouldnt be the problem)", "ups": 3, "depth": 0, "author": "DoOmXx_", "replies": [{"selftext": "use report bug in amd adrenalin to feedback the issue", "ups": 1, "depth": 1, "author": "Ricky_0001", "replies": []}, {"selftext": "&gt;chrome\n\nsame issue!", "ups": 1, "depth": 1, "author": "Lion021", "replies": []}]}, {"selftext": "I had to roll back to the previous release. This was crashing my system whenever I opened a web browser. It would hang so badly as to look like it was crashed. Had to hard reset and once I rolled back the issue went away.\n\nReally weird.", "ups": 3, "depth": 0, "author": "Jbstargate1", "replies": []}, {"selftext": "Weird there no fixes for brightness issue on Samsung monitors", "ups": 2, "depth": 0, "author": "RevenantX0", "replies": [{"selftext": "Notes suggest there is a fix.", "ups": 3, "depth": 1, "author": "Affectionate_Ad_233", "replies": []}]}, {"selftext": "**Heads Up! There are two versions of the 23.9.2 driver.** If you manually update your drivers for multiple computers make sure you download the driver separately for Polaris and Vega (rx 400, rx 500, vega, VII, pro duo).", "ups": 2, "depth": 0, "author": "CrispyPizzaRolls", "replies": []}, {"selftext": "I'm guessing it's just this https://www.amd.com/en/support/kb/release-notes/rn-rad-win-23-20-11-01-liesofp", "ups": 3, "depth": 0, "author": "RockyXvII", "replies": [{"selftext": "No fixed issues. :(", "ups": 1, "depth": 1, "author": "OSSLover", "replies": [{"selftext": "Previous \"Known issues\" not listed. Maybe silent fix?", "ups": 1, "depth": 2, "author": "Appropriate_Pen4445", "replies": [{"selftext": "It's possible there are other fixes and maybe support for Cyberpunk update 2.0 and phantom liberty. We just have to wait for AMD to update the page unfortunately", "ups": 2, "depth": 3, "author": "RockyXvII", "replies": []}]}]}]}, {"selftext": "High idle power still not fixed \ud83d\ude1e", "ups": 3, "depth": 0, "author": "Wonderful-Middle-543", "replies": [{"selftext": "Depends on the monitor setup.  \nThey fixed it for many users already including me.\n\nYou should send them an error report with your setup details.", "ups": 6, "depth": 1, "author": "OSSLover", "replies": [{"selftext": "I already have. They're still not fixed for a ton of freesync monitors like from Dell, (alienware) and asus. Few tv's too apparently. Idling at 100+ watts even on a single monitor setup. Same gpu as you.", "ups": 3, "depth": 2, "author": "Wonderful-Middle-543", "replies": [{"selftext": "Sad to hear.\n\nMy Samsung QLED 144Hz is fixed and it only consumes 5-20W depending if I move the mouse fast.", "ups": 3, "depth": 3, "author": "OSSLover", "replies": [{"selftext": "Crazy lucky. I wish I took this into consideration before buying monitors but here we are. We need a list of monitors that still have high idle power issues with rx 7000.", "ups": 1, "depth": 4, "author": "Wonderful-Middle-543", "replies": [{"selftext": "unfortunate...yet another reason to buy Nvidia even with their stupid pricing", "ups": 0, "depth": 5, "author": "ofon", "replies": [{"selftext": "yeah I might go for a 5090 when that comes out if it's still not fixed. Unless it's like $1800 msrp.", "ups": 1, "depth": 6, "author": "Wonderful-Middle-543", "replies": [{"selftext": "Idk man...I'm sure a 5090 is gonna be awesome...but I see them being about 2000 at least this coming generation especially since they'll begin using GDDR7 over GDDR6.", "ups": 1, "depth": 7, "author": "ofon", "replies": []}]}]}]}, {"selftext": "I noticed that fix as well. If you drag a window or move the mouse the card runs almost full tilt.", "ups": 1, "depth": 4, "author": "LimitClean155", "replies": []}]}]}]}, {"selftext": "I recently contacted support about this after sending multiple reports with the report tool. They confirmed they are still looking at it even if it's not in the driver notes.\n\nThis is what they answered me with:\n\n&amp;#x200B;\n\n&gt;Thank you for the response.  \n&gt;  \n&gt;Please note that high idle power usage when using multiple display with high refresh rate was optimized with 23.8.1 drivers and further optimizations are being investigated for improvement with future drivers.  \n&gt;  \n&gt;[https://www.amd.com/en/support/kb/release-notes/rn-rad-win-23-8-1](https://www.amd.com/en/support/kb/release-notes/rn-rad-win-23-8-1)  \n&gt;  \n&gt;I request you to wait for future driver updates and check the status.  \n&gt;  \n&gt;Thank you for contacting AMD.\n\n&amp;#x200B;\n\nKnowing [this](https://www.reddit.com/r/Amd/comments/15v9m4e/reason_for_high_idle_power/?utm_source=share&amp;utm_medium=web2x&amp;context=3) is the cause, they will be rolling fixes eventually with each driver. In my case it's not fixed with dual monitors, but using a single 4k144 it idles down to 10watt, which wasn't the case before. Hope this helps  \n\n\nUpdate: This new driver (23.9.2) fixed my idle power finally", "ups": 2, "depth": 1, "author": "343guilityspark", "replies": [{"selftext": "Good to know they're working on it. It's not like I have crazy rare underground monitors so I hope they get fixed soon", "ups": 2, "depth": 2, "author": "Wonderful-Middle-543", "replies": [{"selftext": "Just used DDU and installed this new driver, idle power and vram maxed out has been fixed for me finally. Try sending bug reports and contacting them via mail, it may actually be helpful", "ups": 1, "depth": 3, "author": "343guilityspark", "replies": [{"selftext": "I just tried DDU again but unfortunately the vram is still stuck at 2500, 100 watts on idle", "ups": 2, "depth": 4, "author": "Wonderful-Middle-543", "replies": [{"selftext": "Have you tryed changing refresh rate? What monitor you use?", "ups": 2, "depth": 5, "author": "aRx4ErZYc6ut35", "replies": [{"selftext": "Yeah I have. I don't know the model number off the top of my head but the Alienware 280hz 1440p monitor. Turning down refreshrate does nothing, same on my asus tuf 1440p 170hz.", "ups": 2, "depth": 6, "author": "Wonderful-Middle-543", "replies": []}]}]}]}]}, {"selftext": "I sent PM to AMD Dev on reddit a month or so ago they asked in the comments for anyone who still has problems , 23.9.2 fixed my monitor setup too , mine are 2x 1440p 144hz monitors LG+ASUS", "ups": 2, "depth": 2, "author": "usfy99", "replies": [{"selftext": "I was late to that thread and contacted them instead. By the looks of it they really care about our feedback, I was worried I wouldn't get it fixed. Glad you got yours fixed too", "ups": 2, "depth": 3, "author": "343guilityspark", "replies": []}]}]}, {"selftext": "Hey there, can you tell us which displays, GPU and connectivity methods (HDMI, DP | Version) you're using?\n\nCheers", "ups": 2, "depth": 1, "author": "AMD_Vik", "replies": [{"selftext": "Certainly,\n\n-DELL AW2723DF (connected via Silkland DP 2.1 Cable)\n-Asus VG32AQL1A (connected via Monster HDMI 2.0)\n-LG 27GK750F (connected via (non branded DP 2.1)\n\nThe first two are the main issue. They individually use 100 watts even when downclocked to 60hz. Have tried everything, including VRR/Freesync on/off and DDU.", "ups": 1, "depth": 2, "author": "Wonderful-Middle-543", "replies": [{"selftext": "Thanks for this. To be clear, this is with NV31, right? Can you tell us exactly which model GPU you're using in this config?", "ups": 2, "depth": 3, "author": "AMD_Vik", "replies": [{"selftext": "Yes, it's the 7900 XTX Sapphire nitro+. I also had the same issue on a 7900 XT (non xtx) (MERC 310 BLACK)", "ups": 1, "depth": 4, "author": "Wonderful-Middle-543", "replies": []}]}]}]}, {"selftext": "Acer 10w idle, 7800xt", "ups": 1, "depth": 1, "author": "feorun5", "replies": []}, {"selftext": "for me 165hz and 60hz still drains a lot but its fixed when i set the first one on 144", "ups": 1, "depth": 1, "author": "yous2of", "replies": []}, {"selftext": "did you turn on variable refresh rate in Windows setting?", "ups": 1, "depth": 1, "author": "hirotwofanboy", "replies": [{"selftext": "Yes", "ups": 1, "depth": 2, "author": "Wonderful-Middle-543", "replies": [{"selftext": "turn it off, if the power consumption still high, then GG", "ups": 1, "depth": 3, "author": "hirotwofanboy", "replies": [{"selftext": "Yeah tried that already, no change. Rip", "ups": 1, "depth": 4, "author": "Wonderful-Middle-543", "replies": []}]}]}]}, {"selftext": "Yep same, have to fiddle around with CRU to temporarily fix it but I can't do it for my 165Hz monitor, this one needs a driver fix so.. RIP 165Hz gaming", "ups": 1, "depth": 1, "author": "GloriousPudding", "replies": []}]}, {"selftext": "I'd think it's based off the hotfix we got a few days ago\nJust lies of p support I think", "ups": 2, "depth": 0, "author": "Catsanno", "replies": []}, {"selftext": "Same here, just installed would love to so the release notes too, but it says page not found", "ups": 1, "depth": 0, "author": "Stormyse", "replies": []}, {"selftext": "Users pairing an RDNA based graphics product with either Polaris or Vega based graphics product are recommended to use the AMD Auto-Detect and Install Tool.\n\nIs this the beginning of EOL?", "ups": -1, "depth": 0, "author": "ZacXme", "replies": [{"selftext": "How do you get EOL from that statement?", "ups": 3, "depth": 1, "author": "Opteron170", "replies": []}]}, {"selftext": "It's on the official release notes page.", "ups": 0, "depth": 0, "author": "Grunthos_Flatulent", "replies": []}, {"selftext": "Ahhh", "ups": 1, "depth": 0, "author": "Puzzleheaded_Owl_417", "replies": []}, {"selftext": "I saw these drivers show up in my control panel so I tried to install them but they fail on 7000 series.", "ups": 1, "depth": 0, "author": "Opteron170", "replies": []}, {"selftext": "[https://www.amd.com/en/support/kb/release-notes/rn-rad-win-23-9-2](https://www.amd.com/en/support/kb/release-notes/rn-rad-win-23-9-2)\n\nThe website works fine (now?)", "ups": 1, "depth": 0, "author": "Fit_Factor_5306", "replies": [{"selftext": "Yes, now. :P", "ups": 1, "depth": 1, "author": "OSSLover", "replies": []}]}, {"selftext": "I have 6650XT for gaming and RX570 for FM video.\n\nDoes it mean I cannot upgrade drive anymore after 23.9.2, or I can install both version?", "ups": 1, "depth": 0, "author": "MaKlaustis", "replies": []}, {"selftext": "anybody knows how to enable anti-lag+ in starfield?", "ups": 1, "depth": 0, "author": "wolnee", "replies": [{"selftext": "In the driver.", "ups": 1, "depth": 1, "author": "OSSLover", "replies": []}]}, {"selftext": "Anybody know why i'm possibly getting an update failed when downloading the drivers from Adrenaline?", "ups": 1, "depth": 0, "author": "urlond", "replies": [{"selftext": "I'm pretty sure I read somewhere that you should not download via Adrenaline. I forgot where, but yeah, sometimes AMD does this. Even with motherboard drivers which is kinda scary.", "ups": 1, "depth": 1, "author": "vBDKv", "replies": []}]}]}
{"post": {"title": "Relive VR and Virtual Desktop", "subreddit": "Amd", "selftext": "Has anyone here tested both of these? I am trying ReLive now and it's not terrible but before I pay anything for Virtual Desktop I want to know if the experience will be fairly similar.\n\n&amp;#x200B;\n\nEdit: I ended up just buying Virtual Desktop and it really is a lot better and easier for those on a Quest. It looks way better than Air Link or AMD ReLive. For anyone finding this on Google looking for a comparison like I was. ReLive is a significant downgrade even from Air Link. While it can look good often times games are comparable to FSR 2.1 in performance mode visually. Very responsive though. Air Link is somewhere in the middle. It doesn't look bad it just doesn't look great. Virtual Desktop looks like I am playing a flat screen game when you crank up the settings. Just get Virtual Desktop. ", "ups": 11, "permalink": "/r/Amd/comments/16nncbn/relive_vr_and_virtual_desktop/", "num_comments": 14}, "replies": [{"selftext": "Try alvr or just use wired link/ airlink, only waste your money on vd when every other solutions doesn't work for you", "ups": 4, "depth": 0, "author": "crazyplayer2481", "replies": [{"selftext": "I'm asking about the differences between the two in quality. I have no idea what ALVR is either. I see a bunch of stuff on Google about setting it up but no clue as to what it actually is.", "ups": 1, "depth": 1, "author": "GloriousKev", "replies": [{"selftext": "VD now has a lot of optional features like upscaling, a sharpening slider, quickly change automatic/forced/disabled frame generation, 4 codecs to choose from including HEVC 10-bit and H264+ with 400 Mbps cap, and you can disable data encryption to reduce latency.\n\nAll those things make games on VD look almost as sharp as playing on a flatscreen if you're able to run at least the \"High\" resolution preset.\n\nAll AirLink offers to improve the image is a mild sharpening filter that you can't adjust.\n\nBtw to change any settings except for codec you don't need to disconnect the headset and restart the whole software like with AirLink.", "ups": 3, "depth": 2, "author": "Audisek", "replies": []}, {"selftext": "Like I said, stick to wired link/ airlink since its the best overall (image quality and others) and free\n\nVd's image quality is decent, but the cap 150mbps (idk if they increase in later versions) bitrate and shitty h265 implementation that doesn't work on most amd drivers makes it even worse than alvr (not to mention mediocre tracking quality compare to link/airlink)\n\nAs for relive, everything is crap, limited config options, tried it once, never touch it again", "ups": -1, "depth": 2, "author": "crazyplayer2481", "replies": [{"selftext": "You are completely outdated, their new streaming options look awesome, and can go far beyond 150mbs. VD is now very superior to airlink in all ways, except for not being wired (which matters if your wifi sucks), and costing money.", "ups": 5, "depth": 3, "author": "pixelcowboy", "replies": [{"selftext": "Guess they uncapped the bitrate, but if their shit h265 codec still gonna crash whenever I update my graphics driver then it still not worth spending 20$, I'd rather buy a cheap link cable and some accessories for that kind of money. Also if you own a quest pro, good luck getting face, eyes tracking to work with vd", "ups": 0, "depth": 4, "author": "crazyplayer2481", "replies": [{"selftext": "Not only did they improve it, but they now have HEVC 10 bit streaming, which makes a world of difference in dark games. It's awesome dude, no comparison to Link. Link is fine, but VD just has much more functionality and is a lot better integrated into the quest.", "ups": 3, "depth": 5, "author": "pixelcowboy", "replies": [{"selftext": "If anyone find it worth spending money then good for them, but for people who need face tracking and others oculus software features than its better to keep using link, why waste money buy a streaming app for your 1000$ headset when it can't even support the features that other solution already provide for free", "ups": 0, "depth": 6, "author": "crazyplayer2481", "replies": []}, {"selftext": "A question about vd, we are talking about vd from meta quest store, not the steam one ? I'm a bit confused about those versions.", "ups": 1, "depth": 6, "author": "admfrmhll", "replies": [{"selftext": "Yes, if you have a quest you need the one from the quest store . The steam one is really just a desktop interface with some extra utilities.", "ups": 1, "depth": 7, "author": "pixelcowboy", "replies": []}]}]}]}]}]}]}]}, {"selftext": "VIrtual Desktop is a great app. The guy behind really puts love into it.", "ups": 2, "depth": 0, "author": "Sacco_Belmonte", "replies": []}, {"selftext": "I tried relive vr once back when I traded over to a 6900xt. It sucked. I use VD most of the time with my quest 2, it works well. \n\nedit: This was like april last year.", "ups": 2, "depth": 0, "author": "duplissi", "replies": []}, {"selftext": "Virtual desktop is absolutely worth it if you have a Quest headset. It's not expensive either, just get it and you won't regret it. If it doesn't work for you you can always refund it.", "ups": 1, "depth": 0, "author": "pixelcowboy", "replies": []}, {"selftext": "Virtual desktop is easy to use and very versatile", "ups": 1, "depth": 0, "author": "SkinnyDom", "replies": []}]}
{"post": {"title": "x3d core parking seems to still be all over the place.", "subreddit": "Amd", "selftext": "so, i just got a 7900X3D from microcenter for $450. It suits my needs well between gamimg perf and multi threaded needs for my budget. A 7800x3d is not on my radar, sorry. i need at least 12c. i am NOT here to bash this CPU, Just sharing my observations.\n\nbut. after a fresh W11 install and some testing....the 3DCache optimizer seems to kinda work but still Needs refinement. \n\nHeres what i found\n\n1. Hogwarts legacy. \n\n50/50 shot if it chose X3d or Normal cores. however none were 100% parked\n\n2. Gmod/Source games \n\nX3d cores were preferred. normal cores parked \n\n3. Minecraft \n\nX3d cores were preferred. normal cores parked \n\n4. Voices of the void\n \nSeemed to prefer normal cores with most x3d cores parked\n\n5. Hitman 3\n\nSeemed to prefer normal cores\n\n6. KSP2. \n\nX3d cores were preferred.", "ups": 128, "permalink": "/r/Amd/comments/16n02bd/x3d_core_parking_seems_to_still_be_all_over_the/", "num_comments": 117}, "replies": [{"selftext": "Open Game -&gt; Windows Key + G -&gt; Settings Gear\n\nMake sure the \"Remember this is a game\" is checked then restart game, or if it doesn't exist, you are fine. Microsoft is slow at updating their game bar games list (starfield wasn't added until days after release... and thats one of their big ticket games). This will make it so the x3d ccd is active ccd whenever you play that game.", "ups": 124, "depth": 0, "author": "ChaosShadow", "replies": [{"selftext": "I have a 7950x3d and I had no idea about this. Thanks for the info", "ups": 40, "depth": 1, "author": "kearnel81", "replies": []}, {"selftext": "Option B is manually assign CPU sets (or core affinity if you want to be extra strict). Then you don't have to rely on Microsoft game settings/game bar at all, epecially since a handful of games are better on the non-3D cache CCD.", "ups": 10, "depth": 1, "author": "LordAlfredo", "replies": []}, {"selftext": "I'm using this method and haven't had an issue other than a recent beta which was not detected as a game in gamebar.  I set it as a game and didn't have any further issues.", "ups": 5, "depth": 1, "author": "shamowfski", "replies": []}, {"selftext": "it makes zero sense that the xbox game bar is used to make a amd cpu work properly. why isnt this handled by the amd 3d cache driver included in the amd chipset drivers?", "ups": 5, "depth": 1, "author": "mkdr", "replies": [{"selftext": "Because AMD. They should have just released both chiplets with 3D cache, but they went cheap, as if Zen didn\u2019t have insane profit margins already.", "ups": 5, "depth": 2, "author": "hpstg", "replies": [{"selftext": "The better solution would have been to always prefer the X3D chip, and not market \"up to 5.7 GHz\". Losing 10% performance in some applications from a clock speed deficit isn't nearly as noticeable as losing 30% from running software on the non-X3D", "ups": 3, "depth": 3, "author": "Noreng", "replies": []}, {"selftext": "https://www.techpowerup.com/308123/amd-reports-first-quarter-2023-financial-results-losing-money-just-like-intel-but-not-nearly-as-much", "ups": 2, "depth": 3, "author": "mkdr", "replies": [{"selftext": "Please learn to read fiscal results first. They have a 44% margin.", "ups": 1, "depth": 4, "author": "hpstg", "replies": []}]}]}]}]}, {"selftext": "Hence why the 7900X3D is a risky buy IMO. You're at the mercy of Microsoft and microcode updates.", "ups": 56, "depth": 0, "author": "-WielderOfMysteries-", "replies": [{"selftext": "Maybe a dumb question, but the 7800x3d doesn't suffer from this?", "ups": 5, "depth": 1, "author": "Saadieman", "replies": [{"selftext": "No. Chips that have 2 CCD's of unequal performance will be handicapped by the window's scheduler being intelligent enough to know which CCD to send a given task to. \n\nThe 7800X3D has only 1 CCD. Some 7800X3D's have 2 CCD's but they all have 3D V cache.", "ups": 11, "depth": 2, "author": "-WielderOfMysteries-", "replies": [{"selftext": "If it has 2 CCDs, one is sure just dummy. Because having 8 cores on 2 different CCDs will reduce performance compared to  having all of them at the same die.", "ups": 12, "depth": 3, "author": "Yilmaya", "replies": [{"selftext": "Yes the second one in 8 core parts is just a failed die.", "ups": 4, "depth": 4, "author": "BinaryJay", "replies": []}]}]}]}, {"selftext": "Unless you do the core parking, tweaks yourself, which can be upsetting to some", "ups": 6, "depth": 1, "author": "Caphelion", "replies": [{"selftext": "Upsetting probably isn't the right word.  Pain in the ass, definitely.  It is kind of silly to have to do anything at all.  I don't think their half x3d dual CCD solution is very elegant.  They should have made them full x3d or just stuck with it on their low core parts until they were willing to make them homogeneous.", "ups": 13, "depth": 2, "author": "BinaryJay", "replies": []}, {"selftext": "I would sooner buy a 13900k than I would be willing to do that for every game manually.\n\nIt's why the 7800x3d is such a good option compared to the two more expensive SKUs.", "ups": 3, "depth": 2, "author": "lagadu", "replies": [{"selftext": "At that point if I need cores I\u2019d just get a vanilla 7950x", "ups": 2, "depth": 3, "author": "letsfixitinpost", "replies": []}]}]}]}, {"selftext": "In case you find a workload that is working non-ideally in this regard, you can add a manual over-ride for the \"AMD V-Cache Performance Optimizer Driver\".\n\nJust ensure that the \"CPPC Dynamic Preferred Cores\" option located under CBS is set to \"Auto\" or to \"Driver\".\n\nJust add a new key under: HKEY\\_LOCAL\\_MACHINE\\\\SYSTEM\\\\ControlSet001\\\\Services\\\\amd3dvcache\\\\Preferences\\\\\n\n\\- Right-click the \"App\" folder in the tree and select New &gt; Key.\n\n\\- Name the newly added key (folder) to e.g., Hogwarts Legacy.\n\n\\- Right-click the \"Hogwarts Legacy\" key (folder) and select New &gt; String.\n\n\\- Right-click the \"Hogwarts Legacy\" key (folder) again and select New &gt; DWORD (32-bit) Value.\n\n\\- Name the newly added string (REG\\_SZ) as EndsWith. Double click the EndsWith string-entry and set the value to whatever the game binary (exe) is called, e.g., in case of Hogwarts Legacy: HogwartsLegacy.exe.\n\n\\- Name the newly added DWORD (REG\\_DWORD) as Type. Double click the Type value-entry and set it to either 0 or to 1, depending on the scheduling over-ride you prefer for this workload. 0 = \"Frequency\", 1 = \"Cache\".\n\nIf you are unsure if the entries you added are set or formatted correctly, refer to the sample entry for \"League of Legends\" already existing.", "ups": 18, "depth": 0, "author": "The-Stilt", "replies": [{"selftext": "this works quite well and is easy to do. at least on hogwarts legacy it made the X3D CC0 preferred. Reg edits are right up my alley. thanks", "ups": 8, "depth": 1, "author": "matthewv1998", "replies": []}, {"selftext": "This reassures me that getting the 7800X3D was the right decision. \n\nI can\u2019t imagine doing this for every game that has issues, remembering that I did the override, then undoing it when I delete the game (I know I don\u2019t have to but OCD).", "ups": 1, "depth": 1, "author": "KBDHands", "replies": [{"selftext": "R9 7900x3d for multitasking and games at same time .. u can play games / coding / editing / 3D model smoothly .. but R7 7800x3d only for games", "ups": -1, "depth": 2, "author": "Aromatic_Fishing_406", "replies": []}, {"selftext": "Why would you need to delete the entry?", "ups": 1, "depth": 2, "author": "Cygnal37", "replies": []}]}]}, {"selftext": "[https://github.com/cocafe/vcache-tray/](https://github.com/cocafe/vcache-tray/) get this app and set it to 'prefer v-cache' and add profiles for source-based games that don't actually benefit from vcache (if you are after avg fps, it actually boosts 1% lows quite nicely in tf2)", "ups": 9, "depth": 0, "author": "GuttedLikeCornishHen", "replies": []}, {"selftext": "The parking is just a shit hack job from AMD.\n\nWhat if you have other stuff wanting CPU time? You obviously want that to run on the frequency cores - you don't want them parked.\n\nThe frequency cores should be preferred by default. Use CapFrameX to put games onto the cache CCD, and see if it improves your frame rate. No parking required.\n\nIn my case I am heavily GPU bottlenecked in everything so I don't even bother.", "ups": 9, "depth": 0, "author": "Liam2349", "replies": [{"selftext": "Cores get unpacked automatically if there's a need. It's not that dumb of a system.", "ups": 1, "depth": 1, "author": "Star_king12", "replies": [{"selftext": "And then processes, including the game, are scheduled indiscriminately across both CCDs - this is why the system is dumb.", "ups": 2, "depth": 2, "author": "Liam2349", "replies": [{"selftext": "That's just Windows scheduler being itself.", "ups": 0, "depth": 3, "author": "Star_king12", "replies": [{"selftext": "Exactly. That is the problem. \n\nAMD needs to make their own equivalent of Intel's Thread Director, but it is more complex because it is not simply a case of one CCD being faster than the other - it depends on the workload.\n\nIdeally what we need is for AMD to make something functional with CPU Sets.", "ups": 3, "depth": 4, "author": "Liam2349", "replies": [{"selftext": "That's why it's never going to be the same as intel. It'll always have to live in the OS side of things to be able to update it quickly and to allow modifications by the user.", "ups": 1, "depth": 5, "author": "Star_king12", "replies": [{"selftext": "Which would be fine - AMD just needs a built-in solution.\n\nCapFrameX works really well for games - the issue is for any software that you want to go on the cache CCD, which you also want to allow to overflow to the next CCD if there is demand. There is no way to do this currently.\n\nIt is theoretically possible with CPU Sets, available through Process Lasso, but the feature did not work for me and the dev blamed Windows.", "ups": 2, "depth": 6, "author": "Liam2349", "replies": []}]}]}]}]}]}]}, {"selftext": "It's normal that 7900X3D is basically a 7600X3D in game.\n\nA lot of new game start to max out 6 cores and this will cause 7900X3D to perform worse than 7800X3D.\n\nYou should have got a 7900X or 7950X instead of 7900X3D if you think you really need 12 cores. 7900X3D is really a bad choice for obvious reasons. It's not good for gaming and also not good for multi-threaded workload.", "ups": 8, "depth": 0, "author": "Mikeztm", "replies": [{"selftext": "If OP actually needs 12 cores, a 7950X3D would pay for itself. The 7900X3D is for people who believe a 7800X3D isn't enough, but not willing to pay for a 7950X3D", "ups": 3, "depth": 1, "author": "Noreng", "replies": [{"selftext": "I can't imagine needing more than 8 cores but not being able to use 16.  It's a weird part and a weird way to save a few dollars IMO.", "ups": 2, "depth": 2, "author": "BinaryJay", "replies": []}]}, {"selftext": "if the Xs didnt run at 95c constantly, i'd consider but they do. and even if they're \"designed\" to, i'm not going to do that. \n\nFrom doing my own testing here at work, they are slower. however not slow enough to consider it a big compromise. \n\nIts coolable, the 3d cores are blatently faster when used right for $50 more. it would of been a 7900 or 7900x3d", "ups": -8, "depth": 1, "author": "matthewv1998", "replies": [{"selftext": "They are designed to run at 95c and are programed in firmware to maintain that temperature.\n\nX3D are designed to run at 89c and same firmware control applies.\n\nStop watching the temperature and just use the CPU. This is how \"dark silicon\" era chips works. They are impossible to cool and have to run under safety control to prevent them from melting itself in seconds--like those burnt X3D chips.\n\nHiger internal temperature gives them better cooling due to larger temperature difference. And higher temperature never means more heat.\n\nIntel chips are designed to run at 100c and 200W. This is the only way to bring up those chips using 7nm-class nodes and beyond. If you still insist to have your CPU running at \\~50c, you have to go back and buy old 14nm chips or start looking into sub-ambient cooling.", "ups": 8, "depth": 2, "author": "Mikeztm", "replies": [{"selftext": "nah, sorry. my chip runs at max 82c under full load in cinebench, rendering in blender and marvelous designer. i'm happy with that. \n\nit hasnt gotten better either. the same samples i tested at work from launch still now run at 95 when you sneeze at them.\n\n13th gen CPUs, even the 13900k became more tameable with bios updates. Just becuse its designed to run at 95 doesnt make it acceptable.", "ups": -10, "depth": 3, "author": "matthewv1998", "replies": [{"selftext": "It is designed to run at 95c so it is acceptable. We are not more knowledgeable with AMD's CPU than their engineers.\n\nJust you are refusing to accept the physics limit they are facing.\n\nThose amazing people settled with this temperature to bring us more performance and I think we should be thankful.", "ups": 8, "depth": 4, "author": "Mikeztm", "replies": []}, {"selftext": "my 7950x3d goes to 89c in cinebench according to HWInfo64. how does your goes to only 82?", "ups": 1, "depth": 4, "author": "ime1em", "replies": [{"selftext": "its a 7900x3d, 4 less cores does help. \n\ni have it on a Arctic 2 360", "ups": -4, "depth": 5, "author": "matthewv1998", "replies": []}]}]}]}]}]}, {"selftext": "Limiting normal cores frequency in bios will fix it but you may lose a bit of multithread performance (not much).\n\n[https://www.youtube.com/watch?v=LTkXnkSIcso](https://www.youtube.com/watch?v=LTkXnkSIcso)", "ups": 5, "depth": 0, "author": "Limi_23", "replies": [{"selftext": "This is just a roundabout way of setting affinity, but with the side effect of reducing performance for anything that doesn't benefit from extra cache. The main loss will be in games that prefer high frequency to more cache. CS:GO is a common example because it doesn't benefit from the extra cache, and it doesn't use many threads. (So you can actually get the 5750MHz max boost from the frequency cores)\n\nIt doesn't reduce the performance of heavy all core applications because the CPU won't be hitting the 5050MHz limit often.\n\nI don't use the X3D optimizer or game bar. My setup is to use \"Prefer frequency\" in the BIOS + a custom affinity changer tool. Most background processes are locked to CCD1 (frequency) affinity. Games are locked to CCD0 (cache) or CCD1 (frequency) depending on what gives better performance. I have yet to find a game that performs better with all core affinity compared to locked CCD0/CCD1. Applications that can scale with more cores are either left alone or forced to all core affinity.", "ups": 3, "depth": 1, "author": "jedi95", "replies": []}, {"selftext": "That's how I saw it one.  Used 5050 vs 5250, I think.", "ups": 2, "depth": 1, "author": "farmeunit", "replies": []}]}, {"selftext": "Use process lasso and configure cpu sets or affinities, change power plan to avoid cores to park", "ups": 3, "depth": 0, "author": "4lbertGG", "replies": []}, {"selftext": "Myself being a PC power user, I prefer total and complete control over my device.\n\nThat means to use Ultimate Performance power plan, Prefer Frequency inside bios and I have a program that sets affinity after I start a game to be on 3D cores.", "ups": 5, "depth": 0, "author": "lexsanders", "replies": [{"selftext": "&gt; total and complete control\n\n&gt; uses Windows\n\n\nChoose one.", "ups": 19, "depth": 1, "author": "Star_king12", "replies": [{"selftext": "Bro... Seriously... Shove off.", "ups": 4, "depth": 2, "author": "lexsanders", "replies": [{"selftext": "Hurr durr look at me imma powah user", "ups": -2, "depth": 3, "author": "Star_king12", "replies": [{"selftext": "[deleted]", "ups": 2, "depth": 4, "author": "[deleted]", "replies": [{"selftext": "I use dual boot", "ups": -7, "depth": 5, "author": "Star_king12", "replies": [{"selftext": "Me too.  \nMy Linux is Arch Linux and I like vegan food.   \n(No joke)", "ups": 1, "depth": 6, "author": "OSSLover", "replies": [{"selftext": "We should have a child.", "ups": 0, "depth": 7, "author": "Star_king12", "replies": [{"selftext": "And call it our Puppy Linus.", "ups": 0, "depth": 8, "author": "OSSLover", "replies": [{"selftext": "I prefer Pacman.", "ups": 1, "depth": 9, "author": "Star_king12", "replies": []}]}]}]}]}]}]}]}, {"selftext": "linux ftw", "ups": -8, "depth": 2, "author": "WelcomeToGhana", "replies": [{"selftext": "Amen.", "ups": 0, "depth": 3, "author": "Star_king12", "replies": [{"selftext": "[deleted]", "ups": 2, "depth": 4, "author": "[deleted]", "replies": [{"selftext": "Windows is great if you want to read a pdf that is actually a screensaver that's going to steal all your browser cookies allowing an attacker to clone your browser.\n\nIt's also great if you want to update every app on your PC from ONE application that tracks versions of everything installed on your PC, instead of having 50 updaters pop up every time you boot your PC.", "ups": 2, "depth": 5, "author": "Star_king12", "replies": [{"selftext": "Not worth discussing the truth with ignorants, they don't understand how a computer operates, they're part of the problem.", "ups": 1, "depth": 6, "author": "WelcomeToGhana", "replies": []}]}, {"selftext": "You understand jack shit about computers, why are you talking?", "ups": 1, "depth": 5, "author": "WelcomeToGhana", "replies": []}]}]}]}, {"selftext": "This is why Linux is king. Plus it has the added benefit of being far better with AMD", "ups": -7, "depth": 2, "author": "IrrelevantLeprechaun", "replies": [{"selftext": "Does Linux have support for every new AAA releases? How good is it for gaming nowadays?", "ups": 4, "depth": 3, "author": "javiktheprothean_", "replies": [{"selftext": "Pretty good actually, most recent games release with Steam Deck support in mind which makes it possible to run them on Linux without much hassle.", "ups": 1, "depth": 4, "author": "Star_king12", "replies": []}]}]}]}, {"selftext": "Out of curiosity, why core affinity and not CPU sets?\n\n* Core Set is \"prefer these threads, if unavailable use whatever is available\"\n* Affinity is \"only schedule on these threads\"\n\nWhile Sets means you do sometimes get non-ideal scheduling it also means less possible thread contention if you're actively doing other things in parallel (and if you're not then the game gets ideal cores anyways).\n\nAlso are you just assigning every game to CCD0 or actually setting per application? Some games actually run better on CCD1", "ups": 0, "depth": 1, "author": "LordAlfredo", "replies": [{"selftext": "CPU sets don't actually work.", "ups": 0, "depth": 2, "author": "Liam2349", "replies": [{"selftext": "That usually means whatever's running needs more threads than it's being allocated and/or the threads you want it to use are busy. Windows will *try* to use the Set but if none if those threads are available it will pick the next non-preferred core that is available. Sets only work better than affinities if you tune a *lot*, but can be better if used correctly alongside process/service controls since it means less time for secondary threads to wait and higher overall processor utilization. Otherwise it ends up similar to running with zero tuning but with slightly better scheduling behavior on average.\n\nThat said you're not generally wrong given it's a *lot* easier to set affinities and forget + in most circumstances games don't need more threads at a time than one CCD can provide/has available. It's more notable if you're doing other things in parallel when games may become more thread starved.", "ups": 0, "depth": 3, "author": "LordAlfredo", "replies": [{"selftext": "I tried a few programs with Process Lasso, the CPU usage was definitely well below the 8 cores I set it to, and it just didn't work in anything I tried. (CPU Sets).", "ups": 1, "depth": 4, "author": "Liam2349", "replies": [{"selftext": "What were you trying with? Very few current games can effectively use 8 threads (in fact most you'll only see high usage on 1-4).", "ups": 1, "depth": 5, "author": "LordAlfredo", "replies": [{"selftext": "Exactly, so limiting to a CPU set of 8 should be fine. I remember trying it with RPCS3 and Mordhau, but they still ran on the frequency cores when using CPU Sets.", "ups": 2, "depth": 6, "author": "Liam2349", "replies": []}]}]}]}]}, {"selftext": "Because that's what cap frame x works. Ask the dev not me. Press a button and switches affinity.", "ups": 1, "depth": 2, "author": "lexsanders", "replies": []}]}, {"selftext": "Does this also apply to a 5800x3d? I installed it and never configured it.", "ups": 0, "depth": 1, "author": "PiercingHeavens", "replies": [{"selftext": "No", "ups": 1, "depth": 2, "author": "lexsanders", "replies": []}]}, {"selftext": "\"Ultimate Performance power plan\" \"performance\"\n\nchoose one.\n\nBalanced mode gives you better performance on power hungry CPUs.", "ups": -2, "depth": 1, "author": "Mikeztm", "replies": [{"selftext": "Idk what you mean. Ultimate perf disables parking. I was being on topic.", "ups": 0, "depth": 2, "author": "lexsanders", "replies": [{"selftext": "Enable parking gives you more boost capacity on Intel and correct x3d scheduling on AMD", "ups": 0, "depth": 3, "author": "Mikeztm", "replies": []}]}]}]}, {"selftext": "Not all games are recognized as a game by Xbox Game Bar. There is an option within Xbox Game Bar \"Remember this is a game\".  You will need to restart the game you are playing for it to work. CPU heavy games like Star Citizen will activate CCD1, it is normal behavior.", "ups": 2, "depth": 0, "author": "codekeying", "replies": []}, {"selftext": "I have a 7950x3d and have observed zero issues with core parking.  Everyone else has covered the use of xbox game bar so I\u2019ll save the typing.", "ups": 2, "depth": 0, "author": "Purgent", "replies": []}, {"selftext": "Also an owner of the most hated cpu on reddit, I found core parking got wonky if I enabled settings in Bios that effect the CPU, stuff like raising the boost frequency, upping the scalar, or enabling core performance boost. I havent tested enough to nail down if it's just one setting that makes core performance weird, but setting the bios to default always seems to resolve issues.", "ups": 2, "depth": 0, "author": "toxicThomasTrain", "replies": []}, {"selftext": "i do a lot of 3D art and gaming. truthfully i would of bought a 7950x3d but i only had $800 for a whole platform swap and also, OOS at the microcenter i went to anyway.", "ups": 2, "depth": 0, "author": "matthewv1998", "replies": [{"selftext": "I bought a 7950x3d bundle at Microcenter for $799.  Included mobo and 64gb of ram.  This was about 4 weeks ago and I see now it\u2019s no longer available, unfortunately.", "ups": 1, "depth": 1, "author": "Purgent", "replies": [{"selftext": "i would of loved that and wasnt an option when i was there. \n\nended up with this, a 7900x3D, MSI X670-P wifi and 32gb gskill 6000 for 797 bucks. \n\nstill not awful, but could of been signifcantly better.", "ups": 2, "depth": 2, "author": "matthewv1998", "replies": []}]}]}, {"selftext": "In bios, try setting `L3 as Numa domain` to on, see what happens", "ups": 1, "depth": 0, "author": "Kiseido", "replies": [{"selftext": "Can you describe the intended affect of this setting?\n\nI ask to better understand how I can A/B test this in an accurate manner.", "ups": 1, "depth": 1, "author": "Emotional_Deer_6967", "replies": [{"selftext": "It makes the bios/uefi tell the operating system that there is a latency penalty to access data from one domain inside another domain. Ie when a thread is on ccx1, accessing the cache of ccx1 is trivial but accessing the cache of ccx2 from a core in ccx1 will be crazy slow in comparison.\n\nThis allows the os to more optimally schedule the various threads onto the various cores, and is in the recommended tuning guide for Epyc, which his just a giant version of Zen", "ups": 3, "depth": 2, "author": "Kiseido", "replies": []}]}]}, {"selftext": "Are you seeing performance difference when the monitor 3d is unparked? They may be doing random work and not game thread work. Though I'm pretty sure the whole reason they have to park is because the scheduler doesn't know to keep the game threads on the 3d cores so then being unparked should mean they can inadvertently be scheduled into the frequency cores.\n\nI'm pretty sure there's bios setting to change which ccd is the preferred cores. If you run HWiNFO you can see the core performance order, of your frequency cores have lower numbers (higher performance, preferred execution on those) you could flip the bios setting and should see the 3d cores get the lower numbers, then the scheduler at least tries to keep heavy load stuff on the 3d even if the frequency ccd is unparked.\n\nMaybe the 3d cache dri er overrides those settings, possibly on the fly, idk.", "ups": 0, "depth": 0, "author": "NewestAccount2023", "replies": [{"selftext": "[deleted]", "ups": 0, "depth": 1, "author": "[deleted]", "replies": [{"selftext": "Yes people have also shown how cores unpark and the game runs fine still, it's more complicated than \"it's always busted\". That's why I asked op if they saw actual performance drop or just left resource monitor open and saw unparked cores while gaming.", "ups": 1, "depth": 2, "author": "NewestAccount2023", "replies": []}]}]}, {"selftext": "Turn em off in bios", "ups": 0, "depth": 0, "author": "Icy_Durian2606", "replies": []}, {"selftext": "You need to follow the guide by AMD using XBOX control pannel.", "ups": 0, "depth": 0, "author": "IGunClover", "replies": []}, {"selftext": "Got a 7950x3d and using Process Lasso, it's  working  perfect. I still thing it's the best way to run the 2 cdds x3d chips.", "ups": 0, "depth": 0, "author": "Jix101", "replies": []}, {"selftext": "I have the 7950x3d and I have had really good results with process lasso. I set the cpu in bios to prefer CCD0 (cache). Then in process lasso I set everything else (like C, D, E drives) to only use CCD1 (frequency). This puts all of my games on drive G using only the cache CCD while all other processes use the frequency CCD.\n\nSome have mentioned the opposite. Setting bios to Frequency and process lasso to only add my game drive to cache but I noticed sometimes it didn't work as the bios would still control some things to prefer frequency. YMMV", "ups": 0, "depth": 0, "author": "BulgersInYourCup42", "replies": []}, {"selftext": "Isn't w11 bad?", "ups": 0, "depth": 0, "author": "RareParadox", "replies": []}, {"selftext": "Intel hardware thread director/scheduler is the best. Amd should have done the same for scheduling between cores. What happens when gamebar breaks in a windows update?", "ups": -1, "depth": 0, "author": "nathsabari97", "replies": []}, {"selftext": "Lmao y u need 12c? Just bc u \u201cneed\u201d 12 cores?", "ups": -11, "depth": 0, "author": "ibeerianhamhock", "replies": [{"selftext": "They posted another comment that said they do a lot of 3D art which will utilize the extra cores. No need to sound so condescending.", "ups": 4, "depth": 1, "author": "RedChaos92", "replies": [{"selftext": "I\u2019m surprised op didn\u2019t get a 7950x. You don\u2019t need a x3d to still game at high fps and if you are doing 3d art, you benefit from the cores.", "ups": 8, "depth": 2, "author": "stubing", "replies": [{"selftext": "i only had $800 to spend to platform jump, a new CPU, board with at 3-4 m.2 slots and 32gb of ram. a 7950 in any flavor wouldnt of worked. would of been more ideal? yeah.", "ups": 1, "depth": 3, "author": "matthewv1998", "replies": [{"selftext": "Is the price of the 7900x3d that much cheaper than a regular 7950x?", "ups": 0, "depth": 4, "author": "imizawaSF", "replies": [{"selftext": "450 vs 549 for a normal one and a 650 for a x3d.", "ups": 2, "depth": 5, "author": "matthewv1998", "replies": []}]}]}]}]}]}, {"selftext": "AMD screwed up with the core parking solution.  They should have either put stacked cache on both dies, or made a deal with the maker of Process Lasso and make a user-friendly interface for it.\n\nIn short, don't use the core parking method.  Get rid of that driver (do a search on how to remove it without re-installing Windows, which no one should ever do as anything but the last choice), then get Process Lasso.  A similar program would work as well, but that's the most popular and well-known.\n\nSet core affinities for all game processes to the stacked-cache cores, and when you plan to run a heavy workload in the background while gaming, assign the non-stacked-cache cores to those processes.", "ups": -2, "depth": 0, "author": "RealThanny", "replies": [{"selftext": "They did make them with cache on both dies, but obviously you know better then all of amd right lmao\n\nhttps://youtu.be/7H4eg2jOvVw?si=ws_6I6rKyRBWVabz", "ups": 1, "depth": 1, "author": "Tgrove88", "replies": [{"selftext": "What an asinine comment.  They tried it out in the labs, but never put it out to market.  They thought, wrongly, that they'd do better with the current single-die solution.  \n\nIt doesn't take a genius to realize that there was much more demand for the product they didn't release, and that it would have worked better for all scenarios where their chosen solution doesn't.", "ups": 0, "depth": 2, "author": "RealThanny", "replies": []}]}]}, {"selftext": "I've found it best to just manually assign core sets 0-11 (CCD0) through Process Lasso and had zero scheduling issues since. Even keep both CCDs enabled at all times with Discord/Firefox/etc on CCD1's cores. Flip this around for games that benefit more for the speed cores than cache cores.", "ups": 1, "depth": 0, "author": "LordAlfredo", "replies": []}, {"selftext": "Why doesn't AMD just put the 3D cache on both chips for the 7950X3D and make it look like extra cache?", "ups": 1, "depth": 0, "author": "ElixirGlow", "replies": [{"selftext": "Watch this they explain why \n\nhttps://youtu.be/7H4eg2jOvVw?si=ws_6I6rKyRBWVabz", "ups": 1, "depth": 1, "author": "Tgrove88", "replies": []}]}, {"selftext": "And that is why I just don't buy X3D &gt;X800 CPUs, same for Intel since p and e cores... I know it gets better or have gotten already, but if I pay this much for brand new CPU I expect it to work perfectly without tinkering with settings and stuff out of the box.\n\nSomeone will say I'm wrong or it got better, but let's be honest - did I pay X $ for a product or being a beta tester?", "ups": 1, "depth": 0, "author": "dharknesss", "replies": []}, {"selftext": "aw", "ups": 1, "depth": 0, "author": "Hekatia_Arrowbane", "replies": []}, {"selftext": "I\u2019m curious, is this CPU a 6+6 or 8+4 configuration?", "ups": 1, "depth": 0, "author": "Stormljones3", "replies": [{"selftext": "With all AMD Zen CPUs so far, all CCXs must have the same number of enabled cores. 8+4 is not allowed. The exception is if all cores in the CCX are disabled. (For Zen 3/Zen 4, 1 CCX = 1 CCD)", "ups": 3, "depth": 1, "author": "jedi95", "replies": []}, {"selftext": "6+6", "ups": 2, "depth": 1, "author": "matthewv1998", "replies": []}]}, {"selftext": "Is project lasso really that much of a hassle? Genuinely curious.", "ups": 1, "depth": 0, "author": "somewhat_moist", "replies": [{"selftext": "i havent installed it yet so hard to say. however, adding a Regedit to mark it in stone worked well.", "ups": 1, "depth": 1, "author": "matthewv1998", "replies": []}]}, {"selftext": "The ParkControl app has been magic on my 5800X3D, not sure how it applies to the latest Ryzen generation", "ups": 1, "depth": 0, "author": "Super-Stanky772", "replies": []}, {"selftext": "Just in case people dont know amazon has the 7950X3D at 594 now.", "ups": 1, "depth": 0, "author": "Eagle1967", "replies": []}, {"selftext": "Games that try to utilize 8 cores (like Hogwart's) will have this behaviour. Because each of your CCDs are only 6 cores, it has no choice than to activate your whole CPU.", "ups": 1, "depth": 0, "author": "RedTuesdayMusic", "replies": []}, {"selftext": "Process Lasso + CPU sets worked wonders for me.", "ups": 1, "depth": 0, "author": "EolasDK", "replies": []}, {"selftext": "Try using a program like process lasso to set cpu affinity /cores and isolate your games to the chiplet / cores that has the v cache", "ups": 1, "depth": 0, "author": "enso1RL", "replies": []}, {"selftext": "ngl if u can still return it i\u2019d switch to a 7800x3d", "ups": 1, "depth": 0, "author": "_KingDreyer", "replies": [{"selftext": "nah, i just got off 8 cores. 8 cores wasnt enough.", "ups": 1, "depth": 1, "author": "matthewv1998", "replies": []}]}]}
{"post": {"title": "5800X3D Lottery Winner?", "subreddit": "Amd", "selftext": "So i upgraded from a 5600 non x, installed the 5800x3d with a new Liquid Freezer ii, at first my temps were maxed 90c with 0% load, then I figured out i accidentally got the AIO pump cable under the cooler somehow... but with that resolved and it mounted with the offset bracket, i went to do some tuning because i had read the temps can be crazy on the 5800x3d, on OCCT torture testing i was seeing 100% usage, completely locked at stable at max boost clock, 4.5ghz and my temps were right around 58C completely stock. I did a -20 under volt on all cores, and my temps dropped to 48C-53C, still max boost clock and usage. Is there any other way for me to boost performance on this chip? Already tuned it with the PBO2 tuner, but it seems like I might have a lot of headroom if this was a normal OCable CPU.  \nThat being said my performance is nuts now. Did a couple of benchmarks. I've got a 3070 FE as well.  \nAll @ 1440p, DLSS Quality  \nRed Dead - Ultra settings, Before - 65fps, After - 101fps  \nCyberpunk - High/med/ultra mix, Before - 45ish, After - 74fps  \nStarfield - Before 58-70fps, After - 58-70fps.... lol  \n\n\nIt just feels so good. But i'd always like a little extra performance..", "ups": 0, "permalink": "/r/Amd/comments/16n22og/5800x3d_lottery_winner/", "num_comments": 116}, "replies": [{"selftext": "post a cinebench 2024 single/multi core result... and see", "ups": 22, "depth": 0, "author": "DHJudas", "replies": [{"selftext": "Still prefer 23 myself atm. 2024 is giving me mixed results", "ups": 9, "depth": 1, "author": "Obvious_Drive_1506", "replies": [{"selftext": "yeah, the new one has some jank to it", "ups": 1, "depth": 2, "author": "pyr0kid", "replies": []}, {"selftext": "yess ,.  \ncbr23 only use cpu ,. less affected by ram.  \nnew cb 2024 more affected by ram. so low ddr4 vs high ddr5 give completely different scores", "ups": 1, "depth": 2, "author": "MAD2310", "replies": []}]}, {"selftext": "It maxed out at 70 for a brief second, but hovering around 66. Very quiet computer rn lol. It actually scored higher than the 5800x. 837 points. \n\n[cinebench 2024 scores and temps](https://imgur.com/a/6fyNLRD)", "ups": 1, "depth": 1, "author": "lehman2724", "replies": [{"selftext": "Single core looks weirdly low. Did you let the full render finish?\n\nFor reference here are my stock r5 7600 scores, which I assume to be roughly equivalent to a 5800x3d. [Single 114](https://cdn.discordapp.com/attachments/170536476059107328/1154187717811392522/image.png) [Multi 835](https://cdn.discordapp.com/attachments/170536476059107328/1154183257135534150/image.png)", "ups": 1, "depth": 2, "author": "Krendrian", "replies": [{"selftext": "That was multi core, lemme run a single core test real quick. But yes I did let the render finish.", "ups": 1, "depth": 3, "author": "lehman2724", "replies": []}, {"selftext": "Well it's taking absolutely forever but here's where it's at now. 1500ish? \n\n[single core r23](https://imgur.com/a/ldckTdd)", "ups": 1, "depth": 3, "author": "lehman2724", "replies": []}, {"selftext": "1466 single core", "ups": 1, "depth": 3, "author": "lehman2724", "replies": []}, {"selftext": "Oh wait you're on 2024 cinebench, my results were on r23", "ups": 1, "depth": 3, "author": "lehman2724", "replies": []}]}]}]}, {"selftext": "Those numbers are physically impossible, as far as i am aware. Far beyond being an outlier or perfect sample. What does R23 multi core look like in power draw and score?", "ups": 16, "depth": 0, "author": "LongFluffyDragon", "replies": [{"selftext": "I mean I'm just reporting what happened. Cinebench was a little higher temps, but OCCT torture test, was literally at 48C. I almost don't believe it either. Like. Something seems inaccurate there.. lol", "ups": 3, "depth": 1, "author": "lehman2724", "replies": [{"selftext": "Utterly physically impossible. Are you using some old garbage like hwmonitor?", "ups": 9, "depth": 2, "author": "LongFluffyDragon", "replies": [{"selftext": "I'm using hwinfo, OCCT AND afterburner. They are all giving me the same numbers. Identical. I would like to know what's happening here too...\n\nHere's everything so far.. I'll run whatever programs you want lol. I think cinebench is a little warmer, but I'm well past the 5, minute heat saturation time and still 45C\n\n[here's everything so far](https://imgur.com/a/pCDcPEp)", "ups": 2, "depth": 3, "author": "lehman2724", "replies": [{"selftext": "ah right,.  \ntry this settings  \ndata set - small     (not large)  \nmode - extreme    (not normal)  \nload type -variable  \n\n\nshare result pls", "ups": 1, "depth": 4, "author": "MAD2310", "replies": []}]}, {"selftext": "What sw do you recommend?", "ups": 2, "depth": 3, "author": "3lit_", "replies": [{"selftext": "Hw info or cpu-z", "ups": 2, "depth": 4, "author": "Alucardhellss", "replies": [{"selftext": "Ty", "ups": 1, "depth": 5, "author": "3lit_", "replies": []}]}]}, {"selftext": "Those things are not impossible. Boost stays consistent once you have the CPU below 60 degree. Goes out for any ryzen.", "ups": 1, "depth": 3, "author": "Jism_nl", "replies": [{"selftext": "Sure, you can probably do it with liquid nitrogen. Not with an AIO, not even remotely close. 70C is considered an extremely low load temp, and OCCT is in a class of it's own.", "ups": 1, "depth": 4, "author": "LongFluffyDragon", "replies": [{"selftext": "&gt;Sure, you can probably do it with liquid nitrogen. Not with an AIO, not even remotely close. 70C is considered an extremely low load temp, and OCCT is in a class of it's own.\n\nNonsense, again....\n\nI have a 2700X - it's consistent below 60 degree and because of that all core boost at 4.2Ghz for a good 5 minutes before the water heats up. After that it drops above 60 degree and the boost lowers the clocks.\n\nThe trick was, adding good thermal paste (MX5 in my case), a 360mm aio with 6 fans in both push pull, a aggresive fan profile, a CPU Socket washer mod (better / more pressure), a little undervolt and that's it.", "ups": -1, "depth": 5, "author": "Jism_nl", "replies": [{"selftext": "If the massive difference in thermal density and transfer between Zen+ and Zen3 with vcache is not immediately obvious, you lack the knowledge to contribute anything meaningful to this topic.\n\nAnd of course it is worth mentioning that on top of the huge density difference and despite the absurdly, uselessly overkill nature of your setup, OP is reporting lower temperatures with less cooling.\n\nMaybe work on rectifying that before you try to be a contrarian for no particular reason.", "ups": 3, "depth": 6, "author": "LongFluffyDragon", "replies": [{"selftext": "There's people with on avg 60 degree in gaming and 71 degree on Cinebench on the X3D. If your temperature is worse, your application just sucks or you don't have a proper cooler.\n\n\"Just because you have a AIO\" does'nt mean it's good. There's a few variables at play with it comes to taking heat as fast as possible away:\n\n\\- Pump flow, the faster the better\n\n\\- Component quality, copper block etc\n\n\\- Used fans in radiator\n\n\\- Used thermal application\n\n\\- Fan profile\n\nThe more static pressure the better the surface of a radiator gets cooled. Now the way you place your rad inside your casing matters too. Obvious frontal intake with the heat being pushed through your case is not effective as putting the rad on top with the heat pushed out out of your case.\n\nI can go on for a while but i think you get my point. There's alot of people not even diving in any of the material these days and blaming it on AMD while in my case for example, knowing exactly how boost works, what the threshold is and made proper adjustments for that to get the best out of PBO.\n\nIs that so difficult to dive in for one night and try to optimize your setup instead of dumping your question or blaming AMD for higher temps? Yes there's a bit difference but there's a majority who gets 60 degrees. So someone is doing something wrong.", "ups": -1, "depth": 7, "author": "Jism_nl", "replies": [{"selftext": "All of those are irrelevant or near-irrelevant to solving the issue of heat transfer across the IHS from increasingly dense chiplets. It is basic physics, there is no ambiguity or special case for CPUs.\n\nAs proven across vast numbers of professionally conducted tests.\n\nGet a modern CPU, then you will understand and stop doing.. whatever you are doing. Hard to tell what, exactly, you are ranting about at this point, but it seems to be trying to blame things on other people under the projection of them blaming AMD for unspecified things.", "ups": 2, "depth": 8, "author": "LongFluffyDragon", "replies": []}]}]}]}]}]}]}]}, {"selftext": "[deleted]", "ups": -2, "depth": 1, "author": "[deleted]", "replies": [{"selftext": "What you observed is a little thing called a 'GPU bottleneck', regularly seen at 4k.\n\n50% gain from zen2 to a 5800X3D is very believable and normal, a lot of that is from the near 100% latency increase between architectures. Even ignoring the cache size, it is much faster than a 3600X in any situation. WoW (and many old mmo/rts games) gets closer to 75-100% just over zen3.\n\nOP also seems incredulous and likely just has bugged sensors.", "ups": 3, "depth": 2, "author": "LongFluffyDragon", "replies": [{"selftext": "I don't believe it's bugged sensors.... temps were accurate on my old setup with an air cooler. 70-80, when I didn't mount the new cooler right it shit up to 90 very fast, now that it's on correctly it is stuck below 50. I believe it's not getting enough power now. But I'm not sure why or how that is.", "ups": 1, "depth": 3, "author": "lehman2724", "replies": []}]}]}]}, {"selftext": "58C on OCCT all core?  Can you post a screenshot?  TIA.", "ups": 4, "depth": 0, "author": "SeniorChiefPogi", "replies": [{"selftext": "This is notorious lie, but w/e ppl Wright here all kind of bullshit things.\n\nI have same combo (5800x3d + liquid freezer ii 240) and:\ncine r23 reaches arround 80\nAida 64 everything stress (+fpu) around 85\nOcct 80+ (can reach 90 if you test max power draw 130-135w)\n\nAnyway this cpu is hella hot and having 50 C on max load... Ye bro maybe if you live on south pole with open case", "ups": 10, "depth": 1, "author": "Vuruna-1990", "replies": [{"selftext": "I know lol.  I have the same CPU with a Noctua NH-U12A.  I get about 82C on R23.", "ups": 2, "depth": 2, "author": "SeniorChiefPogi", "replies": []}, {"selftext": "I promise you this is not a lie, unless my system is reporting inaccurate temps, literally ran the tests last night. I'll run it again in the morning and post it here. With videos if you want. This is literally my first post in this sub, I don't even use reddit, just thought I'd see if this was normal or what is actually happening if the temps I'm seeing are not accurate.", "ups": 2, "depth": 2, "author": "lehman2724", "replies": [{"selftext": "Is it boosting above base clocks? My friend recently had about 50c on load with pbo -25 and found that CPU was stuck at 3.5ghz.", "ups": 3, "depth": 3, "author": "Kazurargh", "replies": [{"selftext": "Mine is boosted to max boost clock.", "ups": 1, "depth": 4, "author": "lehman2724", "replies": []}, {"selftext": "Yeah this happens when the motherboard is on an older BIOS", "ups": 1, "depth": 4, "author": "wutanglan90", "replies": [{"selftext": "Im on the most up to date bios tho. I updated before I upgraded to avoid any issues. It's the latest version", "ups": 2, "depth": 5, "author": "lehman2724", "replies": [{"selftext": "Okay, but I wasn't replying to you.", "ups": 0, "depth": 6, "author": "wutanglan90", "replies": []}]}, {"selftext": "Indeed. But for my friends case, it was the latest bios causing the issue. Downgrading fixed it.", "ups": 1, "depth": 5, "author": "Kazurargh", "replies": [{"selftext": "Ah... interesting. I mean on not on a bets bios or anything, just the most up to date", "ups": 1, "depth": 6, "author": "lehman2724", "replies": []}]}]}]}, {"selftext": "Could be you are not pulling max power.\n\nCheck package power, cpu EDC and Core Utility (should be 130% on those tests)", "ups": 0, "depth": 3, "author": "Vuruna-1990", "replies": [{"selftext": "I think you were 100% right. OCCT power would go above 72, cinebench 2024 is hitting 65C and 100W", "ups": 2, "depth": 4, "author": "lehman2724", "replies": []}, {"selftext": "I have no idea, but here's OCCT all core and everything. \n\n[everything so far](https://imgur.com/a/pCDcPEp)", "ups": 1, "depth": 4, "author": "lehman2724", "replies": []}]}]}]}]}, {"selftext": "No idea what motherboard you have, but I'd suggest updating your BIOS and setting the negative offset there instead of using the PBO tuner, that method was useful when you had no access to those settings in BIOS.\n\nMake sure to test your system with different utilities though, OCCT is a good start but you need something like CoreCycler with various presets to test the individual cores for low load &amp; high boost scenarios, some cores don't play nice when the negative offset is set too low for them.\n\nIf you're willing to go down the rabbit hole of BCLK overclocking (which is quite tricky, you can easily make your system extremely unstable since it messes up SATA and other stuff) or want to play around with RAM OC (which also doesn't affect this CPU that much, unless you have some garbage RAM with high latency), you can do that to squeeze a bit more from it, but other than that the 5800X3D is very good as it is and it does even better if you win the lottery and can run -30 on all cores, many people will tell you to do that and call it a day but that's a terrible advice if you don't make sure that it's 100% stable first. For example my 5800X3D can handle -30 on all cores except one.", "ups": 5, "depth": 0, "author": "overqual", "replies": [{"selftext": "Okay, sounds good. I could use a mobo upgrade, but I've got a msi b550 gaming plus.", "ups": 1, "depth": 1, "author": "lehman2724", "replies": []}, {"selftext": "Would you advise using MSI Kombo strike or is Curve Optimizer better?", "ups": 0, "depth": 1, "author": "Appropriate_Pen4445", "replies": [{"selftext": "Kombo Strike is basically one button CO, afaik 1 should do -10, 2 -20 and 3 -30 but the most important thing you need to know is that this is an all cores offset, as I said before - you need to test the capabilities of each core and set the CO individually for them, it takes some time but will provide best results.\n\nAlso I wasn't paying much attention to the other stuff you wrote in your original post but those numbers are... very weird, there is something sketchy going on, are you using HWInfo for monitoring? You shouldn't be using anything else if you want sensible readings, and as others mentioned it looks like you might be PPT/TDC/EDC limited with those temps because they do seem quite unrealistic indeed.", "ups": 2, "depth": 2, "author": "overqual", "replies": [{"selftext": "Thanks for the replay! For the second part, I'm not the OP, hope he reads this.\n\nI was asking about KS as if it has some optimisation built in since I don't have time atm for testing cores, values etc. I tried -30 all cores yesterday, and I've noticed some differences in-game and decided to revert undervolting until I can to it properly.", "ups": 0, "depth": 3, "author": "Appropriate_Pen4445", "replies": [{"selftext": "Oh... my morning brain forgot to check the username, sorry about that lol\n\nNo problem, and yeah - it's pretty much the same, just more simplified I guess and all-cores-only. MSI was first (iirc) to allow any kind of CO with this on their boards for the 5800X3D before they pushed the new AGESA, which enabled it natively. No idea about in-game differences but instabilities from very low CO could be really weird, random and quite hard to troubleshoot if you don't know that CO caused it in the first place. The best case scenario is that you'll get WHEA's (most likely cache hierarchy), those will actually show you which cores are throwing errors. Another case is that you'll get random hard freezes, BSOD's or straight up resets and those could be scary since they usually suggest that something else went wrong. I was quite \"lucky\" and was getting random freezes without any BSOD's or WHEA's, took me a while to troubleshoot this and find out what was happening.", "ups": 2, "depth": 4, "author": "overqual", "replies": []}]}]}, {"selftext": "I use kombo strike 3 with my 5800x3d. Since then temps dropped about 10-15\u00b0c from 80-85 to 70-75 celcius. I use cheap air cooler but these temps are fine for me. Clockspeed constant @4.45ghz according to MSI overlay. Havent done much testing besides that. I have this chip just over a week now.", "ups": 1, "depth": 2, "author": "PrettyQuick", "replies": [{"selftext": "Thanks, will try it later today.", "ups": 0, "depth": 3, "author": "Appropriate_Pen4445", "replies": [{"selftext": "NP, i dont have any experience messing with the CPU manually but the way i understand the kombo strike is just a undervolting for dummies kinda feature. You could probably get better performance doing it manual but so far the results seem fine to me. Temps and performance been good.", "ups": 1, "depth": 4, "author": "PrettyQuick", "replies": []}]}]}]}]}, {"selftext": "My first question, is what did you set all the power limits in BIOS? Did you even change them?\n\nSecond, you need to use HWInfo and see that the core clock speeds and the effective clock speeds are within 20Mhz of each under loaded testing.. Like Linpack Xtreme...\n\nI am going to guess it's only running as far as the current power limits are set... Increasing them can give a body to performance when done correctly.", "ups": 5, "depth": 0, "author": "X-KaosMaster-X", "replies": [{"selftext": "I lowered the voltage -20 on every core with PBO2 tuner, didn't even touch the bios. With all the stories I've heard overvolting it sounds like a bad idea lol", "ups": 1, "depth": 1, "author": "lehman2724", "replies": [{"selftext": "I didn't say overvolt..I said allow it more power....\n\nThere are three power settings..but increasing them increases heat also...but only by a margin.\n\nAnd oh, you didn't use BIOS..ok, well watch some videos about PBO maybe...try learning how it works .but be aware, there are some bad videos with bad settings that don't work on every system.", "ups": 1, "depth": 2, "author": "X-KaosMaster-X", "replies": [{"selftext": "You can't increase power limits past stock on X3D...", "ups": 1, "depth": 3, "author": "Orosta", "replies": [{"selftext": "Yes you can\n.you just can't add boost clock...they updated that like 3 AGESA before the current...go look it up maybe before you comment", "ups": 1, "depth": 4, "author": "X-KaosMaster-X", "replies": []}]}]}]}]}, {"selftext": "Little do we know, OP is currently experiencing winter with his doors and windows wide open :)", "ups": 4, "depth": 0, "author": "ayunatsume", "replies": [{"selftext": "He uses a fridge as a pc case", "ups": 1, "depth": 1, "author": "PrettyQuick", "replies": []}]}, {"selftext": "I'm on a walk while it's stress testing. I'll posts pics when I'm done, but so far this is what I've found. Maybe it isn't getting full power?? \n\n[link](https://imgur.com/a/NlBzKK0)\n\n[link 2](https://imgur.com/HoYtJPt)", "ups": 1, "depth": 0, "author": "lehman2724", "replies": [{"selftext": "For comparison, here is a screenshot of mine running Cinebench and holding steady at 79\u2103. Mine is water cooled, so you can see the coolant temp is 28\u2103.\n\n[https://i.imgur.com/FIO85fN.png](https://i.imgur.com/FIO85fN.png)\n\nEdit: Actually you can see yours is running at 72.2w and mine is at 111w.", "ups": 1, "depth": 1, "author": "dcabines", "replies": []}]}, {"selftext": "R23 multi results, 15105 score, temps max at 75\n\n[R23 results ](https://imgur.com/a/uIf67Xk)", "ups": 1, "depth": 0, "author": "lehman2724", "replies": [{"selftext": "Sounds like pretty reasonable temp and score for r23.", "ups": 2, "depth": 1, "author": "damien09", "replies": []}, {"selftext": "Wow, that's better then my results with a 2k fclk. You should see how high yours will go with such a good chip.", "ups": 1, "depth": 1, "author": "Doomzdaycult", "replies": []}]}, {"selftext": "You can play with the RAM speeds and timings for a few extra % but beware, lots of our Cpus have a problem with higher speeds than 3733. You can be really cocky and try to up the BCLK for some higher clocks but that can get pretty unstable as a lot of the internal processes depend on that. Besides that you can try to get the full - 30 for the UV like I did but that isn't possible with all chips either\n\nBut that's it for most parts, the 5800X3D isn't that interesting for people who desperately want to tinker with the CPU", "ups": 0, "depth": 0, "author": "I-Beyazid-I", "replies": []}, {"selftext": "I've just done a quick and dirty undervolt to -25 all cores on mine and I've still got a bit of head room. I need to test it to see how far I can take it but I just can't be arsed lol", "ups": 0, "depth": 0, "author": "RentonZero", "replies": [{"selftext": "Just did the same with -30 and runs stable. I think I might have something like a lottery winner here", "ups": 0, "depth": 1, "author": "juzi94", "replies": [{"selftext": "Same? First day after installing it, got -30 in BIOS. Forgot about it... didn't encounter any anomalies.", "ups": 0, "depth": 2, "author": "dubtrainz-next", "replies": []}, {"selftext": "Most new batches of 5800X3D will do -30 with no issues. \n\nMy cpu is from 2023 batch and does it easily.", "ups": 1, "depth": 2, "author": "Opteron170", "replies": []}]}]}, {"selftext": "Correct me if I am wrong, you stated that with 5600 and 3070 you had \\~45fps in cp2077? That is odd, that cpu should provide a lot higher fps there.", "ups": 0, "depth": 0, "author": "nzmvisesta", "replies": [{"selftext": "Well I mean it was with ray tracing on, no dlss and mostly high settings. It was equal settings for both CPU's. I think I probably did get a bit higher, somewhere around 50, but it's the consistency with the 5800x3d that is so great", "ups": 0, "depth": 1, "author": "lehman2724", "replies": []}]}, {"selftext": "your starfield performance seems normal, also i get that in cyberpunk and red dead. best way to see is cinebench score", "ups": 0, "depth": 0, "author": "AppropriateMethod793", "replies": []}, {"selftext": "welcome to Yakutsk", "ups": 0, "depth": 0, "author": "pecche", "replies": []}, {"selftext": "hii,. what score / temp/ power consumption you get in cinebench r23 multi?", "ups": 0, "depth": 0, "author": "MAD2310", "replies": [{"selftext": "15105, averaged 73 degrees, but it was hovering at 70 most of the time. Full boost, full power it appears. \n\n[test results](https://imgur.com/a/uIf67Xk)", "ups": 1, "depth": 1, "author": "lehman2724", "replies": [{"selftext": "hi bro try this settings in occt,  \ntry this settings  \ndata set - small (not large)  \nmode - extreme (not normal)  \nload type -variable  \nshare result pls", "ups": 1, "depth": 2, "author": "MAD2310", "replies": []}]}]}, {"selftext": "I have same cpu and same cooler but my temps are much higher up to 90 in games and around 76-80 with -30 all cores", "ups": 0, "depth": 0, "author": "Amasis33", "replies": [{"selftext": "Wow, that is interesting. What cooler do you have? I'm using a liquid freezer ii rev 7 with kryonaut extreme paste", "ups": 1, "depth": 1, "author": "lehman2724", "replies": [{"selftext": "same cooler, arctic mx2 paste", "ups": 1, "depth": 2, "author": "Amasis33", "replies": []}]}]}, {"selftext": "AIO on any x3d part is absolute overkill because they use very little power. A mid-range air cooler would probably be an overkill for 5800x3d.", "ups": -4, "depth": 0, "author": "Sopel97", "replies": [{"selftext": "Ah I see you have no idea what you're talking about. The x3d cache makes these CPU's very hot! My d15 can't keep it below 90(where it throttles) and that's about as good of an air cooler as you can get. I also have decent air flow with 2x140+1x120 pulling in and 2x120 behind and above the d15 pushing out. No GPU load.", "ups": 3, "depth": 1, "author": "MrPapis", "replies": [{"selftext": "lol i have a cheap \u20ac25 air cooler (Gelid Solutions Tranquillo Rev.4)  and it stays under 80\u00b0c 99% of the time. Usually around 70-80\u00b0c in CPU heavy games.", "ups": 2, "depth": 2, "author": "PrettyQuick", "replies": [{"selftext": "What is your definition of CPU heavy games? Very few are actually CPU heavy. \n\nTry and do 2 back to back cinebench runs with hwinfo on. You will be above 80 and you won't be maxing out boost clocks as it throttles at 78. \n\nI reach 78 degrees with my setup and undervolt in a single cinebench run.", "ups": 0, "depth": 3, "author": "MrPapis", "replies": [{"selftext": "5800x3d starts to throttle at &gt;90", "ups": 2, "depth": 4, "author": "Sopel97", "replies": []}, {"selftext": "ACC, iRacing, Starfield lol", "ups": 1, "depth": 4, "author": "PrettyQuick", "replies": [{"selftext": "Yes and you're throttling at 78, so again if you want to check if you actually have a max power CPU do a benchmark. Cinebench 23 is fast easy and free. \n\nAMD software is very simple and bad at telling CPU info so use hwinfo, this also shows multiple different sensors. \n\nEspecially if it is not undervolted than you're loosing serious performance.", "ups": 0, "depth": 5, "author": "MrPapis", "replies": [{"selftext": "It is not throttling at all and stays max boost clock all the time. Maybe in a stress test it would but not in gaming so who cares.", "ups": 2, "depth": 6, "author": "PrettyQuick", "replies": [{"selftext": "So it doesn't boost as high as long as stock. Throttling. And it will change performance. If I have to explain to one more person a little isn't nothing I'm gonna go insane.", "ups": 0, "depth": 7, "author": "MrPapis", "replies": [{"selftext": "Can you explain how a little isn't nothing?", "ups": 3, "depth": 8, "author": "jb12jb", "replies": [{"selftext": "*Mind violently pops*", "ups": 1, "depth": 9, "author": "MrPapis", "replies": []}]}]}]}]}]}]}]}, {"selftext": "No, he is fairly right. Yes, the X3D Cache makes it difficult to extract heat from the die efficiently. A huge cooler or a smaller one is basically irrelevant. The heat just can not transfer out fast enough, which is why a 360 AIO can still see the CPU hitting high temps with increased power, etc\nRunning the 5800X3D with more power than stock or even at stock results in little to no gain. I dropped mine to 70'C under heavy load with an NH-U12 by decreasing PPT et al, a negative curve optimizer, and lost about 200 points in Cinebench, nothing you can notice. \nIt's just pointless to increase PPT, etc. Out of the box, it's not well optimized, undervolting along with curve optimizer and PPT et al. limits is the way to tune this CPU.", "ups": 2, "depth": 2, "author": "Berserkism", "replies": [{"selftext": "No he's not and you just perfectly explained why. \n\nAlthough its true they are harder to cool for the reason you mention but that doesn't make the argument right that it doesn't matter what cooler it is. Sound that out in your head ofcourse a better cooler can cool anything better even if it does it less effectively compared to other CPU's. Just because it's a 360 aio doesn't mean it's a good cooler. Water heats up over time air does not.\n\n D15 and d12u are good coolers and I can do full power not for long term stresstest with max clocks, but I can do it all day for actual usage. Your d12u cannot that gen4 cannot either, you're lowering performance because your coolers are not good enough I don't need to because my cooler is good enough. You literally both just admitted to under clocking the chip to make it run on you midrange coolers. I have a high end cooler that makes the chip run at max stock settings. But only if I undervolt. \n\nI'm sorry but you too have misunderstood this thing less isn't nothing.", "ups": 0, "depth": 3, "author": "MrPapis", "replies": [{"selftext": "I can easily run it up to 80'C and lose nothing hitting Max all core clocks. I chose 70'C because it's the best power efficiency to performance ratio. The bottle neck is the CPU design in this case. A larger cooler just does not result in better performance due to these limitations and the hard cap integrated into the CPU itself. It simply can not boost past a certain point, and with careful tuning, this is easily reached without requiring a large air cooler or AIO.", "ups": 1, "depth": 4, "author": "Berserkism", "replies": [{"selftext": "So you're gonna prove it or its just your word? I would love to optimize my chip more if possible.", "ups": 0, "depth": 5, "author": "MrPapis", "replies": [{"selftext": "It's not that high-end cooling can't do better. It's just the diminishing returns on cooling vs. performance is severe due to the limitations in design and built-in to this CPU. The first step is per core optimisation. It takes a lot of time but will result in the best negative curve optimizer offset. You can then add a negative voltage offset tuning it down as low as possible while being stable. You can then set PPT/TDC/EDC to cap power draw as well if aiming to optimise performance vs. temp/power draw. \nIn the end, what you will end up with is well controlled temps, lower power draw, and maximum boost clocks. Now, of course, you can use bclk tuning to push clocks that are a bit higher, but for more heat and complication, increased cooling you will at most gain a few hundred points in a synthetic benchmark like Cinebench. \nTry Skatterbencher for more detailed information.", "ups": 0, "depth": 6, "author": "Berserkism", "replies": [{"selftext": "Yes but you both kinda heavily implied there's nothing to gain when there is. Especially when considering time as money because people do not hyper optimize a CPU. \n\nI get your point but I say again less is not nothing. Higher end CPU cooler is necessary for a quick and easy full power from it. If you want to be spending hours to save 30 dollars that's a thing but I wouldn't advice it personally. If you can buy a 5800x3d you should have a high end cooler.", "ups": 1, "depth": 7, "author": "MrPapis", "replies": []}]}]}]}, {"selftext": "you can run on ln2 but what's the point if it's not throttling?", "ups": 0, "depth": 4, "author": "Sopel97", "replies": [{"selftext": "I don't know, how is this comment to anything?", "ups": 1, "depth": 5, "author": "MrPapis", "replies": [{"selftext": "the point is his cpu is not throttling with his cooler", "ups": 1, "depth": 6, "author": "Sopel97", "replies": [{"selftext": "He literally said he has lower score. So he is limiting the chip to run like this. \n\nThat's throttling, just self imposed throttling. He also said he spent a lot of time optimizing his chip. Doing per core undervolt takes HOURS. So he is getting lower performance, spent hours getting it up and running. \n\nI paid an extra 30 bucks for a CPU cooler and I get full performance. A cpu cooler I have now had for 6 years. Which I most likely won't change out ever. \n\nWas the the savings worth it? That's individual but I'll say this if you have/had 300-400 dollars for a CPU. And you can't find 60-70 dollars for a lifetime CPU cooler, I mean that just doesn't make alot of sense to me. No matter he could have saved time and have higher performance with a high end cooler. His point about no difference between coolers on this chip is already dead he is wrong by own admittance.", "ups": 1, "depth": 7, "author": "MrPapis", "replies": [{"selftext": "He didn't say what it was running at before he made changes. If he only lost 200 score (which could also just be noise) it was not running much hotter before.", "ups": 1, "depth": 8, "author": "Sopel97", "replies": [{"selftext": "When people say a couple hundred and not writing the actual numbers(which he knows) that could easily be 300-400 or even more. Not that it matter much in real gaming but he is still lower performance and many more hours of optimization in just to save 30 bucks on a lifetime product? It's over dude you're not gonna convince me that it makes sense in a high end build to save 30 bucks on a very good CPU cooler. He might want it for himself, but that's not generally advisable. I would never advice someone to undervolt per core with all the stability testing that entails to save 30 bucks on a 1500+ build. \n\nThe whole thing I had an issue with is them(I don't remember if it was you) said coolers didn't make any difference we have just all agreed and come to conclusion; they do. So no you are just wrong. Better cooler more cooling even if it's less effective on this specific cpu. Everything else is noise and masquerades and cope. Better cooler better cooling and it's also the case for 5800x3d. End of story.", "ups": 1, "depth": 9, "author": "MrPapis", "replies": []}]}]}]}]}]}]}]}]}]}, {"selftext": "Are you reading from AMD software? Its notoriously bad for cpu info. Use hwinfo instead. 50 degrees is in the realm of impossibility. Mine flies straight past 90 degrees after 1 sec of cinebench without pbo. With -30 it's holding at around 78, 4,45 almost all the way through on d15. Not sure I had it setup to run 100% fanspeed at the time though.", "ups": 1, "depth": 0, "author": "MrPapis", "replies": [{"selftext": "I was using hwinfo, afterburner and OCCT for temps. They all said the same thing.", "ups": 1, "depth": 1, "author": "lehman2724", "replies": []}, {"selftext": "Not even joking here. It's at 41 degrees. What is going on here?! I would like to know also haha. I don't believe any of this either\n\n[wtf](https://imgur.com/HoYtJPt)", "ups": 1, "depth": 1, "author": "lehman2724", "replies": [{"selftext": "This is some of the weirdest shit I have ever seen in the hardware world. \n\nThe temps you are checking now isn't the only tab with temps there's another tab you can open with more temp readings. Please let me see you magic.\n\nEdit: sorry it is the right tab the quality was just so bad I couldn't properly make it out. This is weird.", "ups": 1, "depth": 2, "author": "MrPapis", "replies": [{"selftext": "Okay, when I get home brb lol. But I think someone else figured it out, it might not be getting full power, still\n.. 4.5 ghz....\n[power??](https://imgur.com/a/NlBzKK0)", "ups": 1, "depth": 3, "author": "lehman2724", "replies": [{"selftext": "Ohh youre literally drawing half what that chip should be. This is the power draw I see in games, still insane temps tho, but something is definitely wrong.", "ups": 1, "depth": 4, "author": "MrPapis", "replies": [{"selftext": "Okay, I knew it couldn't be right. Maybe it's just OCCT? Because in PBO2 tuner I didn't even mess with power limits. It's still at default 142W, but it's still boosting to 4.5ghz?\nI'll run some other benchmarking/torture software", "ups": 1, "depth": 5, "author": "lehman2724", "replies": []}, {"selftext": "Even when I put it on extreme, which has higher power limits, it still stays 70-80, so... I'm not sure what's going on here. Motherboard maybe? It's a b550 gaming plus, so it doesn't have the extra 4 pin power connector. Is the board not supplying enough power?", "ups": 1, "depth": 5, "author": "lehman2724", "replies": [{"selftext": "Dude you're way too much up in the air you need to do a benchmark that outputs a score, where you note temps, settings and score. \n\nCinebench 23 is good.", "ups": 1, "depth": 6, "author": "MrPapis", "replies": []}]}]}]}]}]}]}, {"selftext": "Nvidia driver overhead is so high you were getting ~45 fps with Ryzen 5 5600 in Cyberpunk.", "ups": 1, "depth": 0, "author": "emfloured", "replies": []}, {"selftext": "impossible that temp is with this cpu", "ups": 1, "depth": 0, "author": "emfloured", "replies": [{"selftext": "I will run any tests you guys want. I don't believe it either guys lol", "ups": 1, "depth": 1, "author": "lehman2724", "replies": []}]}, {"selftext": "see my 13th gen (13600k) temp 58-60c and 74watts (for 5-5.1clocks) in OCCT   \n[https://imgur.com/a/pro8y80](https://imgur.com/a/pro8y80)  \n\n\nbcoz you are tested  in large-normal settings ,. small-extreme setting is the torture test which use 85c+ for me. test with those settings and share result pls ,. im curious to check.. (i guess 80-84c??)", "ups": 1, "depth": 0, "author": "MAD2310", "replies": []}, {"selftext": "I'd suggest check a cinebench with and without your offset.\n\nI found performance improvement up until -15 all core, then a mild (1-2%) regression at -20.\n\nYou can go core-by-core, as some will do fine at a greater offset than others, but you're very unlikely to notice any change outside of a benchmark result.\n\nedit:\n\nA bit more detail: You won't notice a change in HWinfo64 in terms of clock speed by using a greater offset, you WILL get lower temps. You *may* get higher performance. Ryzen CPUs will stretch their clocks if the offset is too much, which will make you think you've got a golden chip, and maybe you do, but you need to actually check performance. You'll see it level off or go down even with what appear to be stable, maxed-out clockspeed.", "ups": 1, "depth": 0, "author": "Doebringer", "replies": []}, {"selftext": "Just FYI Starfield responds to memory bandwidth even with 5800X3D, and even gains minimums from a faster SSD. Just a 200mhz increase from 3600 to 3800 was a noticeable uplift for me. That said, you may be GPU bottlenecked", "ups": 1, "depth": 0, "author": "RedTuesdayMusic", "replies": []}]}
{"post": {"title": "There is new AMD AM4 AGESA Combo V2 PI 1.2.0.B", "subreddit": "Amd", "selftext": "Asrock have new BIOSes for several boards, mainly AM4, b500 series, that have this  AGESA Combo V2 PI 1.2.0.B. Any info about this new version is unavailable, except version number. Last well known version is Combo V2 PI 1.2.0.A.", "ups": 30, "permalink": "/r/Amd/comments/16lxaco/there_is_new_amd_am4_agesa_combo_v2_pi_120b/", "num_comments": 17}, "replies": [{"selftext": "This post has been flaired as a rumor, please take all rumors with a grain of salt.", "ups": 1, "depth": 0, "author": "AMD_Bot", "replies": []}, {"selftext": "Security Fixes.\n\n1.2.0.B addresses the AMD Inception issue: [https://www.amd.com/en/resources/product-security/bulletin/amd-sb-7005.html](https://www.amd.com/en/resources/product-security/bulletin/amd-sb-7005.html)\n\n&amp;#x200B;\n\n1.2.0.C (targeted to release Dec 2023) addresses the AMD Zenbleed issue: [https://www.amd.com/en/resources/product-security/bulletin/amd-sb-7008.html](https://www.amd.com/en/resources/product-security/bulletin/amd-sb-7008.html)", "ups": 25, "depth": 0, "author": "Warkratos", "replies": [{"selftext": "Thanks for the info.", "ups": 4, "depth": 1, "author": "simog", "replies": [{"selftext": "Ah no need to install then", "ups": 10, "depth": 2, "author": "Z3r0sama2017", "replies": []}]}]}, {"selftext": "Meanwhile on my 3800x x570 Aorus Master nothing is stable after 1.2.0.3 ![gif](emote|free_emotes_pack|facepalm)", "ups": 11, "depth": 0, "author": "icf80", "replies": [{"selftext": "Interesting.\n\nIt was Spring of 2022 when I got a new Asus B550-I that wouldn't work out of the box with a GTX card (BIOS and Nvidia vBIOS issue) so I had upgraded the BIOS during troubleshooting. The 3600X that I had used for years in an older C7H (with older BIOS) was never stable again, even at defaults, with the AGESA I had to put on the B550-I.\n\nI ended up having to put a (shitty, early) 5600X in that system for it to be usable. And now feel I can't sell the 3600X in good faith because it *may* not actually work for someone now due to something with newer AGESA, possibly.", "ups": 1, "depth": 1, "author": "HateToShave", "replies": [{"selftext": "Same, with everything on auto not even xmp, I get random bsod with any bios after f34 (1.2.0.3)\nWith f34 everything is rock solid. Never had a single bsod ever.", "ups": 1, "depth": 2, "author": "icf80", "replies": []}]}]}, {"selftext": "Thanks - Asus has posted a new bios based on this for their Strix B450-F as well. So I imagine more boards are getting them.\n\nROG STRIX B450-F GAMING BIOS 5201\n\nVersion 5201\n\n11.13 MB 2023/09/06\n\n\"1. Recommended for vital update to mitigate the potential security vulnerabilities\n\n2. Improve system stability\"", "ups": 2, "depth": 0, "author": "cellardoorstuck", "replies": [{"selftext": "That might just be the previous release + new microcode. The C6H recently got 8702 which is just 8701 + microcode and still on 1208 with 120A still MIA.", "ups": 2, "depth": 1, "author": "rilgebat", "replies": []}, {"selftext": "Thanks! Hope pbo is enable for 5800x3d in this one", "ups": 2, "depth": 1, "author": "standard_nick", "replies": [{"selftext": "I want CO for the 5800x3D on my x470 prime pro. C'mon ASUS...", "ups": 2, "depth": 2, "author": "wichwigga", "replies": [{"selftext": "It looks like upgrade to b550 is the only solution.", "ups": 0, "depth": 3, "author": "standard_nick", "replies": [{"selftext": "Defo not buying a new mobo for that", "ups": 1, "depth": 4, "author": "wichwigga", "replies": []}]}]}]}, {"selftext": "Dang my x570 hasn\u2019t gotten for 2 months", "ups": 1, "depth": 1, "author": "luigithebeast420", "replies": []}]}, {"selftext": "Still on [1.2.0.8](https://1.2.0.8).\n\nHave only 1.2.0.A for my board. Anything important except fixed EDC bug?", "ups": 1, "depth": 0, "author": "cptslow89", "replies": []}, {"selftext": "Have they got the new AGESA for Asrock X570 boards yet? \n\nMy Phantom Gaming 4 is still on the October 2022 BIOS with the EDC bug, which made PBO tuning quite a challenge at first.", "ups": 1, "depth": 0, "author": "DryClothes2894", "replies": []}]}
{"post": {"title": "AMD Adrenaline Sofware", "subreddit": "Amd", "selftext": "How can I use the above mentioned software to get better performance in my games? Is it similar to the Nvidia Control Panel or does it not really affect much in game?", "ups": 0, "permalink": "/r/Amd/comments/16m34yu/amd_adrenaline_sofware/", "num_comments": 30}, "replies": [{"selftext": "it is the driver interface... its like the nvidia control panel except:\n\n* Full overclocking is built in\n* The GUI looks like it was made in current day\n* ***Every single driver setting*** is available in game via the radeon overlay.\n\nIt also has some overlap with geforce experience and nvidia broadcast:\n\n* Relive is amd's shadowplay\n* Driver notifications and updates\n* Noise suppression\n* AMD link is amd's gamestream (I don't use it tho, I find remote play to be sufficient for me).\n\n\nbut best of all:\n\n* you only need one application to manage your radeon gpu\n* you do not need an account.", "ups": 8, "depth": 0, "author": "duplissi", "replies": [{"selftext": "I think Nvidia has a winner with the AMD Catalyst-era looking Control Panel of theirs.\n\nFrequently I have a heckuva time finding what I need in Adrenaline, and the Windows 8-era flat touch UI it uses somehow seems even more outdated.   But maybe I'm just an Abe Simpson yelling at a cloud as I still consider vintage Aero to trump Metro UI.\n\nCan't wait for WinXP/Win7 designs to be retro-chic.", "ups": 2, "depth": 1, "author": "clinkenCrew", "replies": [{"selftext": "Part of the problem there is that AMD has been re-arranging adrenaline, so you have to re-learn where everything is, which is annoying. That being said, I do thin,The geforce control panel is fairly easy to navigate. But... its pretty sluggish, takes a minute to launch, settings changes always lock it up for 20 seconds or so, etc. I'd be content enough if they just made it responsive. \n\nJust got to get used to the new interface with adrenaline. you dont have any issues finding things in geforce control panel because its been nearly unchanged for 23 years. \n\nThe convenience and simplicity of having everything pertaining to your video card in one application, that is also completely available in game without atl-tabbing is very nice. This also gives consistency. if you want to turn on DSR/DLDSR you go to the control panel. If you want to a per game graphics profile (like to turn on/off vsync, low latency mode, etc) you go to geforce experience, if you want noise suppression you need to also install nvidia broadcast. With amd all 3 of the equivalent options are all in adrenaline in game and at the desktop.\n\ntbh, the GUI and # of applications to manage my GPU are not at the top of my priority list for a video card, lol. If you ask me to pick, then on just the GUI of the driver AMD gets my vote.\n\nBeen using nvidia and amd/ati gpus back and forth since I got my first pc as a kid. I had a pentium 133 w/mmx, 32mb sdram, 2gb hdd, and both a nvidia Riva tnt, and ati rage 3d back then. oh, and windows 95. I had no idea what I was doing, but I would swap between them to see which would run whichever game I wanted to play better.  Its been a long ass time, so I don't remember which gpu was faster. lol.", "ups": 2, "depth": 2, "author": "duplissi", "replies": [{"selftext": "&gt;Part of the problem there is that AMD has been re-arranging adrenaline, so you have to re-learn where everything is, which is annoying.\n\nYeah, that *is* annoying.\n\nAlthough I do actually wish the new UI launched with the layout it has now, because the placement of things is better/more intuitive.\n\nI've also noticed the UI has become a bit slower to respond than it was when it was initially overhauled. Older drivers had a much more responsive and quick UI, and sections in the current driver feel a lot clunkier than they did.\n\nAnyone else notice the same, or just me?", "ups": 1, "depth": 3, "author": "Rockstonicko", "replies": []}]}]}, {"selftext": "the only thing bothering me is stuff like dx9 only settings", "ups": 1, "depth": 1, "author": "Lyajka", "replies": []}]}, {"selftext": "Yes, it's similar to Nvidia Control Panel. You can't improve your performance there much except for overclocking. If you need better performance in a specific game - try lowering the graphics settings in that game.", "ups": 15, "depth": 0, "author": "Elliove", "replies": []}, {"selftext": "All those \"optmizations\" and other snake oil that people say to change in the Nvidia control panel or Adrenalin are just that, snake oil. They might help very edge cases but 99.9% of the time they do nothing or very very close to nothing. I wouldnt even bother.", "ups": 19, "depth": 0, "author": "Happiness_First", "replies": [{"selftext": "auto undervolting or auto overclocking is usually worth it tho.", "ups": 12, "depth": 1, "author": "Goldenflame89", "replies": [{"selftext": "Thats different than changing the texture streaming optimization or the other options. Overclocking is a different thing and auto overclocking/undervolting does a whole lot of nothing really. At least in my experience, all it did was drop the Mv down like 15 on my GPU and auto overclock basically raised it less than 100. So both you wouldnt notice a difference in anything.", "ups": 4, "depth": 2, "author": "Happiness_First", "replies": [{"selftext": "My gpu auto overclocked close to 400mhz with adrenalin. \n\nThat's not a huge performance gain in the grand scheme but it's still free performance.", "ups": 2, "depth": 3, "author": "jolsiphur", "replies": [{"selftext": "I mean if I manually overclock mine, I can push it 60Mhz from the absolute top limit but I see less than 5% of performance for much, much more heat", "ups": 0, "depth": 4, "author": "Happiness_First", "replies": []}]}, {"selftext": "Setting \"Texture Filtering Quality\" to \"performance\" can give a small boost in some games, and in benchmarks. I've used that trick for years to eek out a few more points from my setups and climb a few ranks on leaderboards, and 3DMark doesn't complain about it.\n\nWhat 3DMark will complain about is modifying the Tessellation factor in the driver, but if you're on an older GPU and playing a game that uses tessellation, dropping the Tessellation Mode to 16X can give you a pretty good improvement without sacrificing *too* much detail if the game is using a 64X factor.\n\nThe Witcher 3 is one example, same with Starfield. (Although Starfield has some visual artifacts like black pixels around edges with a driver modified tessellation factor, but you can still get a few more FPS if you're willing to live with it.)", "ups": 1, "depth": 3, "author": "Rockstonicko", "replies": []}]}, {"selftext": "did they ever fix the voltage control? last time i tried to undervolt with it, it never would actually change.", "ups": 1, "depth": 2, "author": "okiedoke7", "replies": []}]}]}, {"selftext": "Depends on the game.\n\nIf you're running a game with uncapped FPS, Radeon Anti-Lag is a good setting to enable. This is especially good to use on a game that is constantly sub-60fps and feels sluggish, it will lower control latency noticeably.\n\nIf you want to save power or enable driver side framecap while gaming, use AMD Chill with proper framerate values.\n\nIf your game keeps jumping above and then falling below your display Freesync range and stutters, try AMD Enhanced Sync.\n\nIf you're running some old game that does not have upscaler build in, Radeon Super Resolution can give you a simple driver side upscaler that works with most games. Same for sharpening, Adrenaline has a driver side sharpening filter you can enable for games that don't have support for it in game setting menu.\n\nAdrenaline also comes with overclock controls, using them is totally \"your mileage will vary\" and may cause crashes if your card can't handle it. If you crash, Adrenaline should return stock settings so it's fairly safe to play around with. I would not overclock my CPU with it, that's best done from bios itself.", "ups": 3, "depth": 0, "author": "Temporala", "replies": []}, {"selftext": "It is similar to nVidia control panel, but has more functions inbuilt.", "ups": 2, "depth": 0, "author": "Jarnhand", "replies": []}, {"selftext": "You can activate rsr(Radeon Super Resolution) under Hypr-x in the AMD Adrenaline Software.", "ups": 2, "depth": 0, "author": "jonasjj5", "replies": []}, {"selftext": "Try /r/AMDHelp for more answers and helpful stuff for newcomers to the AMD system. I am finding this subreddit is often AMD-product and AMD-stock meta. \n\nThat sub will give you a lot more of what you need!", "ups": 2, "depth": 0, "author": "algaefied_creek", "replies": []}, {"selftext": "I think for me the most confusing part is\u2026do I enable all the features?\n\nWhat do half of these things do? Like anti-lag I assume is Nvidia reflex. But what are the other ones?\n\nI guess I\u2019m so used to turning settings on within the game vs driver level.", "ups": 1, "depth": 0, "author": "marcanthonynoz", "replies": []}, {"selftext": "Depends what you mean by this. You can activate the different features of Radeon cards or even overclock in Adrenaline Software. I got a nearly 20% extra performnace out of my 6800XT after OC", "ups": 1, "depth": 0, "author": "Death_Pokman", "replies": []}, {"selftext": "Dunno about better performance but for better visuals definitely turn on RIS, if you don't you might regret it later, I did. If you are playing older titles which has nvidia gameworks then override tesselation settings to 16x, it might help in a handful cases but it does help. Turn on rebar in bios for better 0.1 and 1% lows, there are games where you would get better fps but you are sure to get better lows in most games. Be sure to check if freesync is on in the software and windows settings. Keep morphological anti aliasing off but you should turn it on per game basis where the anti aliasing is dogshit (didn't affect dx12/vulkan games).", "ups": 2, "depth": 0, "author": "shivamthodge", "replies": [{"selftext": "I mainly use it to enable FreeSync in games and to upscale from 1440 to 4K. I'm unaware of any of the other settings have had a negative impact on performance", "ups": 1, "depth": 1, "author": "askmybollox", "replies": []}]}, {"selftext": "Why don't reviewers benchmarking GPUs ever disclose the Nvidia or AMD control panel settings they utilize?", "ups": 1, "depth": 0, "author": "KlutzyFeed9686", "replies": [{"selftext": "They do... if they don't it's probably because they didn't change anything, which in most case is valid, so most testing is done with stock settings.", "ups": 1, "depth": 1, "author": "The_Ravio_Lee", "replies": [{"selftext": "No they don't. I've never seen it. Link?", "ups": 1, "depth": 2, "author": "KlutzyFeed9686", "replies": []}]}, {"selftext": "[deleted]", "ups": 1, "depth": 1, "author": "[deleted]", "replies": [{"selftext": "You can find instructional videos on YouTube demonstrating how adjusting these settings can either enhance or diminish performance. The control panel settings wield influence over critical factors such as tessellation levels and anti-aliasing implementation. Despite their significant impact on performance, it's intriguing that reviewers omit any mention of these settings when conducting card-to-card benchmarks.", "ups": 1, "depth": 2, "author": "KlutzyFeed9686", "replies": []}]}]}, {"selftext": "It's a million times better than nvidias windows 95 feel and look,\n\nJust increase power limit to the max and add 100mhz then stress testing then another 100mhz followed by stress testing until your PC becomes unstable then back.off 50-100mhz.\n\nAmd adrenaline is so easy to use oh also max out fan speed.when it gets to 90c and you're good to go.", "ups": 2, "depth": 0, "author": "forevertired1982", "replies": []}, {"selftext": "whats your gpu op? its probably worth undervolting and overclocking, also most settings arent great but radeon boost can be good in fast paced fps games. also so is antilag", "ups": 1, "depth": 0, "author": "AppropriateMethod793", "replies": [{"selftext": "RX 6600 XT 16GB", "ups": 1, "depth": 1, "author": "askmybollox", "replies": [{"selftext": "https://www.youtube.com/watch?v=raNwvQHPm_U&amp;t=884s&amp;ab_channel=AncientGameplays  also 6600xt is 8gb", "ups": 1, "depth": 2, "author": "AppropriateMethod793", "replies": [{"selftext": "Sorry yeah so it is, my system ram is 16gb that's my bad", "ups": 1, "depth": 3, "author": "askmybollox", "replies": []}]}]}]}]}
{"post": {"title": "HiGH Power Consumption of AMD GPUs @ video playback ?", "subreddit": "Amd", "selftext": "Last Gen's 6800 (and up) AMD GPUs &amp; the new 7000 series have serious power issues @ video playback (youtube, Netflix, video players etc.)   \n40+ W is stupidly HiGH + meaningless... and while the previous 6700 XT consumes only 16W, the new 7700 XT doubles that power draw with a whooping 33W on \"video playback\"   \nAMD should have solved that \"power issue @ video playback\" on the current Gen but it's gotten worse... ", "ups": 66, "permalink": "/r/Amd/comments/16la61e/high_power_consumption_of_amd_gpus_video_playback/", "num_comments": 125}, "replies": [{"selftext": "My 7900xt consume around 18-22w watching videos on YouTube or other website, on a 1440p144hz monitor.\nMake sure to increase the monitoring pooling intervall to 1,5 or 2sec.\nAnything lower will artificially increase the power usage by waking up the GPU more and increasing frequency and power consumption.", "ups": 30, "depth": 0, "author": "angel_salam", "replies": [{"selftext": "How do you do this?", "ups": 10, "depth": 1, "author": "Ghosttimo", "replies": [{"selftext": "I suspect he was talking about collection of stats in Adrenalin, which is how most of us know how much power GPU consumes. I e , it won't do a thing unless you are collecting stats/using overlay.", "ups": 7, "depth": 2, "author": "8day", "replies": [{"selftext": "ahhh, yea I turned that off. I see what he meant now. Thanks for the help!", "ups": 1, "depth": 3, "author": "Ghosttimo", "replies": []}]}]}, {"selftext": "[deleted]", "ups": 6, "depth": 1, "author": "[deleted]", "replies": [{"selftext": "doesnt matter because the video is fully decrypted regardless of window scaling.\nthe only thing that matters is video resolution on yt.  playing back 1080p60 source vid will take more power than 720p60/30 .. etc..\ntotally regardless of desktop resolution/dpi or any applied scaling (vsr or gpu scaling feature)", "ups": 1, "depth": 2, "author": "Portbragger2", "replies": [{"selftext": "[deleted]", "ups": 1, "depth": 3, "author": "[deleted]", "replies": [{"selftext": "significant / outside of single digit measurement uncertainty\n\nand even if the 1 or 2 watts were not due to random fluctuation you see yourself that if it is 14w or 16w we are still in a very good power usage range for 1440p", "ups": 1, "depth": 4, "author": "Portbragger2", "replies": []}]}]}]}, {"selftext": "Doing that made my card use more power. Powercolor 7900XTX Red Devil", "ups": 3, "depth": 1, "author": "King_Dong_Ill", "replies": []}]}, {"selftext": "On youtube - 7900XTX is giving me 120W on 8K video with HDR (i can watch HRD)\n\nOn 1080p - 60W.... its quite a lot actually. On my 5700XT was like 20W....  \n\n\nIdle power is fixed! 8W! on 4k/144hz screen!", "ups": 27, "depth": 0, "author": "FormalIllustrator5", "replies": [{"selftext": "My 7900xtx draws 100w at idle :(", "ups": 9, "depth": 1, "author": "skicki16", "replies": [{"selftext": "tfw ur straining on the toilet but nothing's coming out", "ups": 37, "depth": 2, "author": "tomato-fried-eggs", "replies": []}, {"selftext": "Lol wtf", "ups": 8, "depth": 2, "author": "Mastercry", "replies": []}, {"selftext": "Same, seems to be linked to high refresh rate displays as if I drop the refresh rate down to 60 on both screens power consumptions drops off to bugger all.", "ups": 4, "depth": 2, "author": "Thimerion", "replies": []}, {"selftext": "240hz ?", "ups": 1, "depth": 2, "author": "yomencheckmabedaine", "replies": []}, {"selftext": "[removed]", "ups": -15, "depth": 2, "author": "[deleted]", "replies": [{"selftext": "It is not even an AMD problem, strictly. The cause is a horrible lack of standards and quality control across the monitor industry.\n\nAll GPUs suffer from it. RDNA3 (less able to downclock memory) was just when it got bad enough for people actually notice or care. Apparently that little hop over the three digit line was it.\n\nAMD's fix is going on a grand quest to manually override every dogshit monitor EDID in existence, it looks like. Still out there battling windmills.", "ups": 10, "depth": 3, "author": "LongFluffyDragon", "replies": [{"selftext": "Wait what? Would this impact my ability to run custom resolutions?", "ups": 1, "depth": 4, "author": "lokisbane", "replies": [{"selftext": "Probably not, especially if you use CRU to beat it into submission.", "ups": 1, "depth": 5, "author": "LongFluffyDragon", "replies": [{"selftext": "Thank goodness. Whoo.", "ups": 1, "depth": 6, "author": "lokisbane", "replies": []}]}]}]}]}, {"selftext": "and you are using the latest drivers ?!!", "ups": -3, "depth": 2, "author": "FormalIllustrator5", "replies": [{"selftext": "Yep, got the card a week ago", "ups": 2, "depth": 3, "author": "skicki16", "replies": [{"selftext": "What is your monitor and cable?", "ups": -3, "depth": 4, "author": "FormalIllustrator5", "replies": [{"selftext": "2 1080p monitors and a 1440p monitor, all displayport cables", "ups": 2, "depth": 5, "author": "skicki16", "replies": [{"selftext": "Well, what you expect? You have  3 different monitors, probably potato quality (both cables and panel) and expect to get 8W total consumption. \n\nYou have normal Wattage, dont get it personal...", "ups": -7, "depth": 6, "author": "FormalIllustrator5", "replies": [{"selftext": "Please enlighten us how the quality of a display or a cable changes the power consumption of the gpu. Feel free to give a detailed, technical explanation.", "ups": 19, "depth": 7, "author": "ger_brian", "replies": [{"selftext": "Displayport pin 20.  Feel free to Google it.", "ups": 2, "depth": 8, "author": "Maldiavolo", "replies": [{"selftext": "Even the cheapest cables nowadays should have pin 20 not connected. I (at least here in Germany) havent seen any DP cables in ages (no matter the price bracket) that had pin 20 so I would consider this irrelevant. The power consumption of his GPU is still too high though. My 4090 with 3 connected 144hz displays has a lower idle and video power consumption and is a way better card.", "ups": 3, "depth": 9, "author": "ger_brian", "replies": []}]}]}, {"selftext": "Bro, my 3070 drew 5 watts with the same monitor setup and cables on idle", "ups": 5, "depth": 7, "author": "skicki16", "replies": []}, {"selftext": "You forgot an /s ?", "ups": 4, "depth": 7, "author": "abija", "replies": []}, {"selftext": "That's incorrect. Even if I unplug all my monitors but one, it still eats 100 watts on idle.", "ups": 2, "depth": 7, "author": "Wonderful-Middle-543", "replies": [{"selftext": "Well you have a problem with your wattage, i get 8W on 4k screen with OC card on 144hz. check your system...", "ups": 1, "depth": 8, "author": "FormalIllustrator5", "replies": [{"selftext": "No, amd just still has issues with select premium monitors like Alienware, asus", "ups": 1, "depth": 9, "author": "Wonderful-Middle-543", "replies": []}]}]}]}]}]}]}]}, {"selftext": "Same. Pretty normal", "ups": 1, "depth": 2, "author": "Wonderful-Middle-543", "replies": []}]}, {"selftext": "Maybe its the 144hz?  I get 26W wathing 4k youtuve.  I do not have an 8k video to test.  I am in Linux", "ups": 1, "depth": 1, "author": "B16B0SS", "replies": [{"selftext": "Yep, HDR and hz are pushing hard on bandwidth so that is the culprit", "ups": 2, "depth": 2, "author": "FormalIllustrator5", "replies": [{"selftext": "I didn't know HDR increased power consumption - I thought it was just a colour profile.  Good to know, ty", "ups": 1, "depth": 3, "author": "B16B0SS", "replies": []}]}]}, {"selftext": "My 7800xt is drawing 70w idle, nothing open but adrenalin\n\n240hz 1080p benq monitor\n\nNot impressed :(", "ups": 1, "depth": 1, "author": "JJSwif", "replies": [{"selftext": "240hz Screen? And why is that...ok, give me a second, dont explain me. \n\nYou are using useless screen, you dont want to switch it back to 120hz and complain about idle wattage?", "ups": 1, "depth": 2, "author": "FormalIllustrator5", "replies": [{"selftext": "you're dumb if you're asking that question.  Also, even switching to 60hz the idle wattage remains the exact same.  It's an AMD problem, stop fanboying", "ups": 1, "depth": 3, "author": "JJSwif", "replies": []}]}]}]}, {"selftext": "Power increase from chiplets interconnects?", "ups": 15, "depth": 0, "author": "sharak_214", "replies": [{"selftext": "monolithic 7600 slurps 30W too, something strange with these cards", "ups": 2, "depth": 1, "author": "sevenfivefiveseven", "replies": []}, {"selftext": "Yes, because chiplets or the IC's cannot be powered down. Just like Ryzen CPU's and in particular threadripper, those things consume power 100W at idle alone just to keep everything \"lit up\".", "ups": -5, "depth": 1, "author": "Jism_nl", "replies": []}]}, {"selftext": "Just now arrived at the party?\n\nThis has been a problem, seems very monitor-refresh specific at this point. Some fixed, some not.", "ups": 10, "depth": 0, "author": "Electrical-Bobcat435", "replies": []}, {"selftext": "Chiplet GPUs are just going to use a tiny bit more idle power, exactly the same as chiplet CPUs do. There is no \"fix\" for it.\n\nAnd video playback is hardly idle. The decoder, memory, pcie, and interconnects in the processor itself are all working hard.", "ups": 6, "depth": 0, "author": "LongFluffyDragon", "replies": [{"selftext": "Yep - Threadripper on it's own consumes 100W on just idle.", "ups": 0, "depth": 1, "author": "Jism_nl", "replies": []}]}, {"selftext": "Watching 1440p and 4k video on youtube will shoot up my power draw to 40w. 1080p and under will use less like 20-30.", "ups": 4, "depth": 0, "author": "rocketchatb", "replies": []}, {"selftext": "If you have 3 monitors it's normal, Nvidia would consume a bit more during playback when you have 3 monitors connected, even though you only look at one of them when video plays.\n\n\nReducing refresh rate impacts directly the power consumption. If you want low power usage just turn to 60hz and you're all set.", "ups": 6, "depth": 0, "author": "professore87", "replies": [{"selftext": "Thats false.\n\n**4090 FE** (stock 450W) / Windows stock settings / Driver stock settings, using 1440p-**240Hz** \\+ 2 monitors and I watch YT / netflix with **\\~20W total GPU power**.\n\nYT: [https://i.imgur.com/45qyUIZ.png](https://i.imgur.com/45qyUIZ.png)\n\n60Hz vs 240Hz makes no difference.", "ups": 1, "depth": 1, "author": "lichtspieler", "replies": [{"selftext": "As I was saying 3 monitors, 2 1440p (165hz and 144hz) and 1 1080p 60hz. 2 ips 1 tn, all 3 Dell. 38W on a Asus 4090. I have returned the card because I don't want to see after 1 or 2 years the connector having some melting, I won't be able to sell it and get the next gen when it's released.", "ups": 3, "depth": 2, "author": "professore87", "replies": [{"selftext": "I use 2x DP (1440p + 4k) and 1x HDMI (1080p). \n\nWhat combination did you use?\n\nAs for the connector, thats of course your choice.\n\nI used the 3090 FE for 2 years with the same 12pin power connector and using the 4090 FE since its release. My case is wide enough for the 12VHPWR cables / bending restrictions, so I dont expect any issues with hotspots/melting - my case is 12VHPWR compatible.", "ups": 1, "depth": 3, "author": "lichtspieler", "replies": [{"selftext": "I have since changed a bit my setup since my gf started playing wow. I had 6800xt and got the 4090 tuf in my fractal define s (old but v good case, low noise). 2x HDMI and 1 dp. Now I have the 7900XT with AOC agon 27 4k (the LG ips from ultragear) (dp) and 1 1440p (HDMI), the 144hz TN one. My gf  uses the 6800xt with the 165 1440p ips. I have the 2 monitors also hooked to my docking station for work (4k on HDMI and 1440p on dp) for days when I work from home.\n\nHave seen too many posts about that connector and also got the 7900xt nitro+ at 730$ before vat, in my country. That moment I decided it's not worth the risk. I don't play as much as I did 10-15years ago... so it was kinda hard to justify the almost 1800$ I spent on the 4090 tuf", "ups": 2, "depth": 4, "author": "professore87", "replies": [{"selftext": "whoa, that is a great deal cheaper for the 7900 xt.  And yah you are right.  I spend too much on the \"extra x\" with my 7900 XTX where a 7900 XT would be more than adequate for me given how much (or little) free time I have", "ups": 1, "depth": 5, "author": "B16B0SS", "replies": []}]}]}]}]}]}, {"selftext": "RX 6650XT 20w on youtube. Idle 3w.", "ups": 6, "depth": 0, "author": "Limi_23", "replies": []}, {"selftext": "Genuine question: why is 40 W a big problem? I know sometimes it's indicative of other problems but the power usage by itself is basically negligible in the long run", "ups": 8, "depth": 0, "author": "lerthedc", "replies": [{"selftext": "Ramps up the temperature of the card which makes the fans spin up in video playback, which is annoying.", "ups": 8, "depth": 1, "author": "celtyst", "replies": [{"selftext": "Most of the 7000 series have such massive coolers they don't need to spin up fans for a long time.", "ups": -2, "depth": 2, "author": "xXDamonLordXx", "replies": [{"selftext": "Not sure why you got a downvote, I have a new 7800 XT drawing 75W for Twitch (170hz and 144hz monitors probably why) and fans aren't spinning.", "ups": 1, "depth": 3, "author": "Foserious", "replies": [{"selftext": "Even after hours of 75W?", "ups": 1, "depth": 4, "author": "vhk7896rty", "replies": [{"selftext": "Yup, fan PMW stays at a solid 0 and when I look in the case the fans are indeed not spinning. The card is massive so perhaps the heatsink is large enough or HWInfo isn't getting the correct board power read.", "ups": 1, "depth": 5, "author": "Foserious", "replies": []}]}]}]}]}, {"selftext": "Personally I just find it annoying. It's not just icing on the cake though for certain things that are more annoying. The RDNA 3 cards don't undervolt/power down too well when it comes to wanting to use lets say 30% of the cards power efficient's in a game that doesn't take many resources.  \n\n\nAnother annoying thing is merely looking around the settings in adrenaline shoots the power usage up to 70-80 somethin watts as you're looking around on it. Personally I find that unacceptable in an age where things are supposed to be power efficient when it comes to electronics. I get it's the 1st generation of MCM, but this kind of stuff happens on the RX 7600 too which is monolithic, so something is off with this gen besides the expected behavior for interconnected GPU chiplets.", "ups": 2, "depth": 1, "author": "ofon", "replies": []}, {"selftext": "It probably doesn't feel that great to go from one gen to another and the power usage dbl for no apparent reason", "ups": 2, "depth": 1, "author": "B16B0SS", "replies": []}, {"selftext": "the power consumption itself is not a big problem for most people. (there'd be a lot more hate for the arc a750/a770 if people really cared about that) \n\nit just sucks to see the cards using more power than other products (amd's older products &amp; nvidia's products) when doing a certain task", "ups": 2, "depth": 1, "author": "GumshoosMerchant", "replies": []}, {"selftext": "Yeah, it's not a big deal but they should still fix it", "ups": 2, "depth": 1, "author": "Nicolo2524", "replies": []}, {"selftext": "It'll wear out your PSU faster? Esp. considering that some people in this thread report 100W.", "ups": -7, "depth": 1, "author": "8day", "replies": [{"selftext": "Minimal effect.  It will pull hundreds more while doing work or gaming.", "ups": 5, "depth": 2, "author": "farmeunit", "replies": []}, {"selftext": "My 6900xt litterelly are 567w peak when gaming so 100w aint gonna kill the psu. Im on a 850w hold psu and Been using that for 4 years now. \n\nNow running a 7900xtx. My idle watt is only 6w though. \nOn my 6900xt my idle was 18w", "ups": 3, "depth": 2, "author": "Nord5555", "replies": []}]}, {"selftext": "Man we live in time where is standards for saving energy if im not mistaken, i didn't read because dont care. But EU communist union bans old lights we must use only led or whatever is this shit which fucks ur vision. So in this same era where they tell you to not use hot water to wash ur hands AMD and Nvidia releases stronger and more redicolous power hungry GPUs which melt ever their power cable... i wonder what the gay eu commission thinks about that haha", "ups": -21, "depth": 1, "author": "Mastercry", "replies": []}]}, {"selftext": "I have a 6650 I get 22W running a light game, a youtube video, and a 4k video...", "ups": 2, "depth": 0, "author": "cp5184", "replies": []}, {"selftext": "Using an efficient player (mpv, dx9 youtube) video playback is about 20w on my 6800xt, not much more than nvidia.", "ups": 4, "depth": 0, "author": "LTyyyy", "replies": [{"selftext": "I use 12w on my 4070ti for full screen YouTube. 20w is not far off \ud83d\ude0a\n\nEdit: 1440P", "ups": 6, "depth": 1, "author": "Rudolf1448", "replies": []}]}, {"selftext": "something i started paying attention to it only does this when fullscreen not when using picture in picture if seen it go up to 80w or more.", "ups": 3, "depth": 0, "author": "Melodias3", "replies": []}, {"selftext": "Amd never solved the power issue. I still get 100 watts idle on 7900 xtx, even with one monitor. It's probably an issue with the GPU architecture itself and will likely never be fixed.", "ups": 3, "depth": 0, "author": "Wonderful-Middle-543", "replies": [{"selftext": "have you enabled variable refresh rate on all of your monitors? Supposedly that makes a big difference and is a must for the idle power thing. Just wanna be clear that by idle, you mean nothing is on your desktop besides the background", "ups": 2, "depth": 1, "author": "ofon", "replies": [{"selftext": "It makes no difference. Trust me I've tried everything", "ups": 3, "depth": 2, "author": "Wonderful-Middle-543", "replies": [{"selftext": "that really sucks...I'm guessing you noticed this past the return window. I'd be furious about 100 watts on idle from a single GPU", "ups": 1, "depth": 3, "author": "ofon", "replies": []}]}]}, {"selftext": "I have 20W or less idle, sometimes less, idle with a 7900 XTX in linux.  Only difference we would have is the driver and the monitor.  Ih ave 4k 60hz tv", "ups": 1, "depth": 1, "author": "B16B0SS", "replies": [{"selftext": "Yeah I'm on 23.9.1 with an Alienware 1440p 280hz, putting down to 60hz didn't do anything", "ups": 2, "depth": 2, "author": "Wonderful-Middle-543", "replies": [{"selftext": "hmm I dunno then.  Well since I don't get it in Linux it must be solvable which is a silver lining.  It seems unlikely for there to be a hardware issue causing high power usage between 7000 series cards.\n\nIf you wanted to troubleshoot you could try a linux live-image, boot into it, and check out power usage there to see if its still sky-high.  A lot of work though", "ups": 1, "depth": 3, "author": "B16B0SS", "replies": [{"selftext": "I'll probably just end up switching to nvidia by next year if they haven't fixed it. I'm upset amd doesn't really think about people who have to idle on windows.", "ups": 2, "depth": 4, "author": "Wonderful-Middle-543", "replies": [{"selftext": "I was worried for the same reason when I ordered the 7900 XTX card I have.  I really did not want to see 70W idle, but I was pleasantly surprised.  Its still higher than my 6700 XT but not obnoxiously so.  You could send them a bug report as I do see that they are trying to resolve it.", "ups": 1, "depth": 5, "author": "B16B0SS", "replies": [{"selftext": "Yeah I've been sending them bug reports ever since it came out, they haven't done anything so I think the bug report option may as well be a \"click for frustration\" button at this point. It's ridiculous how my 7900 xtx gets 90 more watts idle power than my old rtx gpu.", "ups": 1, "depth": 6, "author": "Wonderful-Middle-543", "replies": [{"selftext": "I'd love to help you as it feels bad to imagine you frustrated with your very expensive purchase.  I don't know much about Windows though.  I have seen videos on youtube about how to fix the high idle power usage that you might want to try if you have not already.  \n\n\nthere are a few others, I just watched some out of boredom, I haven't tried any of them \n\nhttps://www.youtube.com/watch?v=4B0Vt\\_c-7C8", "ups": 2, "depth": 7, "author": "B16B0SS", "replies": [{"selftext": "I've seen every video already but none of them help. Just gotta wait for the drivers or swap for a nvidia.", "ups": 1, "depth": 8, "author": "Wonderful-Middle-543", "replies": []}]}]}]}]}]}]}]}]}, {"selftext": "Using DX9 backend helped reduce Chrome video playback from 40-50W to 20-25W for me. \n\nTry going to chrome://flags (or edge://flags) and set \u201cChoose ANGLE graphics backend\u201d to D3D9.", "ups": 2, "depth": 0, "author": "DaveTheHungry", "replies": [{"selftext": "So, I had both been propagating and using D3D9 backend for a while, and it's unfortunately not a fix.\n\nOpen Ryzen Master and check your CPU and SoC power draw with D3D9 backend, it will be way up versus the default backend.\n\nHere is what I see on my system with a 4K60 video:\n\nD3D9 average per component: CPU @ 45W, SoC @ 14.5W, GPU @ 21W  \nD3D9 total wattage: Roughly 78-82W\n\nDefault average per component: CPU @ 17W, SoC @ 13W, GPU @ 33W  \nDefault total wattage: Roughly 63-68W.\n\nAt least on my system, while default backend uses more GPU power, D3D9 actually ends up being less power efficient due to the additional CPU power consumption.", "ups": 3, "depth": 1, "author": "Rockstonicko", "replies": [{"selftext": "This is actually interesting to hear.\n\nFor my system, Default playback remains higher wattage due to VRAM clock getting maxed. I tested a 4k60 video for 30s and checked numbers by logging with Adrenaline. I'm using a R5 5600x + 6800XT with a 1440p144hz monitor and a 1080p60hz ultrawide.\n\nDefault video playback: 43W GPU and 24.8W CPU. VRAM CLK 1988D3D9 video playback: 30.1W GPU and 28.1W CPU. VRAM CLK 838\n\nThis ends up as 67.8W vs 58.2W. Now if I could get VRAM clock to not max out during default playback, it should be tied or lower wattage.", "ups": 2, "depth": 2, "author": "DaveTheHungry", "replies": [{"selftext": "That *is* actually interesting. So I guess it would just be something you'd need to test on your individual setup and configuration to see which backend is best.", "ups": 1, "depth": 3, "author": "Rockstonicko", "replies": []}]}]}]}, {"selftext": "Nvidia isn't better here, same thing. But odd to compare 4k 60hz video playback on x86 gpus, vs a little 15watt total system power orange pi 5, that has a rk3855 in it. And a janky Mali gpu. It runs cool playing back 4k video, and only when really pushed does it get near its max of 15watts when running emulation or cpu heavy tasks.\n\nI get the whole arm vs x86 thing, but this is all gpu drivers and dedicated silicon doing the video decode on both arm and x86. It shouldn't be this bad for amd nor nvidia gpus.", "ups": 3, "depth": 0, "author": "carl2187", "replies": [{"selftext": "Watching 4K on youtube with my 4090 and dual monitors gives me 20W usage, doesn't seem to increase with the video load, same as regular idle so I think dual monitors is stopping it from going any lower anyway.", "ups": 7, "depth": 1, "author": "Bladesfist", "replies": []}]}, {"selftext": "Right now, at this very moment I am writing this while a youtube video plays in picture in picture mode, pop out mode, whatever its called and my 7900XTX is using \\~30 watts.", "ups": 2, "depth": 0, "author": "King_Dong_Ill", "replies": []}, {"selftext": "I see some people thinking this is normal, but I think it is too much, specially considering that these gpus have hardware codec and decode.", "ups": 2, "depth": 0, "author": "keeponfightan", "replies": []}, {"selftext": "33w is not bad for that gpu during video playback my rx 6600 will do 20w in the same situation.", "ups": 4, "depth": 0, "author": "Critical_Ordinary_77", "replies": []}, {"selftext": "240hz 2560x1440p here with nitro+ 7900xtx overclocked 3200mhz 2774 fast timing ram idle with fanspeed on @28% speed gives me 6w idle", "ups": 1, "depth": 0, "author": "Nord5555", "replies": []}, {"selftext": "What data are you checking?\n\nAt any rate, 16W difference is not much of an issue.", "ups": -3, "depth": 0, "author": "20150614", "replies": []}, {"selftext": "1. But it's cheaper than Nvidia.\n2. You spend less on hit you home in winter time.\n3. More Vram for Netflix and YouTube.\n4. UnDErvOlt it!\n5. Writ here you reason why AMG GPU is 100000% better than other GPUs.", "ups": -11, "depth": 0, "author": "GrimReaperUA", "replies": [{"selftext": "Ok", "ups": 4, "depth": 1, "author": "skicki16", "replies": []}, {"selftext": "UA in name = instant downvote", "ups": 2, "depth": 1, "author": "throwawayerectpenis", "replies": [{"selftext": "I don't care, you jast garbage.", "ups": -1, "depth": 2, "author": "GrimReaperUA", "replies": []}]}, {"selftext": "I used to joke that AMD users have free heater for winter. What about summer when goes 40c plus ahah bulldozer all over again", "ups": 2, "depth": 1, "author": "Mastercry", "replies": []}, {"selftext": "undervolting doesn't change the video playback or idle at all.", "ups": 1, "depth": 1, "author": "ofon", "replies": []}]}, {"selftext": "Not going to lie, I could not care less about wattage usage as a single gauge for how a GPU performs.  If that\u2019s the hill you want to die on, cool\nAl Gore will be by with your saving the planet award.  I don\u2019t care as long as my GPU\nIs functioning as it should.", "ups": -4, "depth": 0, "author": "Exact-Explanation524", "replies": []}, {"selftext": "Do you know how many people still run incandescent light bulbs? 30w vs 10w is a drop in the bucket. Multimonitor and youre at 30w idle, thats not a compromise people are going to part with over 20w. Your entire system power draw is going to be 80-100w regardless. I really hope every one of your bulbs in your house is an LED and you have solar and hydro dammed the stream in backyard.", "ups": -9, "depth": 0, "author": "acat20", "replies": [{"selftext": "Incandescent bulbs are wiped out from my local market for years. I haven't seen one for more than a decade.", "ups": 7, "depth": 1, "author": "skylinestar1986", "replies": [{"selftext": "Theyre about 85% of sales globally, but only account for approx 50% of installed bulbs. 10 vs 30w on just the gpu is not something we should be griping about. OP should buy a wall monitor and he\u2019ll have a heart attack.", "ups": -4, "depth": 2, "author": "acat20", "replies": []}, {"selftext": "Almost nobody uses them for regular lighting anymore, but they're still very common in appliances", "ups": 1, "depth": 2, "author": "GumshoosMerchant", "replies": []}]}]}, {"selftext": "the power goes up because of the vrams that runs at their full speed, no idea why they can't just go to intermediate values as compute power in such scenarios is very low\n\nthe annoying thing with my 6800 MBA in that scenarios is that the fans aren't engaged and the card runs very hot like 65+ before they power on for some second\n\nI think that running vrams lets say half speed or engaging fans at minimum value should fix this thing (and power consumption)", "ups": 0, "depth": 0, "author": "pecche", "replies": []}, {"selftext": "40W is not stupidly high for Netflix. The 5500m in a macbook 2019 pro consumes 20W when idling with just an open case and an external monitor. Now that's stupidly high for a laptop for doing nothing.", "ups": -1, "depth": 0, "author": "enigma-90", "replies": []}, {"selftext": "I\u2019m surprised you think 40w is high.\n\nWatching 4k60 content for me is 70w in Firefox and 100w in brave. At idle my power comsumption is low but literally moving a window around the desktop will cause a spike to 100w.\n\nI wish I was getting as low as some people are mentioning here", "ups": -2, "depth": 0, "author": "xreyuk", "replies": [{"selftext": "what GPU do you have?", "ups": 1, "depth": 1, "author": "ofon", "replies": []}]}, {"selftext": "\"A whopping 33W\" LOL", "ups": -3, "depth": 0, "author": "Berserkism", "replies": []}, {"selftext": "When will people understand that a chiplet design, especially one that's brand new, will have different power requirements than monolithic?", "ups": -6, "depth": 0, "author": "IrrelevantLeprechaun", "replies": []}, {"selftext": "Go look at Nvidia. They have the exact same \"problem\"", "ups": -3, "depth": 0, "author": "IrrelevantLeprechaun", "replies": []}, {"selftext": "I'm wondering the same but on the Nvidia side, video playback on my 4080 GPU load seems always high around 20-30%, meanwhile on my 6750XT is only around 5% GPU load, anybody having the same issue?", "ups": 1, "depth": 0, "author": "orochiyamazaki", "replies": [{"selftext": "I have a feeling it may be due to keeping all of the vram modules powered. There seems to be some connection there as the video cards with extra vram (16 gb and higher) tend to have this issue the worst. I'll wait for someone to call me an f'ing idiot now and how I don't understand these things on a technical level and they'll probably be correct.", "ups": 1, "depth": 1, "author": "ofon", "replies": []}]}, {"selftext": "I've got an RX 7600 with triple monitor 1080p, two at 60hz and one at 144hz. With youtube(720p60) and two other browsers running, it sits at 27w GPU board power @ 39c(ambient 70f). Seems ok to me, but just sharing for comparison.", "ups": 1, "depth": 0, "author": "Beneficial_Tap_6359", "replies": [{"selftext": "this is part of why I don't think the video playback thing is entirely an MCM thing since the rx 7600 is monolithic. Maybe it really is just poor drivers", "ups": 1, "depth": 1, "author": "ofon", "replies": []}]}, {"selftext": "The problem is caused mostly by VRAM that is always running at full speed. For example mu brother's RX 580 used to work at 2000 Mhz regardless of situation. This caused it to use 30 watts at idle. A few weeks ago a new adrenaline upsate fixed all this issue and now it only uses 8 to 10 watts which is actually pretty good. Not to mention that VRAM speed goes down to 300 Mhz. I'm sure if they manage to make it dynamic, we all would be happy with idle power usage. I do care about it because unlike most of you I'm an environmentalist and I don't want 20% higher power usage for the sake of 2% performance increase or whatever.", "ups": 1, "depth": 0, "author": "let_bugs_go_retire", "replies": []}, {"selftext": "I get 20-30W using my computer, day to day, with a 7900 XTX.  Linux amdgpu driver.  Memory downclocks to 96Mhz frequently.  It is about 2x what I was getting with my 6700 XT.  Not sure how much power it takes for the additional 12GB of ram this card has.  \n\n\nThis is on a 4k 60hz television display.", "ups": 1, "depth": 0, "author": "B16B0SS", "replies": []}]}
{"post": {"title": "AMD GPUs can now run stable diffusion Fooocus (I have added AMD GPU support) - a newer stable diffusion UI that 'Focus on prompting and generating'. works great for SDXL", "subreddit": "Amd", "selftext": "Fooocus is a great UI! (search Fooocus Github to find it) I am not good at using the long prompts and was surprised that simple prompts can lead to amazing images using  Fooocus (check my twitter pages on example output, link on my profile). If you have used it, what's your thought? \n\nBut it didn't work on AMD GPUs. I was able to figure it out. My fork has been merged into the main repo so it now works on AMD GPUs.\n\nPlease give it a try if you have AMD GPU and let me know what's the speed for your card and your environment! \n\nOn my 6700XT (pytorch1.13 or 2.01, Ubuntu 20.04): 1.77s/it. \n\nI  also created videos for Fooocus and videos for AMD GPUs on Youtube.  Please subscribe to it if you are interested. Link is on my Profile. Thanks! ", "ups": 120, "permalink": "/r/Amd/comments/16kzjo0/amd_gpus_can_now_run_stable_diffusion_fooocus_i/", "num_comments": 40}, "replies": [{"selftext": "I'll Help You\n\nLink to Repo:\n\n[https://github.com/lllyasviel/Fooocus](https://github.com/lllyasviel/Fooocus)", "ups": 20, "depth": 0, "author": "kaisersolo", "replies": []}, {"selftext": "Until ROCm comes to windows with support, this is still just a small step in the right direction", "ups": 16, "depth": 0, "author": "Buris", "replies": [{"selftext": "I think ROCm already came to Windows this year, so I think someone just needs to port one of these programs to it.", "ups": 7, "depth": 1, "author": "pullupsNpushups", "replies": [{"selftext": "HIP SDK is ready for use (actually long before, Blender has used that for years, but the SDK has been kept NDA for unknown reason) but not MIOpen which is a critical dependency of PyTorch and any other ML-based libraries. Though MIOpen for Windows is WIP so it would become available sooner or later.", "ups": 5, "depth": 2, "author": "b3081a", "replies": [{"selftext": "I don't get why there's literally 1 guy working on this and it's not even an amd employee", "ups": 3, "depth": 3, "author": "218-69", "replies": []}, {"selftext": "I know HIP was released for Windows earlier this year, but I also remember reddit posts about more of the ROCm stack coming out too. I'm supposing MIOpen is separate from that?", "ups": 1, "depth": 3, "author": "pullupsNpushups", "replies": [{"selftext": "MIOpen is the library part of the ROCm stack.", "ups": 3, "depth": 4, "author": "b3081a", "replies": [{"selftext": "I looked it up just now, and could only find HIP for Windows on AMD's site. I suppose the rest of the stack isn't ready yet. I'd have to look back at whatever post it was here mentioning that, but whatever the case, I'm supposing you're right and that we'll get it eventually.", "ups": 1, "depth": 5, "author": "pullupsNpushups", "replies": []}]}]}]}, {"selftext": "oh right the SDK for windows, finally. nicee", "ups": 2, "depth": 2, "author": "ilikeyorushika", "replies": []}]}]}, {"selftext": "I know it's somehow unrelated but is there someone here that that tried stable diffusion on windows with an AMD GPU? I recently got an RX 7800 XT and I've been tempted to try this out but I keep hearing different sounds of bell from everywhere.", "ups": 9, "depth": 0, "author": "BestPloot", "replies": [{"selftext": "You can today with directml. Hip sdk for windows was just released a few weeks ago. So native rocm on windows is days away at this point for stable diffusion.\n\nDirectml is great, but slower than rocm on Linux. Once rocm is vetted out on windows, it'll be comparable to rocm on Linux.\n\nRocm on Linux is very viable BTW, for stable diffusion, and any LLM chat models today if you want to experiment with booting into linux.", "ups": 14, "depth": 1, "author": "carl2187", "replies": [{"selftext": "I couldn't get oobabooba working on linux (problems with bitsandbytes, and then problems with docker to run bitsandbytes,) so now I use sd on linux and koboldcpp on windows. I'll stick to full windows once miopen is ported, because i can't get used to linux", "ups": 1, "depth": 2, "author": "218-69", "replies": []}]}, {"selftext": "I made a video last year for AMD GPU in Windows https://youtu.be/_6yP3Gv04-w?si=UUH_qOZ6wwpxywmh", "ups": 2, "depth": 1, "author": "chain-77", "replies": []}]}, {"selftext": "is it work with old 8GB Vram RX 590 ?", "ups": 6, "depth": 0, "author": "shendxx", "replies": []}, {"selftext": "ROCm is still limited to just Ubuntu. Which is shame, as CUDA is available on most major distros.  \n\n\nInstalling ROCm on anything else is major PITA.", "ups": 9, "depth": 0, "author": "LechHJ", "replies": [{"selftext": "Arch has it in its *repos* and arguably a better experience than the officially supported Ubuntu in some ways.", "ups": 12, "depth": 1, "author": "p4block", "replies": []}, {"selftext": "It works on all rhel 8 and 9 clones, and fedora, and centos stream with native packages provided by amd. \n\nAnd any distro with docker, so long as the amdgpu kernel driver is loaded.\n\nThen you can also compile from source on any distro, as rocm is open source.\n\nSo yea. Lots of options besides ubuntu exist.\n\nFedora is the best IMO. Uses the amd rhel 9.2 rocm repo no problem, and fedora provides up to date kernels and user packages for everything else.", "ups": 6, "depth": 1, "author": "carl2187", "replies": []}, {"selftext": "My experience with Automatic111 was that even on Ubuntu, installing Pytorch + the correct ROCm version was a PITA.", "ups": 5, "depth": 1, "author": "SabreSeb", "replies": []}, {"selftext": "U can use [Distrobox](https://github.com/89luca89/distrobox) and run any Linux distro u want :P  I use Fedora that already provides rocm-hip, rocm-runtime, rocm-opencl from official repositories.\n\nhttps://i.imgur.com/5t3ucSu.png", "ups": 3, "depth": 1, "author": "1stnoob", "replies": [{"selftext": "I have Ubuntu on second SSD, so it's not that big deal, but it's annoying. I didn't tried that hard to get it running. I'll try stable diffusion only after i upgrade my gpu, which is unlikely thanks to AMD now.", "ups": 1, "depth": 2, "author": "LechHJ", "replies": [{"selftext": "You actually think is harder then on Nivea or that for them it just works :&gt;&gt;&gt;\n\nAlso you can use [distrobox assemble](https://github.com/89luca89/distrobox/blob/main/docs/usage/distrobox-assemble.md) like i do to have everything magically configured in container :P", "ups": 1, "depth": 3, "author": "1stnoob", "replies": []}]}, {"selftext": "Which system monitor GUI is this?", "ups": 1, "depth": 2, "author": "gsedej_", "replies": [{"selftext": "[Mission Center](https://missioncenter.io/)", "ups": 1, "depth": 3, "author": "1stnoob", "replies": []}]}]}, {"selftext": "u can have working ROCm with stable diffusion tho on every linux distro", "ups": 5, "depth": 1, "author": "kr1spy-_-", "replies": [{"selftext": "To be honest, i tried on opensuse and it was problematic.\n\nProblem: nothing provides 'libffi.so.7()(64bit)' needed by the to be installedOpensuse use 8 already. Arch is no different, too.", "ups": 1, "depth": 2, "author": "LechHJ", "replies": []}]}, {"selftext": "&gt; Installing ROCm on anything else is major PITA.\n\nNah, that's easy. Finding these stupid models from broken google drive links that have gone over some arbitrary download limit or slow ass baidu links, and probably end up being incompatible revisions anyway, is a PITA lol", "ups": 2, "depth": 1, "author": "jackun", "replies": []}]}, {"selftext": "Very interesting, will check it out", "ups": 2, "depth": 0, "author": "Henevy", "replies": []}, {"selftext": "I already ran Stable Diffusion on my AMD for ages.", "ups": 4, "depth": 0, "author": "Prefix-NA", "replies": []}, {"selftext": "Video link [https://youtu.be/Yu0tLmfopXw](https://youtu.be/Yu0tLmfopXw)", "ups": 2, "depth": 0, "author": "chain-77", "replies": []}, {"selftext": "This may help [https://forum.level1techs.com/t/mi25-stable-diffusions-100-hidden-beast/194172](https://forum.level1techs.com/t/mi25-stable-diffusions-100-hidden-beast/194172)", "ups": 1, "depth": 0, "author": "liaminwales", "replies": []}, {"selftext": "Meh i just recently uninstalled linux or i could try this, hopefully this comes to Windows, or would it be possible to get this working on Windows via WSL ? i am guessing not.", "ups": 1, "depth": 0, "author": "Melodias3", "replies": [{"selftext": "Good thing is installing linux is not hard. Currently I don't think it works on WSL. However, they have added some Windows support for ROCm and hope we can use it on Windows soon.", "ups": 1, "depth": 1, "author": "chain-77", "replies": [{"selftext": "Give me a nudge btw if it comes to Windows, anyway may reinstall linux, i had arch linux installed manually step by step so i know linux quite well good enough to figure out how to fix my own errors.", "ups": 1, "depth": 2, "author": "Melodias3", "replies": []}]}]}, {"selftext": "is It actually stable tho", "ups": 1, "depth": 0, "author": "MaterialBurst00", "replies": [{"selftext": "It is. It uses the SDXL model", "ups": 1, "depth": 1, "author": "chain-77", "replies": []}]}, {"selftext": "Ayyy!!", "ups": 1, "depth": 0, "author": "OneYearSteakDay", "replies": []}, {"selftext": "Google collab show unknown gpu. I'm using RX 6600 and 7800 XT.", "ups": 1, "depth": 0, "author": "snorlaxgangs", "replies": []}, {"selftext": "FYI - I was getting close to 6 iterations per second with a 6700xt in ubuntu on a 512x512 image using Automatic1111 (pytorch/rocm).  2 seconds per itr seems like something is wrong and its not using the GPU?", "ups": 1, "depth": 0, "author": "B16B0SS", "replies": [{"selftext": "My results were for SDXL 1024x1024. 512x512 is much faster.", "ups": 1, "depth": 1, "author": "chain-77", "replies": []}]}]}
{"post": {"title": "Discounts on 7800xt", "subreddit": "Amd", "selftext": "Obviously no one knows the answer, but just curious if anyone thinks there may be discounts on the RX 7800 XT this upcoming holiday season.\n\nWhat has AMD (and its OBP) done in past years with newly released cards? Does anyone think the recently Nvidia price drops (4060TI 16 &amp; 4070) will lead to Radeon discounts?\n\nJust curious as to people\u2019s thoughts. I\u2019d love to get a 7800XT - just don\u2019t want to spend $500 &amp; I don\u2019t care about the starfield bundle", "ups": 26, "permalink": "/r/Amd/comments/16kyqa7/discounts_on_7800xt/", "num_comments": 68}, "replies": [{"selftext": "It seems to be selling well. Maybe we will see the 7700XT drop in price before the 7800XT?", "ups": 28, "depth": 0, "author": "20150614", "replies": [{"selftext": "Yeah I could see that too", "ups": 3, "depth": 1, "author": "NelsonCrypto2017", "replies": [{"selftext": "That would be my guess. I'm predicting 7700XT price drops as soon as 6700xts and 6750xts sell out. The 7800XT is in a prime price point as far as competition goes and is selling well. Besides a bundle or maybe some holiday deals on less popular AIBs, I don't see the 5800XT having much movement price wise anytime soon.", "ups": 3, "depth": 2, "author": "Tuned_Out", "replies": []}]}, {"selftext": "The microcenter by me marked up all their remaining 7800XTs but kept the 7700XTs MSRP but none of them are selling still lol. They should launched the 7700 at least at $400 to make it worth while rather than a $50-$70 difference between the 7800XT", "ups": 5, "depth": 1, "author": "ISimpPyramidheadDick", "replies": [{"selftext": "Ouch - I have a Microcenter near me actually", "ups": 2, "depth": 2, "author": "NelsonCrypto2017", "replies": [{"selftext": "That's good news at least, id keep an eye on their open box section. My buddy got a 6950 recently for $540 and it's as good as new with warranty. Coworker also snagged a 5800xt open box for $460. It was obviously returned because of loud coil whine but for the discount he doesn't care. we eliminated the coil whine with a slight undervolt unless the loud is very high.", "ups": 3, "depth": 3, "author": "Tuned_Out", "replies": [{"selftext": "Between my box fan in my room and a loud bird in the background coilwhine would be silent for all I care so that's a steal", "ups": 1, "depth": 4, "author": "ISimpPyramidheadDick", "replies": []}]}, {"selftext": "They were MSRP, but sold out in KC.  I waited the first week and then they disappeared on the weekend.", "ups": 2, "depth": 3, "author": "farmeunit", "replies": []}, {"selftext": "Watch the website like a hawk.  Check first thing in the morning and sometimes during the day, as in maybe 10 am or so and then later in the afternoon, and also at night before going to bed.  Open box items usually have a nice discount and can pop up at any time.  I have got lots of great deals from my local store.", "ups": 1, "depth": 3, "author": "farmkid71", "replies": []}]}]}]}, {"selftext": "-&gt; AMD have a year worth of stock, and will slowly add it on the market -&gt; price won't drop\n\n-&gt; China is buying and storing everything due to future sanctions when hitting Taiwan -&gt; Prices may raise.\n\n-&gt; Nvidia isn't going to release any cards challenging the 7800xt and 7700xt-&gt; no price drop\n\nDoubtful that prices will drop, unless another scandal or a specific model with issues", "ups": 10, "depth": 0, "author": "DjiRo", "replies": [{"selftext": "Good analysis there - you might be right", "ups": 3, "depth": 1, "author": "NelsonCrypto2017", "replies": []}, {"selftext": "You think 7800XT price wont fall due to China hoarding GPU's incase they get sanctioned if they invade/reclaim/occupy Taiwan?\n\n:|\n\nThat's not a take I expected to read today.\n\nNot saying you are correct/incorrect, just an interesting take.", "ups": 3, "depth": 1, "author": "dorkmuncan", "replies": []}, {"selftext": "Great points. Never thought about China storing GPU", "ups": 2, "depth": 1, "author": "Benphyre", "replies": []}, {"selftext": "&gt; China is buying and storing everything due to future sanctions when hitting Taiwan -&gt; Prices may raise.\n\nwhat do yo mean by that? China attacking Taiwan to occupy it? that won't happen soon.", "ups": 1, "depth": 1, "author": "SpicyKetch23", "replies": [{"selftext": "Right now China is already getting its hand on any graphic card with high VRAM due to AI (all those4060Ti16Go go there) and possible upcoming sanction.\n\nRegarding China and Taiwan, they've been preparing to get to the island for a few years now and multiplying exercices. It's not about \"if\", it's \"when\" they'll attack.\n\nAll in all, China buy as much as they can any graphic card, and that keep the prices high.", "ups": 4, "depth": 2, "author": "DjiRo", "replies": [{"selftext": "If thats happen we the gamers will be literally fucked. The covid period will be like walking in the park. Imagine destruction of tsmc. Apple Nvidia and AMD will go back 10 years. Prices will go to the sky. \n\nMaybe Samsung can replace them, are they based in south Korea? \n\nMaybe this is what US wait to complete and steal the fab they making in US now and then they will provoke China to attack. This what they did with current war in EU", "ups": 1, "depth": 3, "author": "Mastercry", "replies": [{"selftext": "&gt;Imagine destruction of tsmc.\n\nIt is gradually being transferred to US. Snail pace. But I bet the first evac flights are reserved for tsmc employees :D", "ups": 3, "depth": 4, "author": "Remote-Trash", "replies": []}, {"selftext": "USA is literally building a TSMC factory as we speak.", "ups": 0, "depth": 4, "author": "IrrelevantLeprechaun", "replies": []}, {"selftext": "*slow blink*", "ups": 1, "depth": 4, "author": "InsertMolexToSATA", "replies": []}]}]}]}]}, {"selftext": "Probably won\u2019t go lower than 450$ this year", "ups": 3, "depth": 0, "author": "Buris", "replies": [{"selftext": "Unless it stops selling well that's probably optimistic. Once the 6700xt and 6750xt sell out I think it's a given the 7700xt will fall pretty heavily to fill the price vacuum. The 7800xt is currently in a very safe price point vs Nvidias offerings and is selling well at least for now. \n\nThat could change if Nvidia price drops. They won't voluntary though, even to be competitive price wise. They command too much of the market and don't feel threatened. If Nvidia stock builds due to lack of sales it's still totally possible tho.", "ups": 4, "depth": 1, "author": "Tuned_Out", "replies": [{"selftext": "It would be interesting to see what would happen if nVidia decided to ruin AMD's party and dropped the 4070 to $500.", "ups": 2, "depth": 2, "author": "WhippersnapperUT99", "replies": []}, {"selftext": "Im not sure if AMD can produce enough cards to compete (of rx7700/7800)", "ups": 1, "depth": 2, "author": "Mastercry", "replies": []}]}, {"selftext": "I\u2019d like that", "ups": 1, "depth": 1, "author": "NelsonCrypto2017", "replies": []}]}, {"selftext": "I haven't done a proper survey, but a quick look at the amazon price history of the PowerColor Hellhound 7900 XT shows $50 price drops every few months since release (and similar for the 6800 XT). If that pattern holds, this holiday season would be right on schedule.", "ups": 3, "depth": 0, "author": "spinlox", "replies": [{"selftext": "This is a completely different beast, 7900 XT was over priced just like the 7700 XT now.\n\nThe 7800 XT price *went up* due to shortage.", "ups": 3, "depth": 1, "author": "rincewin", "replies": [{"selftext": "They're *all* overpriced. (Although this is not unique to AMD.)", "ups": 6, "depth": 2, "author": "spinlox", "replies": [{"selftext": "Compared to the $230 RX 580, yes all of them are over priced. \n\nCompared to the other GPU launches in the past year, this is the first card where the interest is bigger than the supply.  Now if the supply is limited on purpose or AMD was not expecting this much buyers... I cant tell, I'm hoping its the second one", "ups": 1, "depth": 3, "author": "rincewin", "replies": []}]}]}]}, {"selftext": "pretty sure the 770XT will have to drop considerably for any price drops to happen for the 7800XT, also the fact is that its selling like hotcakes right now so why would they drop the price.", "ups": 3, "depth": 0, "author": "ArtsM", "replies": [{"selftext": "Just don\u2019t know if there would be holiday deals - idk", "ups": 1, "depth": 1, "author": "NelsonCrypto2017", "replies": [{"selftext": "I wouldn't count on it, obviously you can wait and see if you don't need a GPU right now, but then ask yourself if saving $20~30 about 3 months from now is worth more than buying now at MSRP and using it for that time. If you don't want starfield you could do a half-way house and wait for the next game bundle, maybe that one is something you want to play so you get the game free and can treat that paying less for the card. :)", "ups": 2, "depth": 2, "author": "ArtsM", "replies": []}]}]}, {"selftext": "The 7800 XT would have to stay on shelves long enough to be discounted, but yeah probably by December the new card hype will fade a bit allowing AMD to drop the price some.  I think realistically the only GPU priced at a point where you can see a decent price drop would be the 7700 XT.", "ups": 3, "depth": 0, "author": "Exact-Explanation524", "replies": []}, {"selftext": "There is nothing in stock in eurozone on MSRP.", "ups": 4, "depth": 0, "author": "LechHJ", "replies": [{"selftext": "Yeah. I thought I would wait a week or two before retailers accumulate more stock and start competing on the price. Instead, RX 7800s now *are* in stock, but the price rose around \u20ac20. I guess this is how they decided to fix the bad RX 7700 XT pricing...", "ups": 3, "depth": 1, "author": "he29", "replies": [{"selftext": "I don't like the fix. The card is not worth for me more than MSRP, i'd rather get cheap 7900 xt than 670 euro 7800 xt, that's insane!", "ups": 2, "depth": 2, "author": "LechHJ", "replies": []}]}, {"selftext": "I can see some xtx  for 559\u20ac which is actually lower than msrp.", "ups": 1, "depth": 1, "author": "pesca_22", "replies": [{"selftext": "Isn't 549\u20ac MSRP? Either way, too big for my case, need up to 320mm.", "ups": 4, "depth": 2, "author": "LechHJ", "replies": []}]}]}, {"selftext": "Right now the 6800 XT is around the same price new, and provides very similar performance. I'd expect the price of the 6800 XT to drop faster, so if you're really tight on cash and don't really need any of the new features of the 7000 series, that could be an option.\n\nYou also might be able to get a 6800 XT used.\n\nRight now the 4070 is still selling at a higher price, is slower, and has less vram, so I don't think it will help reduce the price of the 7800 XT. The 4060 Ti 16 is SIGNIFICANTLY slower than the 7080 XT, so I wouldn't even compare it.", "ups": 2, "depth": 0, "author": "CrispyPizzaRolls", "replies": [{"selftext": "Fair enough. My only concern with the 6800xt is that my PSU is only 650W. The additional power savings on your 7800xt (albeit not significant) is enough to make me more interested", "ups": 3, "depth": 1, "author": "NelsonCrypto2017", "replies": [{"selftext": "i run a Sapphire Pulse 7800 XT with a 600w psu from BeQuiet 80gold, Ryzen 7600 cpu , 32 gb ram all on a B650 AORUS ELITE AX . No problems so far. \n\nEDIT: I undervolted the GPU with the automatic tuning option from adrenaline, from 238 wats/119FPS it went down at 204wats/116FPS in Starfield, that's an epic win.", "ups": 4, "depth": 2, "author": "SpicyKetch23", "replies": []}, {"selftext": "That definitely is an important difference that I overlooked. I have a feeling 650W will be enough for the 7800 XT, but probably cutting it too close for the 6800 XT, just like you said.\n\nThe 4070 does use less power than the 7800 XT, but I still don't recommend it, since it's slower and has less vram anyway. Unless you needed a specific feature from the 4070.", "ups": 3, "depth": 2, "author": "CrispyPizzaRolls", "replies": [{"selftext": "My feeling exactly about the 4070. You didn\u2019t overlook it - you had no way of knowing my PSU", "ups": 3, "depth": 3, "author": "NelsonCrypto2017", "replies": []}, {"selftext": "If you don't have a crazy system, a good quality 550W gold PSU is more than enough for 7800XT. The recommendations are wrote to consider some super low quality power supplies. There are already reports of people running 7800 cards with OC on 550W with no problems whatsoever.", "ups": 2, "depth": 3, "author": "brassramen", "replies": [{"selftext": "It really just depends on the system and how close people want to cut it. \n\nRX 7800 XT when OCed would be [263W](https://en.wikipedia.org/wiki/Radeon_RX_7000_series#Desktop) \\* +15% power limit (OC) = \\~303W\n\nI don't know what CPU the OP has, but many of the [Intel 13th gen 65W CPUs have a turbo of 219W](https://en.wikipedia.org/wiki/Raptor_Lake#Raptor_Lake-S).\n\nThat's 522W alone. Ignoring cooling, memory, motherboard power, hard drives, possibly VR headset, etc.\n\nEven if we went with a lower power CPU, I think we're already a bit too close to 550W.", "ups": 1, "depth": 4, "author": "CrispyPizzaRolls", "replies": [{"selftext": "I\u2019m running a Ryzen 9 5900x. The PSU is a Corsair RM650x", "ups": 1, "depth": 5, "author": "NelsonCrypto2017", "replies": []}]}, {"selftext": "I have a crappy Corsair VS600 power supply right now currently running an nVidia 1080.  I wonder if it would run the 7800 XT.", "ups": 1, "depth": 4, "author": "WhippersnapperUT99", "replies": []}]}]}, {"selftext": "What CPU are you using? The 6800xt will generally run fine on a 650 watt depending on what else is running on it.", "ups": 1, "depth": 2, "author": "Tuned_Out", "replies": [{"selftext": "I have a Ryzen 9 5900x", "ups": 1, "depth": 3, "author": "NelsonCrypto2017", "replies": []}]}]}]}, {"selftext": "Unless AMD start to ship these in greater numbers you even need to wait for weeks (or months) to get one at MSRP.\n\nMaybe early next year if we are lucky...", "ups": 2, "depth": 0, "author": "rincewin", "replies": [{"selftext": "I'm wondering if AMD is holding out until after the Starfield promotion ends.", "ups": 3, "depth": 1, "author": "WhippersnapperUT99", "replies": []}]}, {"selftext": "Considering the robust demand I expect AMD and retailers plan on pushing as many at MSRP this holiday season as possible.", "ups": 2, "depth": 0, "author": "daab2g", "replies": []}, {"selftext": "&gt;just curious if anyone thinks there may be discounts on the RX 7800 XT this upcoming holiday season.\n\nIf you don't value Starfield at say $50 or don't value it at all, then your best bet might be to \"make your own deal\" when the next \"Zipfest\" or \"Affinity\" payment deal at Newegg comes around where you could take 10-15% off depending on the deal.  Alternatively if you have a Best Buy credit card that might get you a discount.  There's also an 8% cash back deal from Retail Me Not.com that shows up at Best Buy every now and then.  (I just used that to get $48 cash back on a TV.)", "ups": 2, "depth": 0, "author": "WhippersnapperUT99", "replies": [{"selftext": "Good ideas - thanks. I\u2019ll check out tue Newegg Zipfest thing", "ups": 1, "depth": 1, "author": "NelsonCrypto2017", "replies": []}]}, {"selftext": "Discounts? Maybe discount back to the original prices.", "ups": 2, "depth": 0, "author": "OrangeYouGladish", "replies": []}, {"selftext": "If it keeps selling at this pace then no, if anything it might see an increase (mostly by retailers). Other than that, lemme get my crystal ball.", "ups": 2, "depth": 0, "author": "zoomborg", "replies": []}, {"selftext": "The 7800xt hellhound has increased 20$ everywhere (499 to 519) for NO reason so yeah, there wont be any discounts haha", "ups": 2, "depth": 0, "author": "pelusilla6", "replies": [{"selftext": "You might be right", "ups": 1, "depth": 1, "author": "NelsonCrypto2017", "replies": []}]}, {"selftext": "once they sell out the 6700 and 6800, they will", "ups": 2, "depth": 0, "author": "dracolnyte", "replies": []}, {"selftext": "Not so long as the 6800 XT remains available.", "ups": 3, "depth": 0, "author": "Mageoftheyear", "replies": []}, {"selftext": "You could probably sell the Starfield code if you don't care for it, for a few dollars or wait a bit as AMD have quite a few 77/7800XTs to sell from all accounts, but then you may not get a sellable Starfield code.", "ups": 2, "depth": 0, "author": "sparkle-oops", "replies": [{"selftext": "You could potentially sell it but you'd need a friend's steam log in, they don't provide you with a code, instead you install a program that identifies the hardware and then through AMD rewards it approves it. \n\nThen you select what program you want to run it on, and it automatically assigns the game to the account signed in. \n\nAnnoying for sure, meant that I wasted money on a key and then got the premium version for free.", "ups": 3, "depth": 1, "author": "nowyuseeme", "replies": [{"selftext": "I mean sortof? At the end the only thing you need to do is give someone else access to your AMD rewards, then the buyer needs to link his steam account, claim the key and then immediately unlink.\n\nIt's a bit cumbersome, especially compared to \"just\" a key, but it's very doable.\n\nYou make it sound like it's a lot harder, but maybe I'm misreading or something.", "ups": 0, "depth": 2, "author": "Hixxae", "replies": []}]}, {"selftext": "That\u2019s true - that could be an option", "ups": 1, "depth": 1, "author": "NelsonCrypto2017", "replies": []}]}, {"selftext": "Prices more likely to drop on the 6000 series than 7000.", "ups": 1, "depth": 0, "author": "Sparta2019", "replies": []}, {"selftext": "Not by this holiday. It\u2019s selling decently. But probably by end of summer next year", "ups": 1, "depth": 0, "author": "Mother-Translator318", "replies": []}, {"selftext": "Fui i run my 7800xt with a 500w ps or 550. Sure good quality. So take the chance :) imo amd usually keeps prices as they are. But hey who knows. Game on.", "ups": 1, "depth": 0, "author": "Giantmufti", "replies": [{"selftext": "Okay thanks - good to know", "ups": 1, "depth": 1, "author": "NelsonCrypto2017", "replies": []}]}, {"selftext": "Is it just me or as the price actually gone up? I can't find the card anywhere for anything near MSRP.", "ups": 1, "depth": 0, "author": "Nebulex", "replies": []}]}
{"post": {"title": "My experience with AMD(for now)", "subreddit": "Amd", "selftext": "So, I have been with AMD for a while now. I currently have a Biostar Rx 6900 Xt and a ryzen 5 7600x.\n\nI replaced my Rx 5600 Xt with the Rx 6900 Xt about 3 mouths before this post, bought a MSI MPG 850GF PSU too. At the time my problem was only my CPU because the i5-10400f bottlenecks the Rx 6900 Xt by a lot. I was getting 40Fps in a game before I upgraded my CPU.\n\nI replaced the i5-10400F I had, with the ryzen 5 7600x and in the same game I get about 140-160Fps.\n\nThe issues I had(or still have):\n1. The Rx 6900 Xt runs really hot(about 90C and 100C on the hot spot) which is likely not the GPU's problem but the manufacturer(Biostar).\nThe GPU is undervolted too.\n\n2. After installing the ryzen 5 7600x on to my motherboard and then in the PC, it started, shutdown and the ram lit up.(the problem was that the motherboard/CPU power cables were not pluged correctly in the PSU) Then I fixed it but then It started doing the same thing exept the fans started to spin. I reseated the CPU, I removed the GPU, the SSDs and started the pc again - same issue. After that I decided to check the issue in google and most of the sources say that the motherboard was the problem(which was not) and finally I found out the right answer which was that the Ram was training(really odd, never had this before)\n\n3. This is not an issue but something that I dont quite like. The CPU was reaching 90C under load( I have an AK400 Zero Dark Plus cooler) which I fixed by undervolting and under powering the cpu, the performance was not different than before, but now it stays at 75C which is the thermal throttle I set.\n\n4. I downloaded the Gigabyte Control Center(because the motherboard was from Gigabyte) and updated/installed all the drivers and software. Ryzen Master, AMD Chipset drivers and netron internet security(or something similar) all failed to download. Searched through google, tried everything but nothing worked. This is not a problem because there won't be any difference in performance but would be great to have them.\n\n\nThat's all, I really enjoy the new parts. AMD did really good with their new hardware. There is such a big performance difference between the old parts.", "ups": 17, "permalink": "/r/Amd/comments/16kv180/my_experience_with_amdfor_now/", "num_comments": 104}, "replies": [{"selftext": "Just so you know, every modern CPU gets hot, especially on lower end coolers. To Intel and AMD, thermal headroom just means extra performance on the table.", "ups": 51, "depth": 0, "author": "CheemsGD", "replies": [{"selftext": "True, the 7000 series are made to reach 95C to get to the max frequency and performance they can get.", "ups": 2, "depth": 1, "author": "Alternative-Fly-1727", "replies": [{"selftext": "All 7000X tunned really aggressive and boost until they hit certain limit and temperature limit tend to be first one. Non-X CPU are much cooler. \n\nAbout GPU temperature: i have RX 5700 from PowerColor(Red Dragon) and it was hitting 95C hot spot(and 76-77C regular temp) from the box. It works that way for 4 years already(but some games i used with 60 fps lock due to tearing). \nKnowing Power Color is AMD partner i would assume it's expected behaviour and seen as a normal temperature.", "ups": 7, "depth": 2, "author": "RAZOR_XXX", "replies": [{"selftext": "Yep, you have power color, such a big difference. Biostar is just bad. The biostar model was the cheapest one so I went with it. I could have easily gotten an asus rx 6800 xt for 320 euro 2nd hand to fix the problem.\n\nI bought my GPU for about 550-600 euro btw.", "ups": 3, "depth": 3, "author": "Alternative-Fly-1727", "replies": []}, {"selftext": "I had a r5700xt which was the exact same way, worked perfectly but the fans would give me jumpscares. I would boot up tarkov and it would go straight to 80C, 110 hot spot, runs perfectly at that temp, stays at it, and has shown no signs of slowing down.", "ups": 1, "depth": 3, "author": "jdaprile18", "replies": [{"selftext": "Would it stay 110C all the time? And it sounds like the driver problem i had in 2019.", "ups": 1, "depth": 4, "author": "RAZOR_XXX", "replies": [{"selftext": "The Hotspot is currently 110 the entire time I'm playing starfield, game still runs great", "ups": 1, "depth": 5, "author": "jdaprile18", "replies": [{"selftext": "I would've already repasted GPU if it was hitting 110C. Hope your will be fine.", "ups": 1, "depth": 6, "author": "RAZOR_XXX", "replies": []}]}]}]}]}, {"selftext": "You need to update your view on CPU thermals.\n\nModern CPUs are manufactured in tiny processes and are physically very small, it's hard to conduct heat out of it, and therefore by design, they are to work with such temperatures during load.\n\nWhat matters is power usage (i.e. actual source of heat) and performance. If power usage is low and performance is high, the CPU running at 95C is not an issue at all. It's not going to die early or anything because of this.\n\nYou might ask why use a better cooler then? The answer is better cooling, instead of decreasing the temperature in the past, it now gives better sustained peak performance.", "ups": 17, "depth": 2, "author": "kazenorin", "replies": [{"selftext": "I am still stuck in that early am4 era :D", "ups": 5, "depth": 3, "author": "Alternative-Fly-1727", "replies": [{"selftext": "Yeah, I kind of know how that feels, it's a convenient metrics that you used to be able to push down with a bit of work. Which could be satisfying.", "ups": 4, "depth": 4, "author": "kazenorin", "replies": []}]}]}, {"selftext": "Is there an eco mode in the bios for the CPU? Most people were reporting only a 1 to 3% performance decrease but drastically less thermals with it active. If you want to go down the rabbit hole of custom undervolting it you can get way less thermals for only 1%ish loss as well.", "ups": 1, "depth": 2, "author": "Tuned_Out", "replies": [{"selftext": "Yeah, already fixed the high temps with the CPU by undervolting, setting curve optimizer and putting a 75C thermal throttle temp.", "ups": 1, "depth": 3, "author": "Alternative-Fly-1727", "replies": []}]}]}]}, {"selftext": "Biostar isn't a good AIB partner. They literally made the worst RX 6600 model and i haven't heard much about their other models. Next time i suggest going with trusted AIB's like Sapphire, Powercolor, ASRock or XFX.", "ups": 20, "depth": 0, "author": "X_irtz", "replies": [{"selftext": "Yeah I've seen reviews on the biostar rx 6600 but my 6900 looks to have a normal heat sync, plus that its a 3 fan model.", "ups": 3, "depth": 1, "author": "Alternative-Fly-1727", "replies": [{"selftext": "I looked it up, it's basically just a rebranded reference model, it should be fine in that case, better than whatever they come up with. I have no idea how you even found that model though, can't say i've seen it before in any stores.", "ups": 8, "depth": 2, "author": "X_irtz", "replies": [{"selftext": "Maybe even worse than a reference card, also there is two biostar models, one as reference card and the other like a XFX. I have the XFX looking one.\nBought it online. I live in Europe so stores aren't a good source for parts.", "ups": 3, "depth": 3, "author": "Alternative-Fly-1727", "replies": []}]}]}]}, {"selftext": "&gt;The Rx 6900 Xt runs really hot(about 90C and 100C on the hot spot) which is likely not the GPU's problem but the manufacturer(Biostar). The GPU is undervolted too.\n\nIn my region, it was the cheapest 6900XT costing the same as 6800XTs  \nThis model - [https://www.biostar.com.tw/app/en/vga/introduction.php?S\\_ID=298](https://www.biostar.com.tw/app/en/vga/introduction.php?S_ID=298)\n\nOther users reported that the cooling is meh on this model, but their temperatures are lower than yours.  \nThere's possibility that it could be fixed by changing thermal paste/thermal pads.\n\nApparently Biostar also made 6900XT ref card,  which costed  more, but haven't seen it selling for decent prices.  \nThis model - [https://www.biostar.com.tw/app/en/vga/introduction.php?S\\_ID=285](https://www.biostar.com.tw/app/en/vga/introduction.php?S_ID=285)  \n\n\n&gt;This is not an issue but something that I dont quite like. The CPU was reaching 90C under load( I have an AK400 Zero Dark Plus cooler) which I fixed by undervolting and under powering the cpu, the performance was not different than before, but now it stays at 75C which is the thermal throttle I set.\n\n  \n90C under the load seems to be normal working temperature for Ryzen 7000 series", "ups": 6, "depth": 0, "author": "Erufu_Wizardo", "replies": [{"selftext": "I just clicked the first link that you posted, and yes this is my GPU.\nBtw in the site I bought the GPU it looked like a reference card but i got the black one sent to me, now I feel scammed.\n\nI already figured out the temps of the CPU, but I prefer to have my components under 75C.", "ups": 3, "depth": 1, "author": "Alternative-Fly-1727", "replies": [{"selftext": "Yeah, half of the shops in my region used incorrect photos as well.  \nBut the stated model name was correct.\n\nI was also considering this card but in the end went for 6800XT from Asus.\n\nMy 6800XT can match your 6900XT with overclock.\n\nBut the other reddit user owning this Biostar 6900XT said they managed to OC it and get additional 5-10% performance uplift.  \nThey have lower temps than you in stock, but their card has issues with ZeroRPM.\n\nSeems like a lottery.", "ups": 2, "depth": 2, "author": "Erufu_Wizardo", "replies": [{"selftext": "Yeah, over all I wouldn't recommend biostar. I won't be doing overclocking because I'm happy with the performance I get now, and also the thermal issue with the GPU seems to have died down(running 68C normal temp and 85C hotspot).", "ups": 2, "depth": 3, "author": "Alternative-Fly-1727", "replies": [{"selftext": "Yeap, their GPUs are hit or miss.  \nI've also seen reviews for their 6600XT.   \nThe buyers of their 6700XTs seemed to be satisfied, but..\n\nFor 6000 series, hotspot temps lower than 100C are seen as okayish/normal.  \n100-110C is a red zone though.\n\nBtw you can try installing additional fans in your PC case. Might help too.", "ups": 2, "depth": 4, "author": "Erufu_Wizardo", "replies": [{"selftext": "The biostar rx 6600 xt might be the worst card. The only good thing about biostar might be the price.  \n\n\nI have 4 fans rn, I have 2 more slots at the top of the case, I don't really think it'll help.", "ups": 2, "depth": 5, "author": "Alternative-Fly-1727", "replies": [{"selftext": "My general impression is that  budget brands are lottery in general.  \nLike I was looking reviews on AsRock 6800XTs.  \nSome people were happy, but one guy had to repaste his new card and change thermal pads to get normal temps. Like it was 20C difference or something.", "ups": 2, "depth": 6, "author": "Erufu_Wizardo", "replies": [{"selftext": "Cheap manufacturers use cheap paste and pads. AsRock isn't a bad manufacturer, the GPU was probably used for a long time before they decided to change the paste and the thermal pads.", "ups": 2, "depth": 7, "author": "Alternative-Fly-1727", "replies": [{"selftext": "Nah, it was new AsRock GPU. That's the thing.  \n\n\nThen again, every manufacturer has faulty GPUs", "ups": 1, "depth": 8, "author": "Erufu_Wizardo", "replies": [{"selftext": "Yeah, you can never know.", "ups": 2, "depth": 9, "author": "Alternative-Fly-1727", "replies": []}]}]}]}]}]}]}]}]}]}, {"selftext": "Don't touch gigabyte software. Download and install manually from AMDs website.  Gigabyte tries to install bloat on your machine and their software is horrible", "ups": 3, "depth": 0, "author": "tenfootgiant", "replies": [{"selftext": "I already tried to install Ryzen Master and AMD Chipset Drivers manually but they fail to install.", "ups": 1, "depth": 1, "author": "Alternative-Fly-1727", "replies": [{"selftext": "You might have to update your bios first. If you do, make sure to enable xmp again", "ups": 1, "depth": 2, "author": "tenfootgiant", "replies": [{"selftext": "Im going to try to update it, and I use EXPO for the ram profile.", "ups": 1, "depth": 3, "author": "Alternative-Fly-1727", "replies": [{"selftext": "For clarification, I get Error 1720 for the Chipset drivers", "ups": 1, "depth": 4, "author": "Alternative-Fly-1727", "replies": [{"selftext": "[The Error](https://ibb.co/Lk5dybP)", "ups": 1, "depth": 5, "author": "Alternative-Fly-1727", "replies": [{"selftext": "Update BIOS to the newest available, AMD Chipset drivers and Ryzen master fail the installation, with the same error.", "ups": 1, "depth": 6, "author": "Alternative-Fly-1727", "replies": [{"selftext": "I believe the DDU tool has ways to clean out the chipset drivers which is probably part of the cause of the ryzen master issue too.", "ups": 1, "depth": 7, "author": "tenfootgiant", "replies": [{"selftext": "I already tried with DDU it did not work.", "ups": 1, "depth": 8, "author": "Alternative-Fly-1727", "replies": []}]}]}, {"selftext": "This error could mean that the file is corrupt and you would need to download it again. Don't extract the download, just double click it because its a self extracting installer.\n\nThis error could also point to a problem with your Windows Installer service. Windows uses \"MSI\" to install applications without those needing to bring their own setup.\n\nMaybe it could help to check your system for problems, because you will run into install issues with this in the future too, if that's the cause.\n\nYou can try SFC to fix it [https://support.microsoft.com/en-au/topic/use-the-system-file-checker-tool-to-repair-missing-or-corrupted-system-files-79aa86cb-ca52-166a-92a3-966e85d4094e](https://support.microsoft.com/en-au/topic/use-the-system-file-checker-tool-to-repair-missing-or-corrupted-system-files-79aa86cb-ca52-166a-92a3-966e85d4094e)\n\nOr DISM, which also checks your Windows updates for issues [https://learn.microsoft.com/en-us/windows-hardware/manufacture/desktop/what-is-dism?view=windows-11](https://learn.microsoft.com/en-us/windows-hardware/manufacture/desktop/what-is-dism?view=windows-11)\n\nRegarding DISM there is a way to use it with online mode instead of providing a Windows install CD or USB stick. The command would be \"dism /online /cleanup-image /restorehealth\"", "ups": 1, "depth": 6, "author": "Saladino_93", "replies": []}]}]}, {"selftext": "Yeah, different vendors call it different things even though xmp is an Intel thing. Usually EXPO or docp", "ups": 1, "depth": 4, "author": "tenfootgiant", "replies": [{"selftext": "Yeah same thing with SAM and ReBar", "ups": 1, "depth": 5, "author": "Alternative-Fly-1727", "replies": []}]}]}]}]}]}, {"selftext": "You might consider limit fps if you want cooler/silent operation. I always play at max 60fps\n\nAlso, the case and fans better be good with that setup. A cheap case will run hotter. Also, summer is a big factor if you play in a hot room.", "ups": 3, "depth": 0, "author": "-JMG00-", "replies": [{"selftext": "I have a 144Hz monitor so its a sin to lock my fps.\n\nThe case is shit, I cheaped out 7 months ago when I broke my 20\u20ac case and now i have PowerCase j188-17 which has terrible air flow but has 4 fans( costs about 50\u20ac)", "ups": 2, "depth": 1, "author": "Alternative-Fly-1727", "replies": [{"selftext": "Well, cheap cases are noisy and don't have good temps\nDon't know yours, I have a bequiet 500dx and works wonderfull and it's not super expensive.\n\nAbout high refresh rate monitors, in my opinion those are a bit of a trap. To reach those FPS in anything res except 1080 you need the best setup. If you don't cap FPS, your pc will keep trying its best all the time, in light or heavy scenes, without any moment to relax below 100% usage. You might want to try Radeon chill and set 75-144 FPS as min-max, that will allow action scenes up to 144 and still scenes down to 75, allowing for cooling (in those games you're reaching 144 FPS). Also, don't let vsync off which you give you more FPS than the 144mhz your monitor reaches, this will further increase GPU usage and temps.\n\nEnd remark: 110 hotspot is the max. 100 is fine. In fact Radeon cards will keep pushing further and further given there is power and temp headroom. The more power and the more temp headroom, the more it will push if you don't limit somehow. It's by design.", "ups": 2, "depth": 2, "author": "-JMG00-", "replies": []}]}]}, {"selftext": "1. Your 6900XT is a lemon if it\u2019s getting 90C out of the box. You can try to repaste it or return it, it\u2019s what I would do.\n\n2. There were warnings everywhere that AM5 does RAM training on first boot up\n\n3. 7000 series non-X3D\u2019s get as hot as possible on purpose, overclocking to 95C by default (like a laptop processor). It\u2019s not a bug it\u2019s a feature. I personally like to keep my CPUs a little colder as well though, so I get it\n\n4. That is a problem, never heard of that happening, maybe a security setting in windows or an over-active antivirus?", "ups": 3, "depth": 0, "author": "Buris", "replies": [{"selftext": "I did a test again on The last of us with uncapped FPS and got  68C for the GPU temp and \\~85C for the hotspot/junction temp. I really don't know why it was 90C the last time I was playing. Also as mentioned above, it isn't the card its the manufacturer of the card.  \n\n\nI didn't know about the RAM training, I heard that the AM5 CPUs are really picky when it comes to RAM.  \n\n\nAs mentioned the 95C is not an issue but design, just not my preference.  \n\n\nAbout the chipset drivers and ryzen master, I really don't know how to fix them. Others had the same problem and fixed it by multiple ways but non of them work for me. It isn't a problem but it would be nice to have them.  \n\n\nOver all, I don't know why people say that AMD has driver problems and other stuff, my experience is really good for now.", "ups": 1, "depth": 1, "author": "Alternative-Fly-1727", "replies": [{"selftext": "Why not just download them from the mb manufacturers site? No need for their software, they're more often than not very buggy and not needed.", "ups": 1, "depth": 2, "author": "gr1nna", "replies": [{"selftext": "Still doesn't work. Same error.", "ups": 1, "depth": 3, "author": "Alternative-Fly-1727", "replies": []}]}]}]}, {"selftext": "You could also undervolt your gpu but really I\u2019m just curious what case and fan setup you are working with.", "ups": 2, "depth": 0, "author": "Seno96", "replies": [{"selftext": "I have undervolted my gpu.\nI have 4 case fans installed in my powercase j188-17\nMy panel is open because that case blocks 3 of the 4 fans.", "ups": 1, "depth": 1, "author": "Alternative-Fly-1727", "replies": [{"selftext": "That case looks pretty shit in terms of airflow. And I can see that the default fan layout is probably sub optimal as you would want more exhaust fans. So all this defo adds to the equation.", "ups": 2, "depth": 2, "author": "Seno96", "replies": [{"selftext": "I keep the case open, I have even done modifications to it.", "ups": 1, "depth": 3, "author": "Alternative-Fly-1727", "replies": []}]}]}]}, {"selftext": "&gt;The Rx 6900 Xt runs really hot(about 90C and 100C on the hot spot) \n\nAt which RPM and what is the *GPU current* temp? If the card is not loud then its probably fine.", "ups": 3, "depth": 0, "author": "PantZerman85", "replies": [{"selftext": "Its loud, running at about 80-100% fan speed in MSI afterburner.\n\nEdit: 90C with that fan speed.", "ups": 2, "depth": 1, "author": "Alternative-Fly-1727", "replies": [{"selftext": "If you get 90\\~100C hotspot at 100% fan speed then the card has issues with the cooler. GPU sag, mounting pressure, thermal paste to mention some.\n\nIf you dont want to mess around with the hardware I recommend returning it for another if you can.\n\nMy card had all these issues except maybe the thermal paste (can be read [here](https://www.reddit.com/r/Amd/comments/q88wr2/comment/hxotwis/?utm_source=share&amp;utm_medium=web2x&amp;context=3)). I repasted (MX-5), added shims to the screws around the GPU die and tightened. Now  temps peaks at about 71/92C (21C delta) while maxing out at about 50% fan speed (1700 RPMish).", "ups": 10, "depth": 2, "author": "PantZerman85", "replies": [{"selftext": "The card is new, so I won't open it to change pads or paste because it still has 2 years of warranty.\n\nIt probably doesn't have problems with the cooler, its like 2-3 mouths old.\n\nThe game that it was running 90C was the last of us.\nThe problem is that the model/manufacturer of the card is just bad. I probably won't return it because I have no other issues than that. If it gets really bad I will probably change the paste with MX-4 I have. A nother problem I saw upon inspecting the card is that the VRAM 's pads are half with contact to the heat sync.\n\nAlso its not tension or sag, it looks fine. If you want I can post a picture of the GPU in the case once I get back home.", "ups": 0, "depth": 3, "author": "Alternative-Fly-1727", "replies": [{"selftext": "&gt;The card is new, so I won't open it to change pads or paste because it still has 2 years of warranty.\n\nIf you're in the US (or most other countries), this won't void your warranty. The manufacturer has to be able to prove that your actions damaged the card, and \"because we said so\" isn't a valid excuse for them to deny warranty claims.", "ups": 3, "depth": 4, "author": "ishootforfree", "replies": [{"selftext": "I live in Europe, I'm not sure if its the same but I rather not touch it for now.", "ups": 1, "depth": 5, "author": "Alternative-Fly-1727", "replies": []}]}, {"selftext": "&gt;The game that it was running 90C was the last of us.\n\n90C hotspot, but what is the *GPU current* temp and what RPM was required to hit 90C?\n\nYou want to look at the gap between the *hotspot* and *GPU current.* I would say less than 30C difference is good. More than 30C is bad. Ofcourse in the end its up to you and what you can live with.\n\n&gt;Also its not tension or sag, it looks fine. If you want I can post a picture of the GPU in the case once I get back home.\n\nDoenst hurt to gently push the card/cooler up from bellow to see if it makes a difference to the temp.\n\nWhen I got my card it was running at 100C+ hotspot at 100% fan speed without the included foot/stand. With the foot/stand it was still not great.", "ups": 1, "depth": 4, "author": "PantZerman85", "replies": [{"selftext": "The GPU temp is 90C, the hotspot is 100C.\n\nThe gap between the hotspot/junction temp and the GPU temp is about 10-15C.\n\nI am also thinking of getting a gpu sag bracket, I have a AM5 correction bracket ordered too.\n\nEdit: I'm not sure about the RPM just the fan speed in %", "ups": 1, "depth": 5, "author": "Alternative-Fly-1727", "replies": [{"selftext": "GPU temp of 90c is not fine, hotspot is fine, the ambient temp around GPU isn't. Try opening the side of your case and see what temps you get. GPU worst case temps are between 80 and 92c, you are really close to the worst case limit which is not good at all. Most systems run too hot with the side panel mounted, try without it.", "ups": 2, "depth": 6, "author": "loktari", "replies": [{"selftext": "Side panel off, undervolt on GPU and CPU. Seems to run well. I also had my side panel off when I had 90C on the GPU, Idk why its running cool now but seems to be fixed.  \n[Screenshot of the temps](https://ibb.co/VY7R242)", "ups": 1, "depth": 7, "author": "Alternative-Fly-1727", "replies": [{"selftext": "Looking good", "ups": 2, "depth": 8, "author": "loktari", "replies": []}, {"selftext": "Have you adjusted the GPU fan profile so that it operates at a low RPM while idle? I've found that brought my idle temps down from 45C to 30C on the GPU sensor, 48-50C to 38-40C on the hotspot sensor, and max sustained temps are like 75-80 for GPU sensor and 85-90C for hotspot, where as before having idle fan max temps were hitting about 5C higher.\n\nYou might get some benefit from that if you're outside of the return period. I know repasting yourself will void warranty, can you get it repasted via warranty though? Is that part of the guarantee?", "ups": 2, "depth": 8, "author": "KingFIippyNipz", "replies": [{"selftext": "the lowest my card gen get is 40% fan speed or 0% if I have \"Zero RPM\" mode on in the AMD software. I have also adjusted the fan curve from MSI afterburner.  \nI am still not sure if opening up the gpu will void warranty, I read the warranty terms and I don't see anything that says I can't/can do that.", "ups": 1, "depth": 9, "author": "Alternative-Fly-1727", "replies": []}]}]}]}]}]}]}]}]}]}, {"selftext": "Having PBO as the stock power behavious was really dumb IMO, i can get it on the 7950X and 7900X where you want to edge out the 13900K and 13700K in multicore but it still is 5% better perofrmance for 2x the consumption. Even worse for the 7700X that still loses to the 13600K in multicore and the 7600X that doesn't even get a boost, they're just wasting power. The Eco mode should have been the default.", "ups": 1, "depth": 0, "author": "Cave_TP", "replies": [{"selftext": "The eco mode is the default on the non X models.", "ups": 2, "depth": 1, "author": "Star_king12", "replies": [{"selftext": "The non X ryzen 5 7600 is 65W so it makes sense that it runs cooler.", "ups": 1, "depth": 2, "author": "Alternative-Fly-1727", "replies": [{"selftext": "I think it's crazy that my undervolted 7800X3D hits 77w max under full load. The 3D chips are crazy efficient.", "ups": 2, "depth": 3, "author": "random_reddit_user31", "replies": [{"selftext": "The X3D ryzen variants still surprise me, I was thinking of getting the 7800X3D but I don't really need it for the rx 6900 xt.", "ups": 1, "depth": 4, "author": "Alternative-Fly-1727", "replies": [{"selftext": "Yeah I'm blown away with it. I had a 5800X3D and that was awesome too. I wanted to get on to AM5 and it was only \u00a350 more expensive than the 7700X, so it was a no brainer. But yeah the 7600X is a great CPU and plenty powerful enough for the foreseeable. You're probably better getting a next gen 3D CPU if you feel the need to in the future.", "ups": 2, "depth": 5, "author": "random_reddit_user31", "replies": [{"selftext": "Yeah the 5800X3D is pretty good too, the ryzen 5 7600x performs about the same, but that extra cache helps too.", "ups": 1, "depth": 6, "author": "Alternative-Fly-1727", "replies": []}]}]}]}]}]}]}, {"selftext": "the \"issues\" you mentioned except for the software part are by design and it seems that you did not research enough before making the purchase. It's been generations of CPU/GPU where they maximize the performance by pushing the limits of thermal and power.", "ups": 1, "depth": 0, "author": "_sendbob", "replies": [{"selftext": "Trust me, when spending this much money on a PC its necessary to research before buying.  \nI spent almost 6 months searching for parts, I am aware about the temps of the CPU, I just said that I'd prefer to have it around 75C.", "ups": 1, "depth": 1, "author": "Alternative-Fly-1727", "replies": []}]}, {"selftext": "Not sure if possible, but if you are really bothered by those gpu temps, aftermarket solutions might exist.", "ups": 1, "depth": 0, "author": "Thesadisticinventor", "replies": []}, {"selftext": "Might be worth changing the thermal paste and pads on the GPU with better stuff, it will probably lower your temps. Also, 6900XTs run cooler when under-volted, but this depends on the silicon lottery.\n\n Much like Ryzen 5000 processors when adjusted with Curve Optimizer, the voltage slider in the Adrenalin control panel is a mV offset that is applied to the entire voltage curve within the driver itself. The \"voltage doing whatever it pleases\" is normal behavior.  \n\\- GPU-Z and Adrenalin control panel both read what equates to the \"VID\" of the graphics card; it's what the GPU is asking for, not what the core is receiving. Check out HWINFO64, the line titled \"GPU Core Voltage (VDDCR\\_GFX)\" for the actual core voltage reading.  \nI'd also recommend reading through both of the owners threads for RX 6900 XT's: One on Overclockers UK and Overclock dot net. Both are about 70 pages long, but have a LOT of good information regarding how our RDNA2 cards act and react to overclocking and undervolting.", "ups": 1, "depth": 0, "author": "sparkle-oops", "replies": [{"selftext": "I have undervolted both my GPU and CPU, edited the Curve optimizer, set a new thermal throttle limit. And the temps seem to be fixed now.", "ups": 1, "depth": 1, "author": "Alternative-Fly-1727", "replies": [{"selftext": "Good, have fun :-)", "ups": 2, "depth": 2, "author": "sparkle-oops", "replies": [{"selftext": "Thanks, you too :D", "ups": 1, "depth": 3, "author": "Alternative-Fly-1727", "replies": []}]}]}]}, {"selftext": "What kind of case do you have? I had similar temps to what you have until I changed my case and I switched a triple fan AIO, made a huge difference.", "ups": 1, "depth": 0, "author": "JimJava", "replies": [{"selftext": "PowerCase J188-17, has 4 fans(3 of them are kinda blocked). And I have a Ak400 zero dark plus for a CPU cooler.", "ups": 1, "depth": 1, "author": "Alternative-Fly-1727", "replies": [{"selftext": "Looks like a decent case, check the airflow, maybe the hot air is not exhausting out the back fast enough, also check if thermal paste has dried up.", "ups": 2, "depth": 2, "author": "JimJava", "replies": [{"selftext": "Nope, the paste is new on both the CPU and GPU. The processor is supposed to reach high temp. I see you have an AM4 CPU and a X3D variant which is a lot cooler than an AM5 X CPU.", "ups": 1, "depth": 3, "author": "Alternative-Fly-1727", "replies": [{"selftext": "Have you considered an AIO or do you think the improvements will only my marginal?", "ups": 2, "depth": 4, "author": "JimJava", "replies": [{"selftext": "Yes, I have considered an AIO but I know the maintenance of them and it just costs too much for a cooler. 2 times the price of the cooler I have rn.", "ups": 2, "depth": 5, "author": "Alternative-Fly-1727", "replies": []}]}]}]}, {"selftext": "Blocked fans? What do you mean? Are those fans moving air at all?", "ups": 1, "depth": 2, "author": "-JMG00-", "replies": [{"selftext": "3 fans of the case are blocked by the front panel. General case design flaw.", "ups": 1, "depth": 3, "author": "Alternative-Fly-1727", "replies": []}]}]}]}, {"selftext": "absolutely under no circumstances.. should you EVER install motherboard manufacturer's own software (bloatware)...\n\nAlways download drivers from, in this case, AMD for the chipset drivers. Honestly basically no one needs or should have ryzen master installed. and you definitely don't want netron internet security malware installed.", "ups": 2, "depth": 0, "author": "DHJudas", "replies": []}, {"selftext": "As for issue number 4. I also have a gigabyte mb, b450 aorus elite. I also have app center installed, but I only did it to download RGB fusion and disable rgb. There are other apps, like norton (don't use that crap) and other apps about cpu overclock, fan control, bios update. Trust me don't use any of it, your are better of doing those thing from bios. Don't use these 3rd party apps, this is bloatware, and it can mess something up sometimes.", "ups": 1, "depth": 0, "author": "nzmvisesta", "replies": [{"selftext": "Sure, thanks for ur opinion", "ups": 1, "depth": 1, "author": "Alternative-Fly-1727", "replies": []}]}, {"selftext": "How much 7600x cost you. This shit is expensive af i mean only their motherboards, decent ones starts at 250$?++ how can u have so much money and cheap on GPU brand. From what i read biostar is dogshit from rx500 to now. And on top of that this is AMD heater idk even what wattage is", "ups": 1, "depth": 0, "author": "Mastercry", "replies": [{"selftext": "I got a gigabyte b650 gaming x ax for ~200 euro and the cpu for exactly 250 euro. Worth every cent.\nI don't have so much money, the gpu blew my wallet up even with the cheap brand.", "ups": 1, "depth": 1, "author": "Alternative-Fly-1727", "replies": []}]}, {"selftext": "Avoid motherboard apps and control centers. They are buggy messes at best, performance hogs and vulnerabilities at worst. \n\nGo to AMD's website, get drivers for GPU and chipset from there. If you go to your mobo page, they usually have age old versions of both, avoid.", "ups": 2, "depth": 0, "author": "BigHeadTonyT", "replies": [{"selftext": "Yeah, already tried downloading the Chipset drivers and ryzen master from the official page, nothing works :/", "ups": 1, "depth": 1, "author": "Alternative-Fly-1727", "replies": [{"selftext": "[https://www.amd.com/en/support](https://www.amd.com/en/support) Chipsets, AM5, your chipset.", "ups": 1, "depth": 2, "author": "BigHeadTonyT", "replies": [{"selftext": "I'll try this tomorrow, let you know if it worked.", "ups": 1, "depth": 3, "author": "Alternative-Fly-1727", "replies": [{"selftext": "It didn't work:/", "ups": 1, "depth": 4, "author": "Alternative-Fly-1727", "replies": []}]}]}]}]}, {"selftext": "All the current CPUs get really hot but they made to support those temps.\n\nGPUs on the other hand, they got much lower temps, specially the new 40xx nvidias, im running a tuf 4090 oc and the max temp i seen was 70c and only on cyberpunk with RT maxed, most games gpu stays on low 60c.\n\nWhile playing, my 7800x3d is always 10c hoter than my 4090, im using Galahad 360 on the cpu.", "ups": 1, "depth": 0, "author": "ldontgeit", "replies": [{"selftext": "Yeah, already figured out the high CPU temps and I also fixed them using the bios.\nYou have an asus tuf and I have biostar, makes sense that I would get higher temps.\nI also re did the test in TLOU and I got 68C GPU and 85C hotspot.", "ups": 1, "depth": 1, "author": "Alternative-Fly-1727", "replies": []}]}, {"selftext": "I personally do not like Gigabyte, I only had bad experiences with them. Many people suggest that you also didn't buy the best GPU model. So this is where some of the problems come from. \n\nAlso for the first startup always give your PC some time. Training the ram for example is normal. But just in general do not get that suspicious if it just takes longer at the first startup. \n\nI do not know why it seems like 50% of people are complaining about the temperature. Modern CPUs and GPUs are supposed to use any available thermal room. They are supposed to run at 90\u00b0C. If they only do it while Boosting, then they are working correctly. You should rather check for the noise (especially GPU), high idle/low performance temp and for the performance itself. Is it getting hot because it is boosting and actually staying quiet or is it thermal throttling because it is too hot.", "ups": 1, "depth": 0, "author": "RealKillering", "replies": [{"selftext": "Well, gigabyte is still better than those CPU-killer and error packing asus mobos lol.\n\nI didn't really know about GPU brands and went with the cheapest. Didn't know about training too. \n\nI already knew about the 95C CPU temps before even buying the CPU but it's just that I don't like to have an oven in my PC. Also I don't know if updating to windows 11 has to do with it but now the GPU runs at 68C GPU temp and 85C hotspot under 100% load so it's all good now. And if you're asking about the Idle temps well, it's about 33(lowest) and 48(highest), on average about 39C. I can hear it because my headset doesn't isolate sound but it's not that bad.", "ups": 1, "depth": 1, "author": "Alternative-Fly-1727", "replies": [{"selftext": "It is not that easy to know about all the brands. I just wanted to say that I made the same mistake with gigabyte.\n\nActually the temperature does not really change the thermal output. Tdp of 60 Watts (if it is 60 Watts in reality) will stay at 60 Watts. The temperature gets determined by the cooling. So you can heat your room more with a 60 \u00b0C CPU than with a 90 \u00b0C CPU. A lot of people forget that. I once read that someone turned the fan up to cool down the CPU, so that the room will not get that hot. But this will just mean that the temperature gets transferred faster to the room, it does not reduce the heat output.\n\nBasically low temperature only matters if you try to get more performance, meaning you want it to boost more. \n\nDid you build a PC before? Maybe it didn't take that long the other times, but ram training always happened. It just can take longer if you have more ram and I think the new DDR5 ram also takes longer than old DDR3 ram.", "ups": 1, "depth": 2, "author": "RealKillering", "replies": [{"selftext": "Yeah my first pc was with a dual core cpu and a Nvidia Quadro. My second one was with a r9 280x and a ryzen 7 2700, I changed the GPU later on with a rx 5600 xt and the cpu with a i5-10400f and now my current one.", "ups": 1, "depth": 3, "author": "Alternative-Fly-1727", "replies": []}]}]}]}, {"selftext": "Gpu hot spot at those temps are abnormal. you may need to replace the thermal paste with a very good one but the most important is actually to replace or thickenned each of thermal pads, you might get way with probably just re tightenning the screws top to bottom (gpu fans to gpu board). Solved this issue on my old 5700 XT. Dropped from 105c to 75c.", "ups": 1, "depth": 0, "author": "NZT23", "replies": []}]}
{"post": {"title": "7900XT and PTM7950", "subreddit": "Amd", "selftext": "I had been having hotspot issues with this card for some time now. After trying different mounting pressures and various repastes, nothing had been improving my hotspot temperature.\n\n Granted I was using Corsair\u2019s TM30. I don\u2019t know how bad or good that paste is, but I certainly know it isn\u2019t discussed as one of the best. After being fed up with what I can only describe as pump out, I opted to try PTM7950. I purchased it off of Amazon, so who knows if it is the authentic stuff. But, let me tell you. I am running my card OC\u2019d with 15% PL increase.  And I am lower on the hotspot than when I undervolted on paste. \n\nI had to undervolt previously and it would still heat up horribly. I forgot to mention that I had a horrid delta between GPU temp and Hotspot temp of near 40 degrees! \n\nI just finished applying it, it was a bit of a pain trying to separate the film from the pad. But nothing too horrible. This stuff is incredible! I am playing Red Dead Redemption 2 with the OC, just a few days ago my Hotspot was spiraling into 105C. That was the moment that made me realize I had to do something. \n\nMy current delta is down to 16~20ish degrees C. They say this stuff can only get better over the next couple of hours. So it should improve if even by a minor couple of degrees further. \n\nIf anyone is dealing with hotspot issues and has repasted to no avail. Look into PTM7950. It has fixed my issues. \n\n/Rantover\n\nEdit: My model is an XFX Speedster Merc 310.", "ups": 28, "permalink": "/r/Amd/comments/16kgz7o/7900xt_and_ptm7950/", "num_comments": 58}, "replies": [{"selftext": "I just used the TM30 that a customer provided with a new mobo. He seemed to be very into Corsair...case, psu, extra fans, RAM, and the thermal paste were all Corsair. I've got to say I was not impressed with the TM30. Paste needs to stick to the processor and heatsink and the TM30 just doesn't. It has something like an oily coating that lets the paste just slide around on the surface. Don't recommend.", "ups": 8, "depth": 0, "author": "Hippie_Tech", "replies": [{"selftext": "Yeah, I picked it up at Best Buy just to have in emergency. Then I realized I had issues with the Hotspot, and it helped but not to the degree necessary, plus it pumped out of the GPU.", "ups": 2, "depth": 1, "author": "davethepiloto", "replies": []}, {"selftext": "I have tested a lot of pastes over the years. TM30 is one of the worse performing pastes I've tested. Near the bottom of the list.\n\nYou are just as well off buying bulk tubs of cheap Chinese TIM you can find on aliexpress for 1/4 the cost of a syringe of TM30, because it performs exactly like that stuff.", "ups": 2, "depth": 1, "author": "Rockstonicko", "replies": [{"selftext": "I've \"received\" multiple single-use syringes of thermal paste very similar to TM30 and none of them worked well. That non-sticky paste is just useless, in my opinion. I've used StarTech (really cheap) paste that works better than TM30 and others like it.", "ups": 2, "depth": 2, "author": "Hippie_Tech", "replies": []}]}]}, {"selftext": "AMD and custom AIB should be using this on stock like NVIDIA does with 4090 especially on next gen cards if they do not adress flatness of the cooler and gpu chip, cos if its not flat it contribute to faster pumpout, especially at really high wattage.\n\nLeast they should use some other phasechanging thermal material or pad that cannot be pumped out.\n\neven if its 2c higher then regular paste, after pumpout that 2c lower temp will be 15-20c higher temp compared to the thermal interface material that does not pumpout but runs 2c higher then a fresh aplication for example.\n\nanyway if they use PTM7950 nothing will beat it, any repaste attempt will be worse always unless same stuff is used.", "ups": 6, "depth": 0, "author": "Melodias3", "replies": [{"selftext": "I agree, no reason to not use this type of stuff on their enthusiast level products.", "ups": 3, "depth": 1, "author": "davethepiloto", "replies": []}]}, {"selftext": "Okay after seeing this I'm convinced to buy it. I also have the exact same card and facing hotspot temp issues. I'm going to order it from Aliexpress, hope I get a genuine one.", "ups": 3, "depth": 0, "author": "VinumNoctua", "replies": [{"selftext": "You will not regret it! I hated the idea of gaming and constantly having to watch my Hotspot temp! Like wtf? Why do I have to be concerned about that when I\u2019m trying to have fun", "ups": 2, "depth": 1, "author": "davethepiloto", "replies": [{"selftext": "The only downside for me is that my warranty will be gone when I disassemble the GPU. I'll have to ask XFX to confirm it though. Are you living in NA region?", "ups": 2, "depth": 2, "author": "VinumNoctua", "replies": [{"selftext": "I live in the US, the warranty stickers are meaningless, they cannot refuse warranty here for that. However, I bought the GPU secondhand which XFX will not honor warranty since I am not the original purchaser. Even though the product is less than a year old! Crazy! But, I won\u2019t be buying XFX in the future that is for sure. I loved that about EVGA, transferrable warranty.", "ups": 6, "depth": 3, "author": "davethepiloto", "replies": [{"selftext": "Asrock is still denying warranty for this.  I'm in the middle of trying to get them to rma a bad 6950xt. They still think they can get away with voiding a warranty over a sticker. Hotspot temp was high, and the card kept crashing, so i installed new thermal paste. Fixed the temperature issue, but it still crashes under load.  Asrock is currently refusing to warranty it even after I brought up the FTC rules and the magnuson moss warranty act.", "ups": 3, "depth": 4, "author": "Ferox63", "replies": [{"selftext": "If the card crashes under load...it was bad in the first place", "ups": 3, "depth": 5, "author": "YukiSnoww", "replies": []}, {"selftext": "That is rough, I guess I will avoid them in the future as well.", "ups": 3, "depth": 5, "author": "davethepiloto", "replies": []}, {"selftext": "If it was working before you disassembled it (minus paste &amp; temp issues), then there's a good chance it's still working and you've got mounting pressure issues.\n\nI've disassembled &amp; reassembled / messed about with different waterblocks then going back to stock cooler to sell etc., and the worst experience I ever had resulted in 20+ attempts until I got the mounting pressure right (symptoms prior to getting it right, was crashes under heavy load also).\n\nI even went out and bought a new GPU as I was worried I'd actually broken it. Getting everything as tight as you can before backing everything off by 2 or 3 rotations then tightening in a 'x' pattern a quarter turn at a time is the most reliable technique I've found so far. \n\nCould be worth a go.", "ups": 2, "depth": 5, "author": "BigGirthyBob", "replies": []}]}]}, {"selftext": "I purchased XFX warranty stickers on Aliexpress. It is identical to the original. They will never know you opened the GPU (Well, just repaste with \"normal\" paste if need send GPU for RMA).", "ups": 1, "depth": 3, "author": "Lmpk13", "replies": []}]}]}, {"selftext": "Acording to LTT between aliexpress and ebuy7 one the ebuy7 one performed better but the one from moddiy is safest option as its most legit however more expensive.", "ups": 1, "depth": 1, "author": "Melodias3", "replies": [{"selftext": "Yeah I saw that video but it would be **very** expensive (mostly shipping) for me to buy from them. I saw Aliexpress and Moddiy has 1 or 2 degrees of difference after 5 minutes so I think it's not worth for me to buy from there.", "ups": 1, "depth": 2, "author": "VinumNoctua", "replies": []}]}]}, {"selftext": "Another happy customer.\nWelcome to the club!", "ups": 3, "depth": 0, "author": "Electrical-Bobcat435", "replies": [{"selftext": "Yep, amazing stuff!", "ups": 2, "depth": 1, "author": "davethepiloto", "replies": []}]}, {"selftext": "I have the Sapphire Pulse 7900 XT but what size did you have to cut it for your GPU? I\u2019m looking at the 40x40mm..", "ups": 3, "depth": 0, "author": "deafjamman", "replies": [{"selftext": "Yeah so I purchased the 40x80 and it was more than enough. I used less than 30% of it. That is assuming you don't rip or mess up somehow and have to start over. I had no issues so I did not need to use more material than enough for one square over the die. 40x40mm should be fine assuming you don't mess anything up!", "ups": 2, "depth": 1, "author": "davethepiloto", "replies": []}]}, {"selftext": "Where the hell do yall buy this at? I can never find it!", "ups": 3, "depth": 0, "author": "DicksMcgee02", "replies": [{"selftext": "I got it off of Amazon, just one of the top listings.", "ups": 2, "depth": 1, "author": "davethepiloto", "replies": []}]}, {"selftext": "I did the same thing to my 7900XTX with the same result. It went from 105C hotspot down to hovering in 80s most of the time, maybe creeping into the 90s at the worst. That's with an aggressive fancurve and undervolt thrown in.\n\nThis stuff (the pad version at least) is fucking amazing. Definitely recommend.", "ups": 3, "depth": 0, "author": "The_American_Viking", "replies": [{"selftext": "Yeah, as I ran through my game more the temperature kept going down. Last I saw I was around high 60s, low to mid 70s at max. Perhaps it gets even better later in the week.", "ups": 2, "depth": 1, "author": "davethepiloto", "replies": []}]}, {"selftext": "You used it in paste form or thermal pad form?\n\nIf a thermal pad, what thickness?", "ups": 2, "depth": 0, "author": "PotentialAstronaut39", "replies": [{"selftext": "I was not aware they had a paste form. I used the thermal pad form that melts into a paste after it is exposed to the temps of the GPU. As far as thickness I believe they list it as .02mm on Amazon. I picked up a 40x80mm they list it as 40x80x.02mm.\n\n\nForgot to add. This size was more than enough with more than say 70/75% left over for future uses. I didn\u2019t have any issues some people tear the pad and start over. So it might not be the case for all to have as much left over.", "ups": 3, "depth": 1, "author": "davethepiloto", "replies": [{"selftext": "Did you buy it from the JOYJOM seller on Amazon? I've got some coming in tomorrow from them for my RX Vega 64 and RTX 3080. I'm hoping it yields similar gains.", "ups": 3, "depth": 2, "author": "psnipes773", "replies": [{"selftext": "I think it may have been them! All I can say is, I was more than impressed with the product. I dropped near 30C on my hotspot!", "ups": 3, "depth": 3, "author": "davethepiloto", "replies": []}]}]}, {"selftext": "There is a paste but i dont know of anyone that used it.  Practically everyone using the pad form, it is 0.2mm thick i believe.", "ups": 2, "depth": 1, "author": "Electrical-Bobcat435", "replies": []}]}, {"selftext": "Ordered some ptm7950 and thermal putty for my taichi 7900xtx. It reached 107c running kingdom hearts 3 with 200 resolution scale at 1440p. In regular gaming hotspot stays around 70-80 and the GPU core 50-60. I have a decent case albeit I am using subpar airflow fans. If I swap them out I'll be able to significantly knock it down maybe 1 or 2c. My ambient room temp during the day is 29-31c. That 107c value could realistically be interpreted lower when the summer is over so like high 90s low 100s. I made a similar post asking about ptm and it convinced me to pull the trigger.", "ups": 2, "depth": 0, "author": "slicky13", "replies": [{"selftext": "Yeah that is the reason I posted, I had been wanting to pull the trigger too.  I finally made the jump and I was just too impressed not to share. I wanted the information out there for people going through the same thing. Goodluck!", "ups": 5, "depth": 1, "author": "davethepiloto", "replies": [{"selftext": "I haven't gotten it yet, I was only going to get the ptm7950 but I was worried about thermal pads ripping and sizes for replacing. Another user posted about thermal putty and was skeptical. Nvidia fe 4090 and AMD 7900 xtx reference cards both use thermal putty for VRAM so I pulled the trigger on both. I'll post my findings as well when I replace them. Just hoping I don't strip any screws as they're easily tiny enough to strip.", "ups": 3, "depth": 2, "author": "slicky13", "replies": [{"selftext": "Ah I see, yeah I had Pads on the VRAM, I believe one part ripped but overall the pads remained in tact. And the ripped one hasn\u2019t posed any problems. But yeah it is best to side on the err of caution!", "ups": 2, "depth": 3, "author": "davethepiloto", "replies": [{"selftext": "You're fine. As long as the parts that make contact are good it doesn't matter as you can splice them. I also got putty as it allows for a better squish and contact with the cooler. GN mentioned there's benefits to having the putty squish as it reaches putty overfills in the spaces between the VRAM chips.", "ups": 2, "depth": 4, "author": "slicky13", "replies": [{"selftext": "Yeah i saw that as well! A putty would be nice down the line on my card, for now though everything seems fine", "ups": 2, "depth": 5, "author": "davethepiloto", "replies": []}]}]}]}]}]}, {"selftext": "I did the Kryosheet on my Merc 310 when I took it apart to do the 550W AsRock BIOS flash. My deltas seem about the same at 465W power, at around 21C, but it goes up to about 30C delta when 500W+ is going through it. I'm happy with it though, as I'll never have to worry about pump out again.", "ups": 2, "depth": 0, "author": "sawthegap42", "replies": [{"selftext": "Yeah that is awesome, and damn how does the card do at 550W? Also what kind of PSU do u have?", "ups": 2, "depth": 1, "author": "davethepiloto", "replies": [{"selftext": "I have some pretty good cooling, so it does pretty well, and can boost pretty consistent over 3Ghz+, but doesn't much like being past 3.2Ghz. After playing around with the new power limits, I've decided it's not really worth it for gaming, as it takes an extra 100W+ to only get about 3-5 more FPS past the 465W power limit settings I have, but it does allow for faster and more consistent game clocks around 2.9Ghz. I am running an EVGA G6 850W PSU, and transient spike of over 700W, it is certainly the lowest power PSU i would recommend with this setup, as it is just enough to keep up with the 5800X3D and 7900 XTX.", "ups": 2, "depth": 2, "author": "sawthegap42", "replies": [{"selftext": "Dang, thanks for the detailed write up! And sheesh that PSU is doing the best it can to hold it all together. I\u2019m impressed. And 3.2ghz is pretty sick! But yeah 100W+ for 3-5fps would not be worth it long term for me either. Thanks again!", "ups": 1, "depth": 3, "author": "davethepiloto", "replies": []}]}]}]}, {"selftext": "I think the only place confirmed to sell the authentic stuff is from moddiy.com", "ups": 2, "depth": 0, "author": "throwawayerectpenis", "replies": [{"selftext": "That is correct, but the aftermarket stuff seems to be good enough as well.", "ups": 3, "depth": 1, "author": "davethepiloto", "replies": []}]}, {"selftext": "I just repasted yesterday with the Kryonaut, I spread it on the whole chip thinly with the spreader and put ''a sausage'' on the middle for the good measure. The hotspot delta is now in the 15-20ish range as well, instead of 30s. Fans are much quieter as well. I've ordered PTM from Aliexpress as well, though, because I'm expecting it to pump out eventually. But I'm glad I did it to know how easy the card was to tear down. It was probably both the paste issue and mounting pressure issue on my card.", "ups": 2, "depth": 0, "author": "ALph4CRO", "replies": [{"selftext": "I\u2019ve heard good things regarding Kryonaut, and I believe you will get similar results with PTM.", "ups": 2, "depth": 1, "author": "davethepiloto", "replies": [{"selftext": "Yep, but the question is, for how long?\n\nPTM shouldn't have that issue. But as long as the temps are fine I'll keep the Kryonaut on.", "ups": 1, "depth": 2, "author": "ALph4CRO", "replies": [{"selftext": "Yeah, that is the thing it could be a few months? I hear that it isn\u2019t prone to pump out. But then again with PTM you really don\u2019t have to worry at all.", "ups": 2, "depth": 3, "author": "davethepiloto", "replies": []}]}]}, {"selftext": "Could you please tell me which seller you bought the PTM from on Aliexpress?", "ups": 1, "depth": 1, "author": "VinumNoctua", "replies": [{"selftext": "''RGeek Official Store''\n\nI just ordered it few days ago. Got 100x160x0.2. Reviews seem good.", "ups": 1, "depth": 2, "author": "ALph4CRO", "replies": []}]}]}, {"selftext": "My red devil 6900xt had a horrid delta to, normal ~72 hotspot 110.\n\nAfter I applied ptm beginning this week my hot spot is max 88 now with the normal still at 72. Much better. Had a case of horrible pumpout.", "ups": 2, "depth": 0, "author": "Acreddo", "replies": [{"selftext": "Yeah, that is rough. Good thing you got that delta down!", "ups": 1, "depth": 1, "author": "davethepiloto", "replies": []}]}, {"selftext": "Is this being used exclusively on the core? or are you replacing all thermal pads for memory etc as well?", "ups": 2, "depth": 0, "author": "ryzeki", "replies": [{"selftext": "Just on the GPU die. Other pads are stock.", "ups": 2, "depth": 1, "author": "davethepiloto", "replies": [{"selftext": "Thanks. I think i will try it out", "ups": 1, "depth": 2, "author": "ryzeki", "replies": []}]}]}, {"selftext": "Just used some PTM7950 on 5700XT XFX triple dissipation card. Ordered from Amazon 40x80 pad kit for Steam deck. Plenty left over to do many cards. Had pump out problems after 3 months with kryonaut. Hot spot delta now 16\\*C with PTM7950, acceptable as long as it can resist pump out.", "ups": 2, "depth": 0, "author": "EarlyClick420", "replies": [{"selftext": "I believe it should not pump out from what reviews state. But then again YMMV, for all of us.", "ups": 1, "depth": 1, "author": "davethepiloto", "replies": []}]}, {"selftext": "Or... don't buy AMD reference cards until they fix this issue. It's such a well known and common problem with using their \"not PTM\" pad solution. Their pads kind of suck, and their cooler design doesn't allow you to use paste unless you offset with spacers, which you run the slight risk of cracking your die and killing the card that way.", "ups": -7, "depth": 0, "author": "Reddituser19991004", "replies": [{"selftext": "I should have specified. I\u2019m using an XFX Speedster Merc 310. But, yeah I understand your stance.", "ups": 5, "depth": 1, "author": "davethepiloto", "replies": [{"selftext": "I wasn't aware Xfx cards had this issue honestly. I know other cards use pads just never heard of it being an issue on cards that aren't amd reference designs. Good to know.", "ups": -7, "depth": 2, "author": "Reddituser19991004", "replies": []}]}]}]}
{"post": {"title": "What Adrenaline settings do you use?", "subreddit": "Amd", "selftext": "What are some of the AMD app settings that you use for almost every game? For example, I use radeon Vsync and radeon chill. \n\nAlso, which one do you think is the most useful/effective for games in general?", "ups": 5, "permalink": "/r/Amd/comments/16k4hxh/what_adrenaline_settings_do_you_use/", "num_comments": 11}, "replies": [{"selftext": "RIS at 5%", "ups": 2, "depth": 0, "author": "dmaare", "replies": []}, {"selftext": "All at stock.", "ups": 2, "depth": 0, "author": "blueangel1953", "replies": [{"selftext": "I have a 7900xtx, and I tried all the fancy settings. They either make quality worse, lower framerates, introduce micro-stutters, or cause frequent crashes. The best experience for me has been with no extras.", "ups": 1, "depth": 1, "author": "robato", "replies": []}]}, {"selftext": "Sharpness at 60%.\n\nFreesync.\n\nI've been messing with the colours for the individual games recently... When gaming press alt+z, then adjust game graphics, down under display is vivid gaming or dynamic contrast as Starfield looked awfully bland at times.", "ups": 1, "depth": 0, "author": "zeus1911", "replies": []}, {"selftext": "Just normal Vsync when needed.", "ups": 1, "depth": 0, "author": "Limi_23", "replies": [{"selftext": "why not the enhanced sync or whatever that doesn't limit your framerate?", "ups": 1, "depth": 1, "author": "Goldenflame89", "replies": [{"selftext": "Fixed frametime is smoother.\n\nUncaped fps has up and downs in frametime it doesn't feel consistent and smooth. I usually try all combinations of features to see which one is smoother and low input lag.", "ups": 1, "depth": 2, "author": "Limi_23", "replies": []}, {"selftext": "enhanced sync is really a near placebo for e-sports players, choppy frametimes make everything unpleasant, and it is counterproductive with adaptive sync.", "ups": 1, "depth": 2, "author": "LongFluffyDragon", "replies": []}]}]}, {"selftext": "I use Settings on a per game basis.  Sharpening if native 20-30% is usually good, if using fsr or whatever 50%. of course freesync and chill at -3 my refresh rate usually, some games dont like this.  dont use enhanced sync or vsync usually but again depends on game.  144hz 1440p monitor here.  Also turn off any sharpening in game if using RIS, dont mix.", "ups": 1, "depth": 0, "author": "Athrob", "replies": []}, {"selftext": "Radeon Chill. Cool, quite and smooth.", "ups": 1, "depth": 0, "author": "EzerchE", "replies": []}, {"selftext": "Image Sharpening if a game needs it. Color correction in global display to simulate sRGB mode since my monitor has a bad implementation.", "ups": 1, "depth": 0, "author": "WAR-Floross", "replies": []}]}
{"post": {"title": "Ryzen 3700 and fixed voltage and frequency", "subreddit": "Amd", "selftext": "Hi,\n\nI have a Ryzen 3700X CPU and Prime B550-Plus Motherboard. I use the stock cooler so the cooling may not seem to be very good.\n\nWhen I set the frequency at 40.00 the temperature gets to above 75 degrees however when I set the voltage at 1.15 and the frequency at 40.00 at the same time, the temperature gets to 70 degrees.\n\nSo I need to know if I can let the CPU to run at 4000Mhz and 1.15V 24/7? Are there any downsides and isn't it harmful?\n\nThanks in advance.", "ups": 5, "permalink": "/r/Amd/comments/16k47re/ryzen_3700_and_fixed_voltage_and_frequency/", "num_comments": 16}, "replies": [{"selftext": "I used to run my 3900x at 1.25vc + 4370,4350,4300,4275, never had problems with it. Your cooler is holding you back a lot", "ups": 5, "depth": 0, "author": "Annual-Error-7039", "replies": [{"selftext": "Oh really? What type of coolers should I use?", "ups": 1, "depth": 1, "author": "mohsenz2006", "replies": [{"selftext": "[https://www.amazon.co.uk/dp/B0B4G8QKZP/ref=twister\\_B0CC271Y25?\\_encoding=UTF8&amp;th=1](https://www.amazon.co.uk/dp/B0B4G8QKZP/ref=twister_B0CC271Y25?_encoding=UTF8&amp;th=1)\n\nfor example, will be excellent compared to the stock cooler and even works on am5 as well.", "ups": 1, "depth": 2, "author": "Annual-Error-7039", "replies": []}]}]}, {"selftext": "Up to 1.25 below 80c there is zero risk for the 3000 series", "ups": 2, "depth": 0, "author": "thelebuis", "replies": []}, {"selftext": "I ran a 3600 at 4.5 @ 1.35v for over a year.\n\nYou're not even close to harmful.", "ups": 2, "depth": 0, "author": "cellardoorstuck", "replies": []}, {"selftext": "No, you are 100% safe, but it's better if you set higher pbo limits and try a negative offset on the cpu vcore. Most motherboards allow you to adjust vcore offset.", "ups": 1, "depth": 0, "author": "kolliasl21", "replies": [{"selftext": "The 3000 series is the last gen where all core oc is still viable you can get better results than with pbo especially if you dont mind to be only cinebench stable", "ups": 2, "depth": 1, "author": "thelebuis", "replies": []}, {"selftext": "Could you please explain to me how to set a negative offset?", "ups": 1, "depth": 1, "author": "mohsenz2006", "replies": []}]}, {"selftext": "Yes, that's safe. I'd strongly recommend using PBO, though. Your CPU will be able to boost higher when there is power and thermal headroom. The default PBO limits should work with the stock cooler the CPU comes with.", "ups": 1, "depth": 0, "author": "Demy1234", "replies": []}, {"selftext": "I have 2000 series APU who run years with fixed 1.375 voltage with 3700 clock(loose silicon lottery)", "ups": 1, "depth": 0, "author": "aRx4ErZYc6ut35", "replies": []}, {"selftext": "I have the same cpu, people gave me flack about being a beginner and installing an open loop setup into my first pc I made a month ago, idle it gets to around 42\u00b0C usually in the high 30s tho. Gaming wise never seen it past the 60s. Idk if that's good or bad", "ups": 1, "depth": 0, "author": "Aurey2244", "replies": [{"selftext": "Why would you do an open loop with such extremely low end hardware?\n\nI'd give you flack for buying low end hardware and using an open loop. Doesn't make sense to have a loop unless you're doing at least a 7950x or 13900k today.... otherwise the cost of the loop is better spent elsewhere.", "ups": 1, "depth": 1, "author": "Reddituser19991004", "replies": [{"selftext": "Came free with the gpu , got an full open loop kit with the gpu for 250 plus I was new, I just did what others recommended. I can make my own decisions now", "ups": 1, "depth": 2, "author": "Aurey2244", "replies": [{"selftext": "umm what? Used I assume?", "ups": 1, "depth": 3, "author": "Reddituser19991004", "replies": [{"selftext": "Loops and gpu yes, pump no", "ups": 1, "depth": 4, "author": "Aurey2244", "replies": []}]}]}]}]}, {"selftext": "Really depends which batch of zen 2 you got. post 2020 batch are on a very mature 7nm node by 2020, thus those can be heavily undervalued. This is because the default voltage was designed for launch batch in early 2019.", "ups": 1, "depth": 0, "author": "minhquan3105", "replies": []}]}
{"post": {"title": "What about the old \"Driver issues\" thing?", "subreddit": "Amd", "selftext": "I switched from a 3080 to a 7900xtx recently and didnt really have any issues. Now most people I know are Nvidia hardcores and theyve consistently told me, not to get a radeon bc of \"bad drivers\" or \"very noticable stuttering and latency\" and of course the old \"dlss good, fsr bad\". Not only did people tell me that but I also saw it on the internet very often. However Ive had the 7900xtx for about 2 weeks now, using it almost every day and I didnt have any of the above issues, infact I feel like they were there on the 3080 but are gone now. The performance was stable, fsr worked surprisingly well and the interfaces were pretty user friendly even for an ex nvidia user. So now I ask, are stuttering and driver issues a problem on lower end amd gpus or are those just dead arguments from nvidia fanboys that are stuck in 2018?\n\nEdit: might be important to add that I built a completely new rig so I dont know how it plays out when you still have traces of nvidia drivers left.\n\nEdit too: yes I do also have an amd cpu and am using sam, dunno if that has a huge impact.", "ups": 189, "permalink": "/r/Amd/comments/16j8aj0/what_about_the_old_driver_issues_thing/", "num_comments": 393}, "replies": [{"selftext": "Driver issue can be a hit or miss thing. Hardware Unboxed recently reviewed the RX 7800 XT and faced game crashing issue, but these were not reported by other reviewers.", "ups": 186, "depth": 0, "author": "skylinestar1986", "replies": [{"selftext": "That's the problem with driver issues - if they're not consistent, or something fringe, the user is very upset but others will almost gaslight them saying they're full of beans.", "ups": 100, "depth": 1, "author": "ExtendedDeadline", "replies": [{"selftext": "I've had 1 more bad card with AMD than I've ever had with Nvidia, and that means I've had 1 bad card. The first 7900 XTX I got had issues that seemed driver related but disappeared entirely when I reduced the core clock, so the card was just bad. I suggest that anyone experiencing what they think are driver issues tries this simple test to see if they can pin it on a bad card instead and start an RMA.", "ups": 21, "depth": 2, "author": "alman12345", "replies": [{"selftext": "I never had bad cards with either AMD or Nvidia.\n\nWell, I never had an Nvidia card, so I can't really speak about their or their drivers' quality.  ;-)", "ups": 3, "depth": 3, "author": "georgehank2nd", "replies": []}]}, {"selftext": "LTT I think also had those problems.", "ups": 9, "depth": 2, "author": "Low_Doubt_3556", "replies": []}, {"selftext": "This is the sad truth. I mean, you can cite all the driver issues the RDNA3 series launched with and you'll still have people saying there are no issues.\n\nRemember folks it took about 6 months for VR to work on your shiny $900-1000 GPU. \"But Railven, I don't play VR\" good for you, say that to the people that DID buy to improve their VR experience.\n\nHow about the multi-monitor power consumption that is still in the \"to resolve\" portion of notes. \"But Railven, that affects NV and Intel\" sure does, just not to such severity where pretty much all reviewers show the discrepancy and lots of users are resorting to voodoo tricks and sacrifices to reduce the number.\n\nIn the end, you can list issues you experience, link sources, link articles, link AMD themselves saying they acknowledge the issue and then their attempts to resolve it - but naaaah, everything's fine!\n\nEDIT: I mean my lord they disabled their own code in their own driver suite because it can potentially brick your Windows Install. COME ON!!!!! I don't need people to say \"AMD drivers are garbage\" but can we stop with the \"OMG I was told AMD drivers will light my house on fire and kill my dog, but so far, nothing bad has happened.\"\n\nAll I'll say is \"you're still in the honey moon phase, give it time.\" And I don't mean this as a negative, just people have been stating issues because they exist.", "ups": 24, "depth": 2, "author": "railven", "replies": [{"selftext": "Exactly, I remember really damn well all the driver problems on RDNA3 the whole last year, and a lot of them are still problems, VR performance is still subpar, factory reset is still disabled and I still have my gpu idling at 180W on my dual monitor setup, these people just pretend that release notes are empty or some shit like that lol", "ups": 12, "depth": 3, "author": "HappyColt90", "replies": []}, {"selftext": "Idk what junked card you're using but VR for Radeon was fixed ages ago. Runs flawlessly now.", "ups": 5, "depth": 3, "author": "IrrelevantLeprechaun", "replies": []}, {"selftext": "VR worked fine, it was just closer to 6900xt performance.  Still very useable.  People are dumb and over state things.\n\nNvidia has issues too.", "ups": 0, "depth": 3, "author": "XXNameAlreadyTakenXX", "replies": [{"selftext": "&gt;it was just closer to 6900xt\n\nWhich is awful since you paid for a 7900XTX, not a 6900XT lol", "ups": 14, "depth": 4, "author": "HappyColt90", "replies": [{"selftext": "Agreed, but VR still works fine.  I have a 6900xt.  It runs VR at max.", "ups": -2, "depth": 5, "author": "XXNameAlreadyTakenXX", "replies": [{"selftext": "If you have some old 2k VR headset, sure. Anything newer the 6900xt definitely does not run at max.", "ups": 3, "depth": 6, "author": "jay9e", "replies": [{"selftext": "I have a 6900xt and a 5800x3d.  HP reverb G2.  Runs everything max.  My point was that the 7900s ran vr fine, they just weren\u2019t running at their potential.", "ups": 2, "depth": 7, "author": "XXNameAlreadyTakenXX", "replies": [{"selftext": "Which is a fair counterargument against the card. I paid for the full card, why should I have to wait for a n update to fully use it?", "ups": 5, "depth": 8, "author": "LightChaos74", "replies": [{"selftext": "I agree, just stating that they still ran vr just fine.", "ups": 2, "depth": 9, "author": "XXNameAlreadyTakenXX", "replies": []}]}, {"selftext": "My 3080 for 350 cash also runs fine same performance as your 1000 dollar 7900. If you play anything other then beatsaber like dcs then you will be lucky to get 60fps at low.", "ups": 1, "depth": 8, "author": "SwiftVegeance", "replies": [{"selftext": "6900xt is comparable to 3090.  And yes, there are some use cases where better gpu\u2019s are needed.", "ups": 2, "depth": 9, "author": "XXNameAlreadyTakenXX", "replies": []}]}]}]}]}, {"selftext": "Does that matter? Playable is playable.", "ups": -2, "depth": 5, "author": "IrrelevantLeprechaun", "replies": [{"selftext": "Playable is acceptable on a 200 usd entry level gpu, not a high end 1000 bucks one, if u are okay paying for underperforming hardware idk what to tell you", "ups": 10, "depth": 6, "author": "HappyColt90", "replies": [{"selftext": "Idk what to tell you. VR works flawlessly for me.", "ups": -4, "depth": 7, "author": "IrrelevantLeprechaun", "replies": [{"selftext": "You are hella good customer", "ups": 4, "depth": 8, "author": "hegysk", "replies": [{"selftext": "You are a hella good Nvidia customer. No issues for me on my 7900xtx. Upgraded from a 1070 tho so was always in for a big boost in performance. Nvidia fans are struggling with AMDs rise in popularity", "ups": 3, "depth": 9, "author": "WallySymons", "replies": []}]}]}]}]}]}, {"selftext": "I had literal 10-20 second freezes randomly when playing HL Alyx, and after absolutely 0 changes besides the driver update it ran flawlessly.\n\nVR didn't work fine, even when it didn't have long freezes, the frame time graph was completely atrocious.", "ups": 4, "depth": 4, "author": "ST-Fish", "replies": []}]}]}]}, {"selftext": "Do you remember what games? I want to say that the game that kept crashing was F1 2023, but I don't remember for sure. [Level1Techs](https://youtu.be/AdWv3GwS-kY&amp;t=810) also experienced bugs in it, though theirs wasn't as severe, and they explain at the \"F1\" chapter (14:12 in the video) that this was a bug with a game update, not the GPU; they were able to replicate the issue on a follow-up test on a 4080. (The game updated between testing their other cards and testing the 7800 XT, hence why the issue didn't present itself on said other cards.)", "ups": 20, "depth": 1, "author": "mateoboudoir", "replies": [{"selftext": "Yh the F1 game is just a hot mess recently.  Nothing at all to do with drivers", "ups": 13, "depth": 2, "author": "ATrayYou", "replies": []}, {"selftext": "Check the latest video I think I remember him saying Spiderman", "ups": 7, "depth": 2, "author": "griber171", "replies": [{"selftext": "Ah yes, that's where it was, thanks. I was looking through the review video, that's why I couldn't find it.\n\n[Timestamped.](https://youtu.be/J0jVvS6DtLE&amp;t=925) The two games were F1 2023 (crashing) and Cyberpunk 2077 (instability). So the first can be at least partially chalked up to a game update, and the second HUB suspect to be a driver issue.", "ups": 9, "depth": 3, "author": "mateoboudoir", "replies": [{"selftext": "&gt;two games were F1 2023 (crashing) and Cyberpunk 2077 (instability). So the first can be at least partially chalked up to a game update\n\nI can play it absolutely fine and have been since I got my 7800xt last week", "ups": 1, "depth": 4, "author": "danny12beje", "replies": []}]}]}, {"selftext": "Talk about a testing faux pas. You disable the internet during testing.", "ups": 2, "depth": 2, "author": "mattumanu", "replies": []}, {"selftext": "my friend on 6800xt keeps complaining about mw2 stuttering, so i think at this point it's like a per game basis and not a general problem as it was before", "ups": 0, "depth": 2, "author": "Rapogi", "replies": [{"selftext": "If he's complaining on a 6800 XT he would be complaining more on a 3080. MW2/WZ2 has been like since release and it actually performs better on AMD. Setting a frame cap in game might help a bit though.", "ups": 6, "depth": 3, "author": "SeventyTimes_7", "replies": [{"selftext": "My friend was complaining about stuttering in Warzone then i noticed he had Vsync on and he turned it off and was like WOW ITS SMOOTH NOW.", "ups": 6, "depth": 4, "author": "Prefix-NA", "replies": []}]}]}]}, {"selftext": "Ya, i agree, i think it's just luck of the draw. I've only ever had a 6600xt and 7900xtx. My 6600xt had zero issues at all unless I undervolted too far, but my new 7900xtx has been having entirely random driver timeouts and bsod. I've tried ddu, fresh Windows 10 installs, fresh Windows 11 installs, default ram tunings, docp ram tunings, different ram, and a bunch of other stuff, but still I get seemingly random driver timeouts. Sometimes after 10 minutes, sometimes after several hours.", "ups": 3, "depth": 1, "author": "CMDRTragicAllPro", "replies": []}, {"selftext": "Yeah, I got an xtx this year, and went months without issues. Then suddenly bg3 bricked my drivers, repeatedly, after a weekend of no issues. Then a few weeks later Starfield did the same.\n\nSeems to be fixed for both games now though.", "ups": 2, "depth": 1, "author": "Eloni", "replies": []}, {"selftext": "Those aren't explicitly driver issues, in my case it was a bad bin on a 7900 XTX that didn't run stable at stock clocks/voltages. I would suggest anyone having game crashes to test with a core clock 200-300MHz lower than stock game clock and see if they still experience the crashes, if they do then they should RMA.", "ups": 2, "depth": 1, "author": "alman12345", "replies": []}, {"selftext": "hehe, thats business' as usual on new launches and games on all cards and platforms", "ups": 1, "depth": 1, "author": "nwgat", "replies": []}]}, {"selftext": "If you are happy with it that's most important.", "ups": 34, "depth": 0, "author": "Amstradcpc664", "replies": []}, {"selftext": "Yeah I don't get it. I switched from green to red (3060ti to a 7900xt) and I must say it's pretty solid. Haven't run into any game crashes or anything. Streaming is good too. I've taken full advantage of that av1 encoder and stream to YouTube ( purely out of curiosity). The quality is pretty cool. Can stream 1440p content at 3000 bitrate. That's insane. A lot of popular streamers on twitch are streaming 960p at plus 6000 bitrate. Wild stuff right there. \n\nIm a bit of a COD simp, and this GPU rocks COD and Warzone and from the few other games I've tried, it's been a high fps 1440p wet dream.", "ups": 46, "depth": 0, "author": "blandhotsauce1985", "replies": [{"selftext": "It's been a long road to where the drivers are now, especially OpenGL and VR have been a \"huge\" (huge in these niches) problem - driver wise. There were issues, sometimes very big issues that took AMD years to fix.\n\nI didn't have any issues in the last months but once you encounter an actual bigger driver related issue - like how HEVC was broken for VR for over a whole year, resulting in a rather substantial performance loss (this has been fixed some months ago) - it will more often than not take them months or even years to fix that issue, even though it gets discussed to death on the forums.", "ups": 11, "depth": 1, "author": "ElAutistico", "replies": [{"selftext": "Still had 6900xt performance so very workable.", "ups": 0, "depth": 2, "author": "XXNameAlreadyTakenXX", "replies": []}]}, {"selftext": "1080ti to 7900xtx I\u2019m an ultra wide 1440p 144hz kinda guy. So far it hasn\u2019t been a huge difference in FPS but I\u2019m also able to go from playing on medium-high to ultra", "ups": -2, "depth": 1, "author": "RanaMahal", "replies": [{"selftext": "The 7900XTX hasn't been a huge difference in FPS...?", "ups": 11, "depth": 2, "author": "MercinwithaMouth", "replies": [{"selftext": "Oddly enough, no? I had an ASUS Strix 1080ti I think my card was just an anomaly. \n\nI was maxing out frames on most of the games I played.\n\nAlthough I didn\u2019t play cyberpunk or starfield on my 1080ti", "ups": -3, "depth": 3, "author": "RanaMahal", "replies": [{"selftext": "That is not how anything works. The performance variance between different individual cards of the same model is going to be about 0-0.5%.\n\nChances are you just have a huge CPU bottleneck and/or never needed the extra power.", "ups": 10, "depth": 4, "author": "LongFluffyDragon", "replies": []}, {"selftext": "Doesn't make sense", "ups": 7, "depth": 4, "author": "MercinwithaMouth", "replies": [{"selftext": "Well idk what to tell you lol. I haven\u2019t had a huge difference in FPS but again, like I said, I went from playing with medium-high settings to ultra. The FPS I\u2019m sure on ultra would be a lot different", "ups": -2, "depth": 5, "author": "RanaMahal", "replies": []}]}]}]}]}]}, {"selftext": "I had a 6700 XT prior to a 7900 XTX and I have not had stuttering with either.  I'm sure it happens, but there are a lot of components in a computer that feed each other and it could be a combination of things.\n\nI think its popular to gossip, and regurgitating bad news about things is similar to doing so about people.  It is a bad habit but some find it cathartic.  I won't speculate as to why.\n\nThere are certainly those who experience issues, but many who state \"bad drivers\" as a blanket statement may not own or use an AMD card and are simply self-validating as to why they have non-AMD cards.", "ups": 14, "depth": 0, "author": "B16B0SS", "replies": []}, {"selftext": "I basically did the same as you OP (albeit 3070 to 7900 XT) and have had no issues either.\n\nAll I did was uninstall the Nvidia drivers, remove card, used DDU in safe mode for additional cleanup (not sure if necessary but gave me peace of mind), then installed the new card and drivers once I turned my PC back on.\n\nCouldn't be happier.", "ups": 15, "depth": 0, "author": "spuckthew", "replies": [{"selftext": "Same here 3070ti to 7900xt and its been great", "ups": 6, "depth": 1, "author": "HossBonaventure2nd", "replies": []}]}, {"selftext": "I used to switch between AMD and Nvidia with each new GPU without issue. But that ended with the R9 390 where I took a bit of an AMD hiatus. I ran into too many issues. Driver bug after driver bug would fix issues and cause others. New games took much longer to get optimized drivers, before then they would crash and artifact. I even RMA'd it, but it made no difference. Same kind of issues, and it became clear their drivers were very bad at the time. (About 2015-2017 ish)\n\nIt was a huge load off my mind when I sold it and got a GTX 1060. My other PC was a GTX 1070, then an RTX 3060 TI. For the last 5 years, I've been running Nvidia only and its been a relatively smooth experience.\n\nEarlier this year, in one computer I sold the 1070 for an RX 6600. And last week, I bought a RX 6950 XT relacing the 3060 TI. (3060 TI replaced the RX 6600). So through this year, I've done 1 AMD GPU, 1 Nvidia GPU.\n\nAMD without a doubt has a lot more kinks that need to be worked out than Nvidia. Often little things, but they can still be annoying. If a person isn't tech savvy, I would not recommend an AMD GPU. Nvidia is much more stable and easier to operate.\n\nDon't get me wrong, I'm very happy with my RX 6950 XT. The performance in most games is amazing. Most games run smooth, especially new ones. But I've also been working through little things as well with both AMD GPUs. \n\nAMD fans love to use the \"BoTh SiDeS\" argument, that Nvidia has issues too, but their rate of bugs and driver issues is much smaller. Nvidia's budget for software and R&amp;D is so much bigger, it would be impossible for AMD to keep up with those kind of things.", "ups": 19, "depth": 0, "author": "BeerGogglesFTW", "replies": [{"selftext": "As a Linux user, I would like to add the usual reminder that the roles are reversed on Linux. If any Linux users are reading - AMD drivers are much smoother than NVidia on here.\n\nI am sorry for the dual booters out there because it's not the easiest choice in the world for you all. Prioritize driver stability on Windows or Linux? I guess it depends on what you use the most, but you need to be really committed to whatever you choose. I would argue AMD drivers on Windows are much more stable than NVidia drivers on Linux, but you also need to use Linux for the majority of your time for this argument to be convincing to you.", "ups": 12, "depth": 1, "author": "chic_luke", "replies": []}, {"selftext": "I haven't had any drivers issues with my amd gpu, whereas my 1060 would need a rollback twice a year. I still think the AMD reputation for driver issues is yelled from the top of towers and then parrotted by people who haven't used them.", "ups": 12, "depth": 1, "author": "plushie-apocalypse", "replies": [{"selftext": "I have not had any issues yet on my 7900xtx and it's first  amd gpu. At first I was worried cus of comments like this but it's been nothing but perfect so far tbh maybe even better for non tech savvy people like me cus of how easy there software is to understand. I was so confused looking at  the text wall in nvidias settings  but amds explains everything when I hover over it with my mouse.", "ups": 5, "depth": 2, "author": "chi_panda", "replies": [{"selftext": "IMO the whole software package from AMD is vastly superior to nvidia's but reviewers tend to ignore the overall package quality and focus on solely FPS, which is fine I guess, but then they *do* mention those driver issues, so it's no wonder there is bias.", "ups": 7, "depth": 3, "author": "i14n", "replies": []}]}]}, {"selftext": "I've had zero issues with AMD for literal YEARS.", "ups": 8, "depth": 1, "author": "IrrelevantLeprechaun", "replies": []}]}, {"selftext": "I can only comment on my limited experience: I\u2019ve owned a 970, 1080Ti, 3080 and now a 7900XTX. I never had any issues with any of the Nvidia GPUs, and then experienced frequent game crashes when I switched to the 7900XTX. I tried lots of obvious things like DDU and even uninstalled tons of applications that might be causing conflicts. In the end I tried one last thing, which was a complete fresh install of Windows. That was 1 month ago and I have had exactly zero issues since the clean install. I suspect the issue was that I had carried over from intel to AMD and Nvidia to AMD and crypto mining and blah blah blah, so much crap on my PC. Either way, zero issues now and smooth sailing. The XTX is a powerhouse!", "ups": 17, "depth": 0, "author": "inmypaants", "replies": [{"selftext": "You should always clean install when changing motherboards. Which I assume you mean by intel to AMD. Not surprised you were having issues. But at least you figured it out in the end.", "ups": 22, "depth": 1, "author": "random_reddit_user31", "replies": []}, {"selftext": "Switching GPU brands can definitely lead to crashes. For only the GPU, DDU should normally be fine.\n\nNow you also switched the CPU brand. A full clean install should be standard procedure.\nI think 99% of those driver issues come from people that do not do clean installs. \n\nI even had problems from going from a 3700x to 5800x3d. The only thing that fixed it was a clean reinstall.", "ups": 9, "depth": 1, "author": "RealKillering", "replies": []}, {"selftext": "I want to upgrade from a 3080 and can get a 7900xtx for $850. Does that generation leap feel significant? I play on a 3440x1440 monitor and just feel that I could use a bit more horse power and RAM, but I have never used an AMD graphics card, but currently running a 5800x3d like yourself.", "ups": 3, "depth": 1, "author": "NoToe5096", "replies": [{"selftext": "It's around a 50% increase in raw performance and about 10-25% better RT. If it's something you feel like a deal for you at this price - why not.\n\n&amp;#x200B;\n\nI've jumped from 6900  XT to 7900 XTX, no regrets aside from higher idle power draw but it'll probably get fixed later.", "ups": 3, "depth": 2, "author": "Mungojerrie86", "replies": []}, {"selftext": "I feel the leap only on the games that need more VRAM, otherwise it\u2019s still a decent jump but not massive.", "ups": 1, "depth": 2, "author": "inmypaants", "replies": []}]}]}, {"selftext": "They literally had to disable factory reset on driver installation because it was bricking windows installs lol\n\nThey also had high idle power usage on multiple monitors for like a whole year.\n\nVR performance on 7000 series was lower than a 6950XT.\n\nThe last drivers caused a lot of blue screens on my system\n\nAmd Drivers have problems and it's ridiculous to pretend they don't.", "ups": 4, "depth": 0, "author": "HappyColt90", "replies": []}, {"selftext": "It's just internet echo-chamber effect, some people see a post about how drivers are bad (and in 90% of cases the cause of such problems are an unstable RAM overclock (or overheating ram sticks in badly ventilated case)/ PSU failure or something non-related to GPU at all) and they assume this affects everyone while in reality it's like 0.0001% of users. In reality, all vendors have bugs, for example, release-time KPP drivers for Ada had a bugged implementation of p2p memory    functionality which caused the workstations to literally shut down if you tried to access this subsystem (which was erroneously (or \"erroneously\") enabled on consumer GPUs in 4xxx gen and disabled in 3xxx gen). Nvidia acknowledged this issue only after some bigwig publication wrote about it in march or april while it was reported to them back in autumn 2022.", "ups": 23, "depth": 0, "author": "GuttedLikeCornishHen", "replies": [{"selftext": "That overheating ram sticks is definitely an issue most people are unaware of...   \n\nI switched from a rx480 blower model to a 3090, so I suddenly had a lot more heat in the case, and even worse, the card blows the hot air directly to the memory sticks.  \n\nI have pretty good airflow in the case, but it still wasn't enough, and I started getting crashes in some games and of course first suspected the GPU but then tracked the issue to the memory... which had been working well for 2-3 years at that point. I thought that maybe it got unstable in some bios update, and raised latencies until it was stable again. But of course I wasn't running the GPU at 100% while testing the ram stability, so the issue was still present randomly.  \n\nThen I went and removed half (2 of 4) of the memory sticks for a test and almost burned my fingers because they were so hot... and figured out the actual reason. I added a fan blowing at the memory and that helped.", "ups": 8, "depth": 1, "author": "jkk79", "replies": [{"selftext": "Computers are so funny sometimes, especially when you build your own. All these parts that are independently designed creating a pretty complex system of electronics and thermals and interactions. My first PSU I got in my build was super unstable and it always manifested in \"driver crashes\" from the AMD driver, but I'm guessing only because it was the biggest user and most strict about power and was being fed crap. Upgraded the PSU to a Platinum and all issues are resolved, solid as a rock.", "ups": 6, "depth": 2, "author": "calinet6", "replies": [{"selftext": "Yeah, bad PSU's can cause all sorts of crap. I've had one blown on me 20+ years ago and it killed 2 hdd's. After that I've had 2 more PSU's dying but luckily they didn't cause any additional damage.  \n\nAnd now couple of months ago, after the memory issues were solved, I found 2 games that would just randomly cause my PC to shut down (fallout4 VR and cyberpunk). There was a loud click from the PSU and system just shut down. \n\nI instantly knew it must be the overcurrent protection... And I was using a kWh meter on the computer power cord, and it was showing max usage \"only\" at 630W, while my PSU was a 750W Seasonic platinum (single 12V rail). It shouldn't trigger nowhere near that. But then I did read about that particular PSU and apparently that's an issue with them, they don't do well with transient spikes and the overcurrent protection kicks in.  \n\nI bought a 1000W titanium rated Be Quiet! PSU to replace the old one, and it works really well. BUT the big surprise was, that the system now only uses max 550W... So the efficiency is MUCH better with this one, not only because it's titanium rated but also that I'm nowhere near the maximum wattage.", "ups": 5, "depth": 3, "author": "jkk79", "replies": []}]}]}, {"selftext": "I wonder why it gets attributed to the AMD GPUs then. It's not like RAM overclocks are exclusive to people with AMD GPUs.", "ups": 2, "depth": 1, "author": "Podalirius", "replies": [{"selftext": "Mind share. Typically - and I'm generalizing here - Nvidia users will troubleshoot assuming the issue lies elsewhere while AMD users, especially ones without prior experience, default to blaming \"bad drivers\".", "ups": 3, "depth": 2, "author": "Mungojerrie86", "replies": []}]}, {"selftext": "No it isn't. AMD has a terrible track record on driver support (look at the Fury), the final year of R9 280/280x/7950/7970 drivers have a major bug over displayport that breaks high resolution and my 5600xts crashed constantly under 3d load.", "ups": 2, "depth": 1, "author": "other_goblin", "replies": []}]}, {"selftext": "There are many important points to this. \n\n1. Firstly most people have a bad experience and will use that as enough data to say something general about a brand. But AMD people are alot more vocal and willing to tinker(all linux users use AMD). Which is also one of the reasons AMD sub is larger than Nvidia despite alot less people buying AMD GPU's. I know people also go to AMD for CPU's so that skews it. But not many years ago AMD was still larger than Nvidia+intel subs while selling alot less.\n2. And this joins awfully well with the fact that most Nvidia people are not techy types. They either need a Cuda GPU for work, so already there we have Nvidia \"driver\" win or they are gamers usually buying prebuilds or laptops eg in general not techy types. And the types to exaclty be mad when it just doesnt work, and sometimes that will just be because they are unable to troubleshoot normal issues that pop up in the PC space.\n3. Nvidia has ALOT more money which of course means bigger teams, higher paychecks more work can be done in a shorter time. AMD drivers often dont release as is and will over time be faster. Nvidia almost never is. So by this fact alone we can say Nvidia do release more \"finished\" drivers, but also dont do as much to squeeze out performance over time. This increases chances of bugs. Finewine its called, you can like it or not but its a necessary evil because AMD just doesnt have the ressources to develop as Nvidia does. Does not mean its alot worse.\n4. People do not know how to upgrade a system - Nvidia people who have been used to slapping in a new GPU without removing old drivers will see more problems with a switch to AMD GPU's with those older drivers as they are just alot less compatible. Nvidia probably makes sure their drivers doesnt conflict too much because they know people will just do drop in upgrades \"because it works\". You saw that alot like 3+ years ago people are becoming aware that old drivers should not be left for new hardware. Its easier going from same brand than going to a new brand and a completely different architecture. \n5. A new release isnt the same everytime. 5000 series had issues it was a completely new arch and it had troubles for years. I had it it was there but it was a good GPU for the money and aged very well. 6000 series the \"refresh\" of the arch was alot more polished although not perfect but it was a good release and was pretty stable and very competitive. 7000 series first MCM GPU for gaming, that actually works well seen in that light. But its a very different architecture to anything on the market so comparing it to something like Ada that is simply an evolution of the old one with added hardware instead of a reinvention of the core itself, 7000 series really should have more issues than 4000. And thats how it goes some GPU arch's are more difficult to release than others. 8000 series probably not being a true MCM chip because they cant get it working in time is a testament to this. Now 9000 series should be 8000 series refreshed but with 2-4 actual GPU's build in. This is a problem we have tried to solve before with SLI/Crossfire but it was simply a crude solution demanding per game optimization and even then not really succesful. Making it work on a single board will be huge, when we are capable. Literally Ryzen all over again but its straying from the known and familiar so it will be more diffcult to get drivers right.  \n\n\nIn general i believe Nvidia has a leg up but a smaller one. This leg up is not just because they are a better company but the fact that they have been more aggressive when it comes to scummy market practices and delivering proprietary tech to make sure people cant leave their ecosystem. AMD going the opposite way doesnt look as cool when buying this new shiny thing. But in actual practices they compete very well and most of the innovative Nvidia features comes on Radeon aswell before its even relevant. Just look at RT its all the craze and it is really cool, but 90% of games people sit down and play is not RT. But it weighs so heavily on some peoples mind its just really disproportionate to how usefull it is. Same with DLSS yes its better. But when you're at home playing a game you are not gonna practically notice much difference, your experience will not be that much better. People are different some will notice more than others but overall an example like 7900xtx is directly comparable to a 4080 each has some wins and losses.\n\nEdit: sometime I write in absolutes like \"all Linux users use amd\" which of course isn't true but their support is unchallenged and have much higher proportional amount of users compared to windows. Most people should use amd on Linux is a better way to put it. \n\n \nI'm an AMD fanboys so am biased. But I only am that because Nvidia fucked me, as a former Nvidia user, and I see them continue this road of giving out minimum to like about them while really trying to cut out the market for them alone to a point where they are almost a monopoly and we should legit fear AMD dying because people think they are a bad buy. A fear Nvidia is happy perpetuate to get to their ultimate goal of being a monopoly. Which will be bad for all of us. We need AMD/intel to be viable alternatives and this also means pushing truth which is dlss isn't a huge win and RT is an edge case which is very cool but isnt as valuable as some would like to make it.", "ups": 21, "depth": 0, "author": "MrPapis", "replies": [{"selftext": "\\&gt; All linux users use AMD\n\nYou'll get some shit for that, but it's right in spirit. If you're a linux user and you have any experience with graphics cards, you're buying an AMD for your next card if you don't already have one, no question.", "ups": 4, "depth": 1, "author": "calinet6", "replies": [{"selftext": "I edited to point out this is more of a understanding the meaning behind than my actual words being completely true. I did not spend enough time on this to a point it's bulletproof but it is in the right direction imo.\n\nEdit: it's a fair criticism though!", "ups": 4, "depth": 2, "author": "MrPapis", "replies": []}, {"selftext": "AMD for Linux desktop, Nvidia or AMD for GPU compute on Linux servers depending on what FP size you're working with.", "ups": 2, "depth": 2, "author": "hardolaf", "replies": []}]}, {"selftext": "\"This is a problem we have tried to solve before with SLI/Crossfire but it was simply acrude solution demanding per game optimization and even then not really succesful.Making it work on a single board will be huge, when we are capable. \"\n\n&amp;#x200B;\n\n[https://www.pcgamer.com/quick-history-multi-gpu-video-cards-2014/](https://www.pcgamer.com/quick-history-multi-gpu-video-cards-2014/)\n\nIt's been done before. That said, I expect there to be new engineering challenges. I'm pretty sure the latencies have come down considerably in modern GPUs. I wonder if future AMD GPUs will have 3D Vcache. Might have been possible already on the 7000-series. Looks like there was room for it. https://www.notebookcheck.net/AMD-Radeon-cards-with-3D-V-Cache-may-become-a-reality-as-Navi-31-GPU-found-to-integrate-possible-connection-site-for-3D-cache.688185.0.html", "ups": 3, "depth": 1, "author": "BigHeadTonyT", "replies": []}, {"selftext": "thanks for this, this make a lot of sense", "ups": 2, "depth": 1, "author": "Watchful_Heavenly", "replies": []}, {"selftext": "&gt; (all linux users use AMD)\n\nlol. Most of the more popular distros are popular because they come with proprietary nvidia drivers. AMD has more market share than they usually do on linux (because their drivers are so much better on linux, since you just use the open source ones and not the shitty amd ones) but nvidia is still fairly popular even on linux. It's probably closer to 50/50 on linux.", "ups": 4, "depth": 1, "author": "BFCE", "replies": [{"selftext": "most popular distros don\u2019t come with nvidia proprietary drivers. their package repositories usually have them, though.", "ups": 5, "depth": 2, "author": "outofstepbaritone", "replies": []}]}, {"selftext": "This should be top comment (hell it should be fucking pinned on this sub). This is very well written and covers a lot of points. Nicely done.", "ups": 1, "depth": 1, "author": "CrzyJek", "replies": [{"selftext": "Lmao what are you talking about. That's a fanboy's delusional overgeneralization.\n\nAll AMD owners are 1337 h4x0r script kiddies that know how to  fix issues while all Nvidia owners can't tell left from right? When people buy expensive stuff they expect it to work. That's on the companies to figure out, not the customer.", "ups": 8, "depth": 2, "author": "Headrip", "replies": [{"selftext": "Oof. Someone needs to work on their reading comprehension.", "ups": -5, "depth": 3, "author": "CrzyJek", "replies": [{"selftext": "Sorry, I own a Nvidia card and only yesterday learned how to tie my shoes.", "ups": 10, "depth": 4, "author": "Headrip", "replies": [{"selftext": "Ah, that makes sense then.", "ups": -6, "depth": 5, "author": "CrzyJek", "replies": []}]}]}]}, {"selftext": "I love the point about how the Reddit AMD community working together to solve issues creates a vocal AMD fan base on Reddit.  \"Neanderthal marketing\" is really a grassroots community.\n\nIf you prefer an anti-social clean corporate GPU owning experience then AMD can be kinda bad.\n\nIts like how back in the day Chevy owners complained about expensive Japanese car parts.  You have to fix the Chevy more, but its easier to do, and kinda fun if you can get by without your car sometimes.", "ups": 0, "depth": 2, "author": "stu54", "replies": []}]}, {"selftext": "Nice write up. I have to constantly mention 2 points with AMD drivers. 1. If it's at release then expect issues. It will be fixed. But if that is a problem for you then yeah, you should explore alternatives or wait. I refuse to cite the AMD \"fine wine\" argument with drivers because yeah the longer support is nice but this is counter balanced by the typical (not always) initial 6 months of bs. I don't mind it personally because I can troubleshoot and have been in this game for decades but if someone is asking me advice I'll be up front with them about the pros and cons. \n\n2. On the other hand if I'm honest about the initial driver bs I have to point out the other bs. People cite bs launch benches months or years later. Nvidia is just the best out of box at release, plain and simple. As you stated, they got the resources. But an AIB I have now with updated drivers and my own custom tinkering is going to smash any benchmark from release now that it's been a year later with AIB releases and driver updates. I'm guessing Intel will be in this same boat as they get deeper in the game, at least for a few gens. But anyways, my 7900XTX I got now for $889 that levels any $1000 release benches on YouTube is a much different value proposition than the release benches people still site  currently. Especially at 2k, it's just a monster and can pretty much level a 4080 by a moderate margin in raster. Its a totally different ball game with AMD cards 6 months to a year after release which is their own fault but still something people too often ignore.", "ups": 0, "depth": 1, "author": "Tuned_Out", "replies": [{"selftext": "Yeah I've been red for long enough to know what you're talking about but reality is of you just let the leader get everything because he is the leader then we don't have competition and we know how that goes. I'm political about this because consumer hardware is no joke and what Nvidia could/would do with a monopoly can send chills down my spine. We cannot imagine what they are capable of. We see what they do while still keeping being in good graces of their users and things can be much worse.", "ups": 2, "depth": 2, "author": "MrPapis", "replies": []}]}]}, {"selftext": "I can laugh my ass off when people mention latency.\n\nMy gaming rig is humble Ryzen 7600 + Radeon RX 6800 XT. I play with unlimited framecap with Anti-Lag and Enhanced Sync enforced via drivers for every game I play. I am yet to encounter any issues while keeping my latency roughly around 6-7 ms, that is insanely low considering already \\~ 10 ms is great number (I was having around 10 with Nvidia).\n\nI can only imagine how much better the situation gets with Anti-Lag+ (RDNA3 exclusive), but the truth of the matter is the latencies are extremely low on modern Radeons. And performance is very good, I switched from 3080 10G and I am getting higher FPS pretty much everywhere these days as drivers matured really nicely.", "ups": 20, "depth": 0, "author": "nodating", "replies": [{"selftext": "Is this on 1440p. Looking to upgrade and looking at your upgrade path.", "ups": 5, "depth": 1, "author": "motoryry", "replies": [{"selftext": "Yes, I play in 1440p. No complaints here really, CPU is more than fine since it is still Zen 4 with all the bells and whistles when it comes to IPC and hefty caches. 6800 XT aged like a fine wine in my eyes, unlike RTX 3080 10GB, that card should never have had less than 12GB VRAM.", "ups": 2, "depth": 2, "author": "nodating", "replies": []}]}, {"selftext": "Yep, the most bs argument around. If anything it is slightly better", "ups": 0, "depth": 1, "author": "riba2233", "replies": []}]}, {"selftext": "It's not nvidia fanboys or amd fanboys.... People who have used gpu's from both vendors for more than 20 years will tell you that amd can be inconsistent sometimes.... One generation is great than next one is not... whereas nvidia works pretty much all the time....\n\nIf i remember correctly the whole thing started with WOW the burning crusade having issues on AMD\n\nSo the older you get the less you wanna f around and find out :)\n\nThat's pretty much it everything else is internet noise\n\nFrom what i heard 6000 and 7000 series from amd is fine... 5700 xt was very bad at launch", "ups": 10, "depth": 0, "author": "nexus1242", "replies": []}, {"selftext": "Stuttering and bad frame times were a thing about 10+ years ago.", "ups": 16, "depth": 0, "author": "Vivicector", "replies": [{"selftext": "its just not true i've had all amd graphics cards since 2010 and last year for whatever reason my rx 460 and 570 would just refuse to play league of legends they would only stutter and so i had to buy a used nvidia gpu", "ups": 4, "depth": 1, "author": "coffeeismydrug1", "replies": [{"selftext": "Last year, the 22.5.2 driver introduced DXNavi, which has DX11 optimizations. But it can causes stuttering so you can try to disable it. There are plenty of guides on reddit for it. Sorry for my bad English.", "ups": 2, "depth": 2, "author": "hungluongquoc", "replies": []}]}, {"selftext": "other than the stuttering with 6000 series just a year ago Lol", "ups": -8, "depth": 1, "author": "petron007", "replies": [{"selftext": "lol that never was a thing, maybe for a few, but few of the Nvidia cards have it too, soooo......XD", "ups": 10, "depth": 2, "author": "Death_Pokman", "replies": []}, {"selftext": "Sauce me up, baby", "ups": 7, "depth": 2, "author": "danny12beje", "replies": [{"selftext": "just open any Ancient's driver review and scroll to the comments or google rx6000 stuttering.   \nIts really is not some treasure you have to dig for \ud83d\udc80\n\n&amp;#x200B;\n\nThis guy below saying \"maybe for a few\", as if thats still not a driver problem \ud83d\ude02\ud83d\ude02Holy", "ups": -8, "depth": 3, "author": "petron007", "replies": [{"selftext": "Do you think nobody has issues with nvidia drivers?\n\nLmfao my man", "ups": 5, "depth": 4, "author": "danny12beje", "replies": [{"selftext": "Where did I say that, holy AMD clowns \ud83d\ude02\n\nJust in other comment I said that if you go on nvidia forums they've got problems of their own.", "ups": -1, "depth": 5, "author": "petron007", "replies": [{"selftext": "Mfer comes to r/AMD and is surprised when people call his nvidia stanning out.\n\nGo outside, touch some grass.", "ups": 0, "depth": 6, "author": "danny12beje", "replies": [{"selftext": "Ah yes, calling out AMD issues means you are stanning for Nvidia, while just in another comment you mentioned Nvidia's issue, let alone running AMD in all your systems the past 10+ years.\n\nHoly, you dont need grass, you need air bro", "ups": 1, "depth": 7, "author": "petron007", "replies": []}]}]}]}]}]}, {"selftext": "That was a Windows bug and affected all cards", "ups": 1, "depth": 2, "author": "looncraz", "replies": []}, {"selftext": "So we making shit up now", "ups": 1, "depth": 2, "author": "jwilde8592", "replies": []}]}]}, {"selftext": "The days of optimized driver for DX11 era and earlier have long gone with DX12 and vulkan, under these two updated APIs, many of the things that the driver used to do are now the developer responsibilities, this was essential to give a low level access to devs, which lifted a heavy weight burden from GPU manufacturers, but well, we are getting shitty optimized games in return due to incompetent devs unfortunately.\n\nAlso, AMD has revamped their whole DX11 drivers, this gave a great boost in performance for these games.\n\nIf u wanna learn more about how DX12 differs than DX11, check this out:\n\nhttps://developer.nvidia.com/dx12-dos-and-donts\n\nQuoted from the article (which is one of the most important things that the driver used to do):\n\nDon\u2019t rely on the driver to parallelize any Direct3D12 works in driver threads\n\nOn DX11 the driver does farm off asynchronous tasks to driver worker threads where possible \u2013 this doesn\u2019t happen anymore under DX12\n\nWhile the total cost of work submission in DX12 has been reduced, the amount of work measured on the application\u2019s thread may be larger due to the loss of driver threading. The more efficiently one can use parallel hardware cores of the CPU to submit work in parallel, the more benefit in terms of draw call submission performance can be expected.", "ups": 3, "depth": 0, "author": "TheFather__", "replies": []}, {"selftext": "On this Vega64 (reference design watercooled) for a bit more than 6 years now, never had issue with it. Still do the job, good frame pacing, its a very reliable card.\n\nNote that i never play new AAA flat games on release, so i probably avoid some issues on nvidia partners games. And my main usage is VR (skyrimvr) with a Vive Pro 1.", "ups": 3, "depth": 0, "author": "Nwalm", "replies": [{"selftext": "Me too, however I do play new AAA releases.  The internet itself was bad mouthing Starfield and poor performance so much I was scared to see how bad it would be.  Played on my Xbox series X at first then just a few days ago loaded it on my PC and was floored by how well it plays.  Not at levels I usually play as far as frame rate goes but it maintains 45FPS constantly and looks amazing as far as clarity and texture quality goes.\n\n&amp;#x200B;\n\nBut that being said next year may be the year I replace her.  But boy I love this GPU and have loved every minute.", "ups": 2, "depth": 1, "author": "SeraphSatan", "replies": [{"selftext": "I always wait for double perf/\u20ac to upgrade. We are not there yet, but normally it should be next gen for me.", "ups": 2, "depth": 2, "author": "Nwalm", "replies": []}]}]}, {"selftext": "Its funny to talk about drives issue and most ppl are to young to remember when Nvidias drives literally killed users gpus. ([source](https://youtu.be/dE-YM_3YBm0?si=6cV2txWT9Wr_6VGj&amp;t=404))", "ups": 3, "depth": 0, "author": "000r31", "replies": []}, {"selftext": "I just got rid of a couple of 5600XT because they were so unstable they crashed the pc repeatedly.\n\nIt's not an \"old\" thing, it's an amd thing. Sometimes it works, other times not. Don't assume it has anything to do with fanboys just because yours works.\n\n\nAMD driver improvements has been a long term meme. In 2018 they were saying \"are these nvidia fanboys stuck in 2013\". Now you're saying \"are these nvidia fanboys stuck in 2018\".\n\n\nThe final year of R9 280 drivers are completely broken as well, 4k 60 over hdmi displayport doesnt work anymore without a ridiculous workaround that is unstable. That was only in 2020.\n\nFurthermore AMD Fury owners lost driver support after like 5 years. Meanwhile the older 980Ti recieves driver updates still many years later.", "ups": 3, "depth": 0, "author": "other_goblin", "replies": []}, {"selftext": "I am willing to bet \"driver problems\" in both AMD and nVidia cards are nearly always related to other PC hardware or software issues. Or people refuse to believe their \"100% stable overclock\" could cause problems with some games.\n\nPersonally my \"100% stable overclock\" was 100% until I started using RTX Super Resolution with Twitch. Then Twitch froze entire PC every few hours until I relaxed memory settings.\n\nReading Steam reviews, it's kind of funny how same people seem to have these issues with many games. Like if a game really crashes every 30 minutes, don't you think EVERYONE would complain and not just you.", "ups": 10, "depth": 0, "author": "Ilktye", "replies": [{"selftext": "That's why I gave up getting involved in CPU/GPU overclocking conversations.\n\nOthers would claim that their ridiculously high overclocks were stable on the basis that they could reach the Windows desktop, whilst my slightly more modest overclocks were definitely 100% stable under any combination of simultaneous CPU/GPU/RAM/subsystem stress tests I threw at them including at vastly elevated ambient temperatures.", "ups": 4, "depth": 1, "author": "Grunthos_Flatulent", "replies": []}, {"selftext": "I wonder why it gets attributed to the AMD GPUs then. It's not like overclocking is exclusive to people with AMD GPUs.", "ups": 0, "depth": 1, "author": "Podalirius", "replies": [{"selftext": "It's because AMD's community allows bug report posts everywhere whereas Nvidia communities force you into dedicated support threads on reddit or onto support-only forums making most users never see the issues with Nvidia.\n\nFor example, the RTX 4090 had tons of launch problems with drivers but unless you were a RTX 4090 owner, you likely didn't hear about any of them. They got fixed after about 4-6 months, but there were tons of them at launch. Annoying problems like breaking multi-monitor setups, pixel data being sent on the wrong data channels, screen images shifting midway, etc.", "ups": 3, "depth": 2, "author": "hardolaf", "replies": []}]}]}, {"selftext": "I'm using 7900XT since January, and actually don't have slightest idea what about people are screaming on the net. Only issue I have is that Adrenaline resets frame cap after driver update, that's it", "ups": 7, "depth": 0, "author": "mixedd", "replies": [{"selftext": "The main driver issue thing:  there were driver issues going back to the 5000 series.  But really not alot after that.\n\nThe main issue I see nowadays is people OC or UV Gpus, seeing issues, and them saying \u201cAmD DriVeRs BAd!\u201d When really its a very different issue.\n\nIve had a 7900xt for about six months.  Zero driver issues, unless you count the time when Msoft updates the drivers automatically.  Ugh.", "ups": 1, "depth": 1, "author": "caydesramen", "replies": []}]}, {"selftext": "https://www.reddit.com/r/Amd/comments/12648u4/7900xtx\\_driver\\_timeout\\_error\\_investigation\\_and/", "ups": 5, "depth": 0, "author": "zigzag312", "replies": [{"selftext": "Driver timeouts are often hardware related though.", "ups": 2, "depth": 1, "author": "The_Countess", "replies": [{"selftext": "They're also OS related... Nvidia has had an on-off-on-again relationship with them for over a decade. They appear and disappear based on what Windows patch version you're running. It was really bad when the RTX 4090 launched and people were running into them constantly when doing GPU decoding.", "ups": 2, "depth": 2, "author": "hardolaf", "replies": []}, {"selftext": "I had driver timeout issues with Ryzen's 7950x iGPU (RDNA2). They were not hardware related.", "ups": 3, "depth": 2, "author": "zigzag312", "replies": []}]}]}, {"selftext": "The only people who like AMD GPUs, are people who actually have them! Fyi", "ups": 4, "depth": 0, "author": "Darth-Zoolu", "replies": []}, {"selftext": "Never had any issues with my 6600 and my current 7700xt. In fact, AMD drivers are much better on Linux in comparison to nvidia. \n\nIt\u2019s an old issue that\u2019s no longer relevant.", "ups": 6, "depth": 0, "author": "SuperbOrchid", "replies": []}, {"selftext": "IMO A lot of the \"driver issues\" were actually poor AIB build quality.  Not all, but a lot.\n\nBut RDNA2 and RDNA3 both seem to be fine.  IIRC both had a few minor issues at launch, but no more than Nvidia cards have to deal with.\n\nSide note: you're almost certainly taking about Windows drivers but my Linux experience with my 7900XT was: Install card, install Linux (kernel 6.4), card was recognised and worked as expected immediately.  RDNA3 needs Kernel 6.3 or later to work properly, as I understand it.", "ups": 7, "depth": 0, "author": "INITMalcanis", "replies": [{"selftext": "I'm not buying \"poor AIB build quality\" argument. I had driver issues for months with Ryzen's 7950x iGPU (which is based on RDNA2).", "ups": 5, "depth": 1, "author": "zigzag312", "replies": [{"selftext": "Why...are you using the iGPU on the 7950x?", "ups": -2, "depth": 2, "author": "danny12beje", "replies": [{"selftext": "Because I don't need dGPU for coding ;)", "ups": 7, "depth": 3, "author": "zigzag312", "replies": []}, {"selftext": "Yes ignore the issue at hand, and blame the user for using a product \ud83d\ude02\ud83d\ude02\ud83d\ude02\ud83d\ude02", "ups": -3, "depth": 3, "author": "petron007", "replies": [{"selftext": "That's not what he said.", "ups": -2, "depth": 4, "author": "Flaimbot", "replies": [{"selftext": "Its classic move of shifting topic and blaming the user, seen it million times now.", "ups": 4, "depth": 5, "author": "petron007", "replies": []}]}]}]}]}]}, {"selftext": "But of course. In any situtation - just keep shitting on AMD drivers.  \n\n\nAnd then something like Horizon Zero Dawn and Starfield comes out, AMD cards having up to double the performance than their Nvidia counterparts, and yet no one is crying about shitty Nvidia drivers. In case of Starfield, it was even proven already that it's drivers issues. How come?", "ups": 7, "depth": 0, "author": "Elliove", "replies": [{"selftext": "When Nvidia drivers are bad, it's the game's fault. When AMD drivers are bad, it's AMD's fault.\n\nI do agree AMD's track record with their drivers hasn't been the best in the past, but then they've been solid af for years at this point. Most \"internet advisers\" still stick to the old argument because it's too much work to do the due diligence and stay up to date with the current situation. Nvidia always makes perfect drivers without fail, even when they don't. That's how much mind share they've got.\n\nIn my experience these days both companies push a bad driver every now and then. Buggy software updates are completely normal - Windows does it literally every single feature update, for example. Software updates are rarely perfect.", "ups": 13, "depth": 1, "author": "HyruleanKnight37", "replies": []}]}, {"selftext": "Historically I have bought midrange cards from AMD and have never really experienced the infamous driver issues. 1st gpu was an HD 7850, then an RX480, 5700xt, and now 6950XR", "ups": 2, "depth": 0, "author": "BaddMeest", "replies": []}, {"selftext": "I've been using ATI/AMD graphics cards almost exclusively since 1992 and very seldom had a problem with drivers with any of them. That's not to say that nobody else has had major problems, but it frequently seems to come down to idiocy rather than anything actually being wrong.", "ups": 2, "depth": 0, "author": "Grunthos_Flatulent", "replies": []}, {"selftext": "It\u2019s BS that\u2019s been perpetuated since the 2900xt days when AMD launched their first GPU after acquiring ATI. I\u2019ve had majority nvidia ranging back to the FX5500. But in that time I\u2019ve had a 4890, 7970, vega 56, 6700, and have never had issues with drivers on either side. I\u2019m not saying it exist in a vacuum but its wildly blown out of proportion imho", "ups": 2, "depth": 0, "author": "5RWill", "replies": []}, {"selftext": "It still happens more than with Nvidia, Nvidia gets theirs fixed faster when they do have them, but AMD is doing better these days.", "ups": 2, "depth": 0, "author": "Blackhawk-388", "replies": []}, {"selftext": "[deleted]", "ups": 2, "depth": 0, "author": "[deleted]", "replies": [{"selftext": "You need to learn computers, AMD has been solid for me since the Athlon/ATi days", "ups": -1, "depth": 1, "author": "Faceh0le", "replies": []}]}, {"selftext": "As someone that had driver issues with two different AMD cards. a 5xxx and 6xxx series card. I can say after a month of trouble shooting. The drivers just simply didn't work for me. There are tons of others the drivers didn't work for also. \n\nSeems like AMD drivers either work or don't. There is very little middle ground for people. \n\nThat being said I have never personally had issues with Nvidia cards are there drivers. Switching back to Nvidia seemed to have resolved my AMD woes. \n\nEven after people assured me it was a hardware issue (Seemed common to blame the power supply)\n\n((went from a 6950xt to a 4080)) the 4080 has given me zero issues.", "ups": 2, "depth": 0, "author": "HotRoderX", "replies": []}, {"selftext": "My 6900xt has had nothing but problems with drivers", "ups": 2, "depth": 0, "author": "commissar0617", "replies": []}, {"selftext": "I went from an 8700k and 2080 to a 5950x and 6900xt. Worst decision of my life... I was all excited about amd becoming viable again, and wasn't liking things that Nvidia was doing, so I wanted to give amd another chance. From the moment I got it (fresh install) the first thing I noticed was a very marginal increase in FPS on games at 2k res. That made no sense whatsoever, and I was quite disappointed. Then came ALL the issues. Random BSODs while system was idle (on stock setting at this point mind you), graphics drivers crashing while in game requiring a full restart to fix, poor optimization and low fps in games that ran fine on my friends PCs with lesser (Intel + nvidia) hardware, videos in Chrome freezing every couple seconds, etc. I tried quite literally everything. Undervolt, curve optimizer, static OC, discrete tpm, win 10, win 11, MPT, you name it, I tried it. Not to mention the desktop stuttering and lagging while just browsing the internet due to AMDs refusal to include something like Nvidia's \"prefer max performance\" setting. The problems got worse and worse, so I thought maybe my motherboard was buggy. I replaced it with an msi unify x-max (or whatever it's called), and while a number of cpu related issues got better, a lot were still present, and the gpu related issues were all still there. Static oc was about the only way I could run the cpu where it would be anything resembling stable, and I eventually used MPT to get gpu to stay at a constant frequency. That solved a little bit, but one thing I haven't mentioned yet was thermals... on both cpu, and gpu, they were ATTROCIOUS. I bought a new liquid freezer ii 360 AIO with the cpu and gpu, and cpu would still thermal throttle. It was actually impossible to keep the cpu Temps at bay, even with undervolt, and underclock, so static oc was the only way to resolve that. For gpu, it also reached max Temps very easily, and no amount of case fans running at jet turbine speeds (I'm talking multiple noctua 3000rpm industrial fans) would prevent it. I swapped cpu for 7950x, and that's has been PHENOMINAL. Almost all cpu issues were gone, but I do still get microstutter, and weird hitching. GPU wasn't salvageable. So I wrote an angry email to amd, and they let me return it for full price, and I used that to get a 4090 strix. While I don't crash anymore, and it's now completely stable, I do still get hitching, and random microstutter, so once 15th series Intel is out, I'm jumping ship. I will NEVER, EVER get an amd gpu again as long as I live. AMD cpus... maybe, but honestly I'm at the point in my life where I just want everything to work... and Intel + Nvidia cannot be beat on that front.", "ups": 2, "depth": 0, "author": "Shadow_12347", "replies": [{"selftext": "&gt; . I swapped cpu for 7950x, and that's has been PHENOMINAL. Almost all cpu issues were gone, but I do still get microstutter, and weird hitching. GPU wasn't salvageable. So I wrote an angry email to amd, and they let me return it for full price, and I used that to get a 4090 strix. While I don't crash anymore, and it's now completely stable, I do still get hitching, and random microstutter, so once 15th series Intel is out, I'm jumping ship. I will NEVER, EVER get an amd gpu again as long as I live. AMD cpus... maybe, but honestly \n\nhad a 5900x and microstutter was a frequent occurrence, in the year that i owned it i'd always try to keep up with the latest bios/chipset updates, but sadly things never got better.\n\n when i decided to upgrade the CPU after some deliberation i went with Intel as i didn't want to wait for 7000 series 3D chips and honestly my trust in AMD has eroded, i knew i'd get a really nice performance uplift as i watched countless reviews but what surprised me the most was how SMOOTH games felt, even games that i thought ran fine before were noticeably better, when i loaded into COD:MW2 for the first time i thought the server glitched or something because the game felt unnaturally smooth, then the game countdown started and i was blown away how fluid everything felt !\n\n&gt; I'm at the point in my life where I just want everything to work... and Intel + Nvidia cannot be beat on that front. \n\nafter spending couple of years on AM4 i have came to the same conclusion, i just want shit to work and Intel + Nvidia have never gave me trouble.", "ups": 2, "depth": 1, "author": "JensensJohnson", "replies": []}]}, {"selftext": "I've had the same experience. Had been using Nvidia my whole life but was always curious about AMD. But I stayed away from AMD because of all the alleged horror stories and driver issues. I finally struck up the nerve to go and spend my money on a 7900 XTX and have had it about a month now and I've been so happy with it. Haven't had one issue, performance has been phenomenal. Quite honestly for the money I saved getting a 7900XTX instead of a 4080, I don't give a shit that my puddles don't reflect colors as vividly as Nvidia RT. I've been playing games at full settings with a 2k monitor and it's pretty glorious. \n\nAlso the AMD software kicks ass unlike Nvidias piece of crap software from the Win XP age.\n\nI found this video quite interesting and is what helped me decide to get an AMD: \n\nhttps://youtu.be/4YAZn7Og4yo?si=jrvyCio3CtKZQ7uH", "ups": 4, "depth": 0, "author": "080128", "replies": []}, {"selftext": "Driver issues seem to be hit or miss, I personally had more issues with Nvidia than AMD", "ups": 3, "depth": 0, "author": "Katoshiku", "replies": []}, {"selftext": "The AMD GPU 'problems' are always over talked, both brands have there driver bugs.\n\nThey also used to use the AMD GPU runs hot or uses to much power, RTX 30XX GPU's made that a harder talking point with 400W power use. \n\nI use both brands, there both fine.", "ups": 2, "depth": 0, "author": "liaminwales", "replies": []}, {"selftext": "The thing about FSR is it looks good until you turn on DLSS and see how that runs. It's like spaghetti: canned sauce on spaghetti tastes good but once you make a homemade sauce it's hard to go back. Difference in Starfield is massive. I thought it was running DLSS and I was like \"this is fine\". Realized after 10 hours of trying to play with normal mods that I added the ini to the wrong folder since no mods would load and finally moved it to the correct one. Looks almost like actual 4k now.", "ups": 4, "depth": 0, "author": "wsteelerfan7", "replies": [{"selftext": "I mean, DLSS uses dedicated hardware that are only present on Nvidia cards, hell, even if AMD/Intel came out tomorrow with a GPU that could do the same as the Tensor Cores (Intel has the XMX Engines so maybe they already did?), i don't think Nvidia would allow them to run DLSS. So, while we compare them because both strive to achieve the same goal, i think it's a bit harsh to fault FSR for being worse considering it does it's job while being hardware agnostic.", "ups": 0, "depth": 1, "author": "WiltedBalls", "replies": [{"selftext": "But OP and others sometimes get caught saying FSR is just as good when it's honestly not close. So they're making the comparison and you're calling it harsh for me to point out DLSS is better because NVIDIA uses hardware.", "ups": 3, "depth": 2, "author": "wsteelerfan7", "replies": []}]}]}, {"selftext": "I mean its one thing to run Radeon for 2 weeks and another for 2+ years. \n\n  \nI've been running AMD hardware for the last decade, and driver issues always come up one way or another. It's definitely something to be warned about, although I am not sure how much and if, Nvidia has the same issues. \n\nAt this point, I don't update to the latest drivers until a few weeks pass, just so I can look up reviews of the driver and check the comments to see if there are any issues.", "ups": 8, "depth": 0, "author": "petron007", "replies": [{"selftext": "Ok fair maybe thats just a thing that happens every once in a while but its not like nvidia is free of that. Every time a new game ready driver came up the old one was so bad for me it felt like malware and the new ones had tons of bugs and got consistently worse. Atleast that was my experience since the 40s came out which is ultimately part of the reason I upgraded to amd instead of nvidia. So I guess maybe driver issues are just a problem, not an amd problem?", "ups": 0, "depth": 1, "author": "MADcrft", "replies": [{"selftext": "The only experience I've had with Nvidia is at my brother's computer and he hasn't had any issues forever now, but if you go on nvidia forums lately, you'll see driver issues, so yeah it seems after 40 series, kinda went down in terms of quality.\n\nMost new games that I play are mostly stable nowadays, but when you start digging into productivity, it goes down the shitter real quick.\n\n&amp;#x200B;\n\nAnd thanks to random AMD fanboy basement dwellers that downvote facts, just open any driver review and see the flood that comes into comments about issues that haven't been fixed for months if not years. Stop coping yall.", "ups": 3, "depth": 2, "author": "petron007", "replies": []}]}]}, {"selftext": "I have a 6800xt, coming from 1080ti, no issues.", "ups": 2, "depth": 0, "author": "FDSTCKS", "replies": []}, {"selftext": "Driver issues aren't unique to AMD. I've definitely has weird problems with Nvidia cards too. I never get fanboys who claim that Nvidia cards don't have problems. They definitely do. I've had my current RX 5700 since launch, and the only issue I has turned out to be from Windows itself, not the card.", "ups": 2, "depth": 0, "author": "RippiHunti", "replies": [{"selftext": "It is possible both have problems, but one has more problems than the other? Just a thought.", "ups": 2, "depth": 1, "author": "Podalirius", "replies": []}]}, {"selftext": "This is generally how it goes...\n\nWhen a game underperforms on an AMD GPU, the Nvidia community yells \"hurr-durr, BAD DRIVERS\"\n\nWhen a game underperforms on an Nvidia GPU (Like say...Spiderman), it's the developers being lazy, it's AMD putting secret code in the game to slow down Nvidia GPUs, it's Lion-O slashing power cables with the Sword of Omens because the Eye of Thundera is red and obviously is on team AMD... Anything but an issue on the Nvidia side.\n\nIt's NEVER that Nvidia drivers aren't up to snuff... Which is just Nvidia Mindshare doing their thing. Nvidia releases borked drivers all the time, people just hate admitting it. \n\nThe big difference is that Nvidia is usually MUCH faster at fixing busted drivers when they do get released.\n\nBy and large, AMD drivers are very stable, and have been for some time. Even the 5700XT driver issue has more or less been debunked as user error, you can research it if you don't believe me.\n\nI've been using AMD GPUs since the ATI days and have never once had any serious issues. I've also owned one Nvidia GPU in there (GTX 970), and it performed great as well.\n\nI dunno... people just like to wave the banner I guess.", "ups": 1, "depth": 0, "author": "Murky-Smoke", "replies": []}, {"selftext": "My buddys experience: 7900XTX crashing in Starfield sometimes.\nDrivertimeouts\n\nwatched yesterday \"Benchmark Boy\" Youtube Livestream. Starfield crashing with 7900XT to. Here the Livestream with the Crash: https://www.youtube.com/watch?v=0_zbDZ_4FaA&amp;t=1037s\n\nmy experience with 5700 Release: totally mess. Problems everywhere. Multiscreen bugs, stripes, flashing, drivertimeouts, WoW Classic crashing over and over...\nAnd that was my last experience where i run away from AMD\n\nOur shop doesnt build PCs with AMD GPUs anymore, to many user contacted us with crash-problems.\n\nDont let people tell you \"you have to do a clean install with DDU\" the problems still occurs with Cleaninstalls. I dont know why, but AMD GPUs are like playing lotterie it works or it works not. Maybe AMD GPUs are much more sensitiv with other hardware.\n\nAnd no, drivers arent better. Watch the official AMD Forum. High Wattage Multiscreen bugs and timeouts are still a problem!\n\nAnother Problem that you will find only @AMD Team: User talking about Driverversions.\n\"yeah i have to rollback because performance wasnt good with the new one. my game crash with this or that driver....\"\nNvidia: just start Experience and allways using the latest update...\n\nNew AMD Bug reports: AMD Software Settings freeze after ~20-30 sec\n\ni dont know what the AMD Driverteam is doing...", "ups": 1, "depth": 0, "author": "AUT_Zachal", "replies": [{"selftext": "Pretty bold thing to say on the AMD subreddit, albeit true and one of the reasons I avoid their GPUs, although I might try their CPU when I decide to upgrade.", "ups": 1, "depth": 1, "author": "Sharpman85", "replies": []}]}, {"selftext": "People get paid to S$%\\^ on AMD.", "ups": 2, "depth": 0, "author": "KlutzyFeed9686", "replies": [{"selftext": "where? how? sign me up bro, if they're paying me ill do it", "ups": 3, "depth": 1, "author": "BFCE", "replies": []}]}, {"selftext": "Guerrilla marketing by Nvidia that was effective and now gets parroted by its users.", "ups": 1, "depth": 0, "author": "uniq_username", "replies": []}, {"selftext": "Drivers were shit during vega, after that they improved.\n\nAnd are today super stable.", "ups": 1, "depth": 0, "author": "Evonos", "replies": [{"selftext": "They were awful in RDNA1 as well. RDNA2 was easily the best launch for driver stability for AMD then they regressed somewhat with RDNA3, with the notable bugs in VR performance and idle power management.", "ups": 4, "depth": 1, "author": "Beautiful_Ninja", "replies": [{"selftext": "Besides the video decoder in the initial driver release my 5700xt has been basically flawless since i got it.\n\nThere were issues, but that was my Asus B450 board thinking it could PCIe 4.0 (it could not), and my friend with the same card needed to do a bios update of his motherboard (MSI B350) before the 5700xt behaved propertly (HWinfo64 showed PCIe transmission errors prior to the bios update).\n\nA lot of other issues were power supply related (it has pretty high piek loads), and ram overclocking that weren't quit stable.\n\nBut a number of those showed up as 'driver timeout' which often just means the driver lost contact with the GPU. not that the driver caused a problem.\n\nBeing the first PCIe 4.0 card widely available, and its high piek power demand were probably the largest contributors to the RDNA1 issues.", "ups": 3, "depth": 2, "author": "The_Countess", "replies": [{"selftext": "[deleted]", "ups": 3, "depth": 3, "author": "[deleted]", "replies": [{"selftext": "The issues the 5700 XT had with drivers was leading users to replace components from their rigs. Issues they didn't have until swapping their GPU, but they were adamant it wasn't the new part.\n\nThen AMD slowly fixes most of the issues and users who did replace PSUs, monitor cables, RAM sticks, and etc sure must have felt \"AMD drivers are fine\".\n\nThat wasn't even 5 years ago and people still acting like \"LOL that was 60 years ago, they've been fine since 1963.\"\n\nIt's bonkers.", "ups": 1, "depth": 4, "author": "railven", "replies": [{"selftext": "I think we're in agreement so here's an anecdote:\n\nWhen I got my 5700xt it was a whole new PC and hoo boy it was a total POS for the first year. The only thing I replaced, until they improved the drivers to the point it no longer crashed multiple times a day, was the mobo but that was for a USB issue (which was actually the mobo's fault.)\n\nYeah. Drivers definitely weren't great for a damn while. Still scared of getting RDNA3 ngl.", "ups": 0, "depth": 5, "author": "uranium4breakfast", "replies": []}]}]}]}]}]}, {"selftext": "Still waiting for Vulcan fix on bg3. Other than that, everything seems to work nicely", "ups": 1, "depth": 0, "author": "Medical-Tomorrow7727", "replies": []}, {"selftext": "The drivers are fine these days. Nvidia isn\u2019t perfect with their drivers either people just ignore those issues for some reason. It\u2019s just fanboys. I\u2019ve been using both nvidia and AMD and my experience with both are roughly the same.", "ups": 1, "depth": 0, "author": "riu_jollux", "replies": []}, {"selftext": "Now try playing The Crew Motorfest", "ups": 1, "depth": 0, "author": "damastaGR", "replies": []}, {"selftext": "AV1 video decoding is still a crapshoot on every driver I've tried for my 6700XT. It's an issue for me watching videos in VRchat, they're a complete stuttery mess and unwatchable, even on beta drivers that were supposed to improve it. YouTube is now also widely serving AV1 and often I will just get a black video player with only audio.\n\nI'm able to manually override it in VRchat to use software decoding (cpu only) but that makes the videos really dark for some reason. YouTube requires me to force VP9 codec for it to be usable.\n\nNone of this was an issue on my RTX 2060 and I have run DDU numerous times and tried several different driver versions. The card is basically new at this point (&lt;2mos) and I doubt it's broken hardware because many others report the same issues.", "ups": 1, "depth": 0, "author": "tenten8401", "replies": []}, {"selftext": "&gt; So now I ask, are stuttering and driver issues a problem on lower end amd gpus or are those just dead arguments from nvidia fanboys that are stuck in 2018?\n\nThey're not dead arguments, but some are definitely overblown, but there are some very valid driver issues in history that made AMD drivers worse. RDNA1 black screen issues for instance were very hit or miss and were certainly a reason you could recommend NVIDIA over an AMD card, fixed now, but valid at the time.", "ups": 1, "depth": 0, "author": "KARMAAACS", "replies": []}, {"selftext": "I've had plenty of crashes and software related issues with my RX 6800, I had issues before with my R9 390, and the same issues with an HD 6000 series before that.\n\nAs someone who's been doing this for close to 20 years, and who's owned plenty of both Nvidia and AMD cards, the driver support is a night and day difference, even by today's standards. \n\nWith that said, I still prefer AMD for the raw power per price.", "ups": 1, "depth": 0, "author": "Albake21", "replies": []}, {"selftext": "Is this one o those AI generated posts I've heard so much about? wtf is this lmao?", "ups": 1, "depth": 0, "author": "Jon-Slow", "replies": []}, {"selftext": "I bought an XTX at release to complete my first all AMD build.. after the 2nd week after release, they were still having hotspot issues, AMD Adrenalin had options that didn't work or would straight up crash if you turned on and I am quite certain there were rare cases of the dies cracking.\n\nI waited 10 days and returned mine. When the $1600 CAD GPU you bought is posting the same Cyberpunk benchmark scores as your old 1080ti, there's a problem. Really got under my skin because the box says right on it, \"8k capable\".. Yeah, right.. 4k@120hz was impossible.", "ups": 1, "depth": 0, "author": "HotDangggg", "replies": []}, {"selftext": "Nvidia fanboys constantly harp on Driver issues, which even HUB boys day is a non issue, outside of 7800xt.  Which possibly due to new release. \n\nThey will tell you about AI features of DLSS image sharpening, upscaling and Frame generation.  None of which should be used if you want to be competitive gamer.  DLSS is good for browsing games, rpgs.  \n\nI\u2019ve owned a Sapphire 7900XT for 4 months. I\u2019ve not experienced any driver issues, any desire for upscaling, etc.  NOR do I have any desire to turn on RT.  I play at 1440p high or ultra.", "ups": 1, "depth": 0, "author": "Active_Club3487", "replies": []}, {"selftext": "I can't remember the source right now but someone went through the known driver issues for NVIDIA and AMD cataloging them by severity. NVIDIA on average had more driver issues and it took longer on average to fix, while AMD had fewer and they got fixed faster, but the driver issues that AMD had were on average more severe.  \n\n\nIt takes many years for reputations to change, people will rarely make a point of drivers working.", "ups": 1, "depth": 0, "author": "Peach-555", "replies": []}, {"selftext": "The truth is driver issues will happen on any GPU regardless of brand.  The issue could be anything from a bad install, component or system incompatibility, or just bad programming of the driver itself.  It\u2019s not strictly and AMD issue and those that might claim it is are stuck in 2018 where a lot of driver issues then were due to poor development.  \n\nI think most of the issues now seem to be either a bad install where some of the information may not have installed correctly or other hardware compatibility issues that will need a patch to repair.  My personal opinion is never be the person in a rush for the next greatest update.  Let other people be the Guinea pigs and wait a couple weeks, especially if you aren\u2019t experiencing any performance issues already.\n\nAs for driver issues or known stuttering, I have a 7900 XT and have not run into a single issue yet, but that has been my experience.  Take individual experience with a grain of salt, I have had friends have issues with NVIDIA, but they can\u2019t be taken as representation of the whole.", "ups": 1, "depth": 0, "author": "Exact-Explanation524", "replies": []}, {"selftext": "It's a myth. Especially for gaming, AMD has rock solid drivers and honestly a better UI and UX.", "ups": -2, "depth": 0, "author": "ZaadKanon69", "replies": []}, {"selftext": "As someone who has bought GPUs for nearly thirty years I have found no significant difference in \"drivers\" or latency or anything like that between manufacturers.\n\nThat's FUD designed to keep people from buying AMD GPUs.", "ups": -1, "depth": 0, "author": "Mygaffer", "replies": []}, {"selftext": "Drivers seem fine now. I tried out AMD in 2018, with a VEGA 56 (going from a 1070) and It was a horrible experience but now I've just switched from a 3080 to 7900XTX and it's much better. However I do think NVIDIA's software is better still, despite forcing you to log on. ShadowPlay can atleast do HDR...", "ups": 0, "depth": 0, "author": "elliebellyberry", "replies": []}, {"selftext": "I used an RX570 for over 3 years with no driver timeouts and on day 2 after upgrading to an RX7600 I got multiple. I heard reports of driver timeouts on 7000 series prior to purchase and brushed them off.\n\nIdk if it was unstable default boost clocks when it was boosting higher due to cooler ambient temps that day, extremely slow shader compilation, me disabling MPO as a precaution or what but it seemed to be bad one day and fine the next.\n\nNever had to disable anything on the rx570 and no one should have to disable hardware acceleration in all of their apps or disable MPO to get a stable GPU.", "ups": 0, "depth": 0, "author": "AbyssalNoob7", "replies": []}, {"selftext": "I owned a 5700XT in 2020 and it was awful for frequent system locking crashes. Card ran hot as well (although within design limits according to AMD support). The last straw was when it crashed during a live stream I was doing and I replaced it with a 2080ti (30 series all out of stock during the pandemic). I had no issues with the 2080ti so stuck with NVIDIA when I upgraded. My experience was that no matter how many patches AMD did, the driver issues were never really resolved. My friend also had a 5700XT and experienced the same issues as me. What I would say is that the latest AMD cards represent amazing value and if you\u2019re not getting driver issues they will be pretty good. Just from my personal experience I have lost trust in AMD, so I\u2019m sticking with NVIDIA. I know NVIDIA are overpriced, but I can\u2019t bring myself to spend a large amount of money with AMD when I\u2019m not certain I will have a working card.", "ups": 0, "depth": 0, "author": "Jon_Padders", "replies": []}, {"selftext": "I guess I'll be the contrarian here. My 6700xt worked with no issues. However, since upgrading to a 7900xtx I have been dealing with:\n\nHigh idle power draw (~200w for a long time, went down to 150 somewhat recently)\nUnable to use instant replay (causes crashes every 10 minutes or so)\nRandom crashes related to discord (it's always the game and discord that will crash, relaunching game but leaving discord closed it will work fine)\nStability problems randomly with overlays (I think this might be related to the last one? Discord crashes about 80% less when I have the overlays on discord disabled)\nMax fps dropping after a few minutes if I don't tab out and back in to a game (this one matters much less, I've only noticed it in valorant where uncapped I can get ~7-800 fps but it'll go in a few minutes to ~400 max. Not temp causing this one, idk what else could)\nOnly twice so far, midgame the screen has blacked out, gpu fans are spinning overdrive, and when I power cycle PC the drivers are completely borked. Had to ddu and uninstall them both times)\n\nSucks as ive actually only ever had AMD parts for 3 gens now, minus a janky 1080ti Morpheus mod. This is the first time one's been such a pain in the ass tho\n\nEdit: I just came back home to find my room warm, gpu running full tilt for who knows how long. Guess who's reinstalling drivers tonight \ud83d\ude2d", "ups": 0, "depth": 0, "author": "21dayjac", "replies": []}, {"selftext": "Short story: Radeon and GeForce are now on par again.  \nLong story: Back in 2000 up 2005 both sides were the same.  \nI had a FX5200 and until a specific version came out, games crashed  \nleft and right.  \nRadeon did a good job too, like with 9800 Pro.  \nBut after that, shit happened.  \nDrivers had various issues, CCC aka Catalyst, more like Chaos, Control  \nCenter did not work or reverted the settings now and than.  \nBSODs were more common, high CPU overhead around 2014 had been  \ndiscovered and many things more that I don't remember.  \nSuddenly 2015 the drivers improved, CCC was still around, but  \nBenchmarks with a Radeon were not that bad anymore.  \nWith that a new trend started, 2016 the RX 400 series released and  \nafter that the CCC got replaced.\nRadeon GPUs were again good enough to recommend them,  \nonly high end models were missing.  \nWell now Vega appeared and all problems before 2016 started to  \nappear. Bad drivers, bugs and low(er) performance were back.  \nRX5000 series fixed quit a bit of that, but with RX6000 AMD reached  \nthe status of 2018 again.", "ups": -1, "depth": 0, "author": "gtrash81", "replies": []}, {"selftext": "Nvidia fanboys loves to spread fake issues.", "ups": -1, "depth": 0, "author": "amit1234455", "replies": [{"selftext": "nVidia fanboys just go all \u201cdurrrrrr nVidia scores .1% better FPS so I blindly buy all their shit durrr\u201d.", "ups": 0, "depth": 1, "author": "Faceh0le", "replies": []}]}, {"selftext": "Those \u201cdriver issues\u201d people are just Nvidia fanboys with little pee-pees.", "ups": -1, "depth": 0, "author": "lead_pipe23", "replies": []}, {"selftext": "Fairly typical bet anyone with an Nvidia GPU knows nothing about PC, hardware, tech, software etc. It may not be universally true, but its damn close.", "ups": -1, "depth": 0, "author": "LordTism", "replies": []}, {"selftext": "Yes. Current AMD cards on the latest drivers actually have less issues than Nvidia. AMD did a lot of damage to their brand during Vega and RX 5000 unfortunately.", "ups": -1, "depth": 0, "author": "GoofyAhPPHead", "replies": []}, {"selftext": "IT's always been a myth.... in fact, historically, Nvidia's drivers have been and continue to be one of the primary factors of being a caveat.\n\nWhile both companies have similar rate of issues on the driver to driver package, the fact that nvidia has had to pull the most graphics drivers, even whql certified due to catastrophic problems can't be glossed over. The last time AMD had to legitimately pull a driver, was catalyst 7.3, and it wasn't even the driver's fault, it was just a bad timing issue where a windows update launched the same day interacted with the new drivers in such a manner causing a BSOD loop, and the solution to the problem was simply to delay startup of the driver.\n\nI mean shit, even today there are still people constantly dealing with or asking about which nvidia driver is best suited for which games they wish to play because one thing nvidia's VERY good at, is having wildly unpredictable outcomes driver to driver, specially when games no longer are in the spotlight. It's kind of funny really, because while AMD's drivers tend to be a bit lacking in maturity at launch, nvidia's usually are top performance usually masking over the fact that there are underlying issues people aren't getting slapped with, the problem however is due to how excessively optimised nvidia focuses on specific games, others tend to suffer, and like i said, once things age, performance and problems start to arise later on. IT's not so much as pure finewine for amd's drivers as they mature, it's also nvidia's slumping with older titles as they continue to micromanage everything with game specific optimisations neglecting to fully test prior titles that such optimisation may break or hinder.\n\nWhat's weird is that the memes that existed painting nvidia badly all constantly seem to be forgotten where as inaccurate and frankly uncharacteristic of ati and amd, get labeled with the problems nvidia is grossly more guilty of.\n\nI mean shit, Nvidia used to call their drivers detonators, and the going joke was that you didn't even have to alter the name of the drivers, you just never knew if the drivers were going to detonate your system or not.\n\nNvidia the first with horrible blowers and hottest temperatures, how many leaf blower memes were there back then? Nvidia's FX5000 series is a great example of a catastrophic failure of a product, but due to the incompetence of customers, still sold like hot cakes. You can't fix stupid, though nvidia's confidence in screwing over customers while customers blindly jumped for joy doing it was mostly set in stone as a real possibility the moment so many geforce 4 mx cards sold.. and even with so many people that later figured out they got screwed, they still ended up giving more money to nvidia anyways, talk about intentionally paying for a monopoly.\n\nA common hilarious occurrence throughout the years as well was and is the fact that in the vast majority of cases, when a problem arises. In the event you have a user with an nvidia gpu, the overwhelming majority of people will completely disregard the gpu or nvidia drivers as being at fault, they will spend copious amounts of time, and they'll be given nothing but assurances that it's always something other than nvidia's gpu or drivers. Thus all the diagnostics and process of eliminations for finding the cause of a problem if one crops up, is focused entirely on anything but nvidia, they will replace a cpu or ram or mainboard or psu or whatever and blame those things BEFORE anything nvidia related right off the hop, no matter what. Meanwhile, for some bizarre reason, throw an ati/amd gpu at users and at the first sign of any level of instability or problem, it's IMMEDIATELY the shitty drivers.... every fucking time. Could be 1 million people running something and maybe 1 or a few run into a problem, and there just isn't any basic level effort or logic put into determining a cause, it's \"the drivers are shit\"... You can't write a better joke really. It's bizarre to witness. As someone that has been a technician and even working with other technicians, as well as building and assembling, doing support calls and on site servicing among numerous other tasks involved, it's utterly amazing how quick people are to blame amd for even the most mundane problem, EVEN if the problem is proven to affect nvidia as well, perhaps even worse.... and yet nvidia doesn't even get a mention for said problem, or it's overlooked as a bug that isn't at all a fault of nvidia.\n\nTake the most recent starfield issue about the sun. Look at the numerous news articles that EXPLICITLY state that it's an AMD only issue. I mean for fucks sakes, look at the number of youtube videos, some of which are getting millions of views, or tiktoks or whatever other forms of media that amplify this \"news\" story. Even when there are plenty of people that have nvidia gpus commenting and stating that \"uh, this also happens to me\"... NONE of these articles or videos or whatever are EVER changed or modified to more accurately state what reality is, it will remain an exclusively amd only issue. What does that do to the google searches or how people view amd even years later when the endless amount of articles never get revised and keep piling up CLEARLY lying about them being affected exclusively or that they may be the cause. I'm sure the starfield one in a few years will be in the vast majority of people's minds, remembered as and amd only issue and probably some stupid internet arguments that will use it as an example of why \"amd drivers bad\".\n\nLike i said, it's all a fucking joke... and you wouldn't believe how much even i resonated with that statement from \"the comedian\" from a certain comic/movie.", "ups": -1, "depth": 0, "author": "DHJudas", "replies": []}, {"selftext": "With so many combinations of hardware for CPU, ram, mboard, monitor, gpu etc, along with many different games. What this means in reality, is that somebody somewhere in the world is going to experience problems with their gpu that somebody with a different set of hardware, or even the same set but a different product line, will never see.\n\nIt's akin to reading a google review about a rubbish restaurant after going there and finding out that everybody hates the restaurant but you were pleasantly surprised by the food and the service. \n\nI'm not talking about the obvious userwide issues but those rarer ones. I could be wrong though so take this with a pinch of salt :)", "ups": 1, "depth": 0, "author": "Xercen", "replies": []}, {"selftext": "I gave my brother my RX 580 and it's been crashing in all the game for a while", "ups": 1, "depth": 0, "author": "lex_koal", "replies": []}, {"selftext": "Not a driver issue but I bought 7900XT from xfx and after a month it\u2019s junction temps rose to 100-110. I searched everywhere but there was no solution. Repaste it and it comes back 2 months after. I see that problems discussion open every month. \nI had gtx 1070 before that for 7 years and had no issue with it all those time. \nI had amd 2xx something before that and I had to tinker with it everyday to get it to work. \nI don\u2019t like it when I pay 1000$ for a gpu and have to work on it to make it playable.\nSold my amd bought 4080 instead.", "ups": 1, "depth": 0, "author": "grk213", "replies": []}, {"selftext": "I had a MSI 7900XTX and I didn\u2019t have a single issue with it. No heating problems no driver issues no gaming issues. I left the glass off my case and my cat used the GPU like a ducking diving board. It broke. I ended up getting a 4090. Said if I had to buy another one I might as well just go all out. Having more problems with my 4090 then I ever did the XTX. But that\u2019s more of a Gigabyte problem. They\u2019re making good in the end though.", "ups": 1, "depth": 0, "author": "Niifty_AF", "replies": []}, {"selftext": "Upgraded a 1060 6GB to a 6600 XT and never had any issues. I recently upgraded to a 6800 XT and my driver crashed frequently. In the end it was the RAM resp. the XMP profile causing it. AMD seems to be very sensitive in that regard. After setting the timings manually (and a bit slower) everything works like a charm now.", "ups": 1, "depth": 0, "author": "R4zr5", "replies": []}, {"selftext": "I had a 3070ti and switch to a 7900xtx, i just installed it, downloaded the amd drivers and started playing cyberpunk, cod warzone, league of legends and etc. never had any issues. I even thought the same \"why do people cry about the old nvidia drivers\".\n\nThen the new tarkov wipe started a few weeks ago and started playing again, it run really well but whenever i started shooting my gun the game froze for like 2-3 seconds everytime. I just run DDU and clicked to uninstall every nvidia driver, restarted the PC and the game is now flawless. (Well, as much flawless as tarkov can be :D)  \nThis was my experience with the big GREEN to RED team transition.", "ups": 1, "depth": 0, "author": "Fredas25", "replies": []}, {"selftext": "Just out of curiosity, are you on an AMD cpu as well?", "ups": 1, "depth": 0, "author": "Pinsir929", "replies": [{"selftext": "Yes, 7800x3d", "ups": 2, "depth": 1, "author": "MADcrft", "replies": []}]}, {"selftext": "I\u2019ve had consistent issues with my sapphire pulse nitro whatever RX 5700 XT. The performance for what I paid is too good to give it up, but it\u2019s been a struggle and I\u2019m not very impressed by the anything other than the hardware. The Radeon software / drivers being in a constant battle with windows isn\u2019t fun either.", "ups": 1, "depth": 0, "author": "viberider", "replies": []}]}
{"post": {"title": "AMD Instant Replay saving separate tracked audio in separate m4a file.", "subreddit": "Amd", "selftext": "I recently purchased a 7800XT about a week ago, and although so far the performance has been great, as well as Adrenaline Software being a massive upgrade over GeForce Experience, I'm disappointed in one feature that their recording feature as. \n\nAs opposed to ShadowPlay, where clips with separate tracked audio are saved under the same mp4 file, AMD's recording software saves clips that have separate tracked audio into 2 files, an mp4 file and an m4a file. So 2 different files are saved for one clip.\n\nThis might seem like a small issue, however for my use case, that will completely screw up my workflow for video editing. I'd have to match the m4a file with the mp4 file for hundreds of clips spanning over many games.  \n\nI know this isn't the case for everyone, however, this is the deal breaker for me using the Instant Replay feature. If they'd simply allow the user to toggle whether they'd want separate files, this would be a non-issue. \n\nFor now though, I will simply be using OBS. Please fix, AMD. \n\n&amp;#x200B;", "ups": 13, "permalink": "/r/Amd/comments/16jpol0/amd_instant_replay_saving_separate_tracked_audio/", "num_comments": 17}, "replies": [{"selftext": "can't you just remux them?", "ups": 6, "depth": 0, "author": "nwgat", "replies": [{"selftext": "I could, but the issue of my workflow getting bogged down wouldn't really change. It'd just be different. I prefer just for the separate audio to be under 1 mp4 file natively.", "ups": 0, "depth": 1, "author": "randomstuff-508", "replies": []}]}, {"selftext": "relive is not aimed at catering to complex, advanced workflows.\n\nthat's really why you should stick with OBS *studio* . and your particular gripe with relive is by far not the only reason to use a more professional application.\n\n\nthink of relive as wordpad in windows. you can write letters, change font, print, etc. but it will never replace a full office suite like libreoffice or msoffice with all possibilities and fine grained options.", "ups": 3, "depth": 0, "author": "Portbragger2", "replies": [{"selftext": "&gt;relive is not aimed at catering to complex, advanced workflows\n\nThis isn't a complex workflow, though. This is simply 2 files being saved with one clip as opposed to 1 file.  \n\n&gt;that's really why you should stick with OBS studio . and your particular gripe with relive is by far not the only reason to use a more professional application.\n\nThis quite literally is the only reason why I'm not using ReLive. I would prefer not to use OBS. Everything worked fine with shadowplay when I had Nvidia because it was all I needed.", "ups": 1, "depth": 1, "author": "randomstuff-508", "replies": [{"selftext": "you are going in circles.\n\nhaving way fewer customization options for file creation (but also in general) simply means relive is not the tool for your individual project's scope. being stubborn won't help at that. \n\nyou can continue throwing half a tamper tantrum or - the wiser choice on all fronts btw - fully familiarize urself with obs and seamlessly embed it into your creative process.\n\ngood luck &amp; keep ur chin up!", "ups": 3, "depth": 2, "author": "Portbragger2", "replies": [{"selftext": "Bro wtf are you talking about? I'm not going in circles, I'm complaining about the fact that ReLive doesn't give you the option to have separate tracked audio save to 1 mp4 file. \n\nIf anything, you're the one who can't understand the fact that I'd much rather use ReLive, but because of their file structure, I'm using OBS. \n\nYour reply was a whole lot of nothing, since it's everything that I've already addressed.", "ups": 1, "depth": 3, "author": "randomstuff-508", "replies": [{"selftext": "u are not addressing the fact that obs is the more powerful more configurable and most deployed streaming and recording tool which also happens to be suitable for your personal use case. nothing else beats it. not even the paid  'xsplit'.\n\nrelive wont have these adjustments you wish for implemented because it is supposed to be an easy to use ootb experience with not many features that could confuse a layman user.\n\nthat makes ur post here nothing more than a sad n pitiful rant which wont improve your situation at all.\n\nyou see... nobody cares... and i'm just here to help you understand.\nnow best to calm down, relax...and get the priorities straight. how about recording those 1000s of videos in obs to earn that sweet youtube monies (sic!) for starters?", "ups": 2, "depth": 4, "author": "Portbragger2", "replies": [{"selftext": "Nice troll, my guy\n\nif you're not trolling, then I feel bad for you.", "ups": 1, "depth": 5, "author": "randomstuff-508", "replies": [{"selftext": "lmk when u're done with your 1000s of vids. then we talk.", "ups": 1, "depth": 6, "author": "Portbragger2", "replies": []}]}]}]}]}]}]}, {"selftext": "record &amp; stream -&gt; settings -&gt; recording -&gt; separate microphone track \\[disabled\\]", "ups": 8, "depth": 0, "author": "Clifton_7", "replies": [{"selftext": "That's not the issue. I WANT the audio tracks to be separate, so then I can adjust my mic volume independently from the game audio. I, however, want the audio to be saved in the same mp4 folder. \n\nOBS does this. Shadowplay does this. Why can't ReLive do this?", "ups": 3, "depth": 1, "author": "randomstuff-508", "replies": [{"selftext": "Relive does this for me, all the files and separate mic tracks are all saved in the same folder.", "ups": 2, "depth": 2, "author": "Faceh0le", "replies": [{"selftext": "Nah that's not the issue, that is actually one of my favorite things about ReLive, that it recognizes the game and makes its own folder for it. \n\nThe issue lies in that ReLive makes 2 separate files, an mp4 and m4a file. 2 files for 1 clip.", "ups": 1, "depth": 3, "author": "randomstuff-508", "replies": [{"selftext": "Isn\u2019t that normal of you have enabled separate track for mic audio?", "ups": 2, "depth": 4, "author": "Faceh0le", "replies": []}]}]}, {"selftext": "Why not just play with obs on then? It sucks but gotta do what you gotta do ig", "ups": 2, "depth": 2, "author": "Goldenflame89", "replies": [{"selftext": "I am. Read last the paragraph of the post.", "ups": 0, "depth": 3, "author": "randomstuff-508", "replies": []}]}]}]}, {"selftext": "It's nonsense, but they probably won't change.", "ups": 1, "depth": 0, "author": "extrapower99", "replies": []}]}
{"post": {"title": "[GPU] ASUS TUF RX 7800 XT in white", "subreddit": "Amd", "selftext": "(Directly from the manufacturer)", "ups": 16, "permalink": "/r/Amd/comments/16jha96/gpu_asus_tuf_rx_7800_xt_in_white/", "num_comments": 15}, "replies": [{"selftext": "Just got mine.", "ups": 2, "depth": 0, "author": "catshaped_cerealbowl", "replies": [{"selftext": "Did they send you a starfield code?", "ups": 1, "depth": 1, "author": "nimabears", "replies": [{"selftext": "I don\u2019t think it\u2019s included if you buy it through here, but I had already received one from previously buying a 6800xt. Which I returned when the 7800 xt came out.", "ups": 1, "depth": 2, "author": "catshaped_cerealbowl", "replies": [{"selftext": "When you returned the 6800xt, did you had to return the starfield code? Or like charged you or smth?", "ups": 1, "depth": 3, "author": "MainAd6132", "replies": [{"selftext": "Nope, kept the game and got my full refund.", "ups": 1, "depth": 4, "author": "catshaped_cerealbowl", "replies": [{"selftext": "I already bought this 7800 xt this morning and i\u2019ll keep it, but i\u2019ll just buy and return an amd gpu from a retailer to get the starfield game. (totally not abusing it)", "ups": 1, "depth": 5, "author": "MainAd6132", "replies": [{"selftext": "I did my return with newegg if you want use (abuse) them", "ups": 0, "depth": 6, "author": "catshaped_cerealbowl", "replies": [{"selftext": "kk thx for the info that I would never use, but hypothetically, if i did do that, it does sound good :)", "ups": 1, "depth": 7, "author": "MainAd6132", "replies": []}]}]}]}]}]}]}]}, {"selftext": "I'm. So. Mad. I WANT THIS SO BAD \ud83d\ude2d\ud83d\ude2d its $10 more than I paid for my 6800XT too", "ups": 2, "depth": 0, "author": "majorsorbet2point0", "replies": []}, {"selftext": "Have had it set up for 2 days and it runs great", "ups": 1, "depth": 0, "author": "SirFlipalot", "replies": []}, {"selftext": "It looks so good holyyy", "ups": 1, "depth": 0, "author": "imastrangeone", "replies": []}, {"selftext": "did anyone else have their order cancelled? placed my order yesterday and it was cancelled today but didnt receive any specific details about why they did so", "ups": 1, "depth": 0, "author": "AdPurple13", "replies": [{"selftext": "i ordered mine on friday morning, got it sunday afternoon with no problems (i live in the u.s. west coast)", "ups": 1, "depth": 1, "author": "MainAd6132", "replies": [{"selftext": "Did it come with Starfield?", "ups": 1, "depth": 2, "author": "mastrofdizastr", "replies": [{"selftext": "Nope", "ups": 1, "depth": 3, "author": "MainAd6132", "replies": []}]}]}]}]}
{"post": {"title": "Does Amd have software that recommends game settings like in GeForce Experience?", "subreddit": "Amd", "selftext": "I'm most likely going to swap to Amd for my next gpu and I enjoy using GeForce experiences suggested game settings feature as a starting point when adjusting my game settings. \n\nDoes Amd offer anything similar?\n\nThank you!", "ups": 10, "permalink": "/r/Amd/comments/16jev95/does_amd_have_software_that_recommends_game/", "num_comments": 28}, "replies": [{"selftext": "GFE game optimization is shit and its good for setting wrong values, just set game settings yourself, its always better.", "ups": 75, "depth": 0, "author": "TheFather__", "replies": [{"selftext": "This. It's a very unnecessary piece of software.", "ups": 23, "depth": 1, "author": "faverodefavero", "replies": [{"selftext": "When I had an Nvidia GPU, it would just set all the settings to whatever the highest was. It was basically worthless because almost all games would auto detect best settings even when the game was demanding.", "ups": 2, "depth": 2, "author": "sdcar1985", "replies": []}]}, {"selftext": "If it actually translated the settings to a target FPS it would be amazing but the sliding bar with no numbers is useless. \n\nDefinitely better to just start with a preset that gives you close to what you want and tweak individual settings up or down from there. \n\nI usually start on medium and bump up things like textures and shadows while keeping particle effects and such on low/ medium. Rarely worth it.", "ups": 3, "depth": 1, "author": "Rizenstrom", "replies": []}, {"selftext": "It\u2019s a good start to get a baseline and adjust everything else manually", "ups": 4, "depth": 1, "author": "Sharpman85", "replies": []}, {"selftext": "It's good for people who are new and don't know what settings mean.", "ups": 3, "depth": 1, "author": "theoutsider95", "replies": [{"selftext": "Or just don\u2019t give a fuck and want to play, I used to use those cuz I didn\u2019t want to spend time tweaking settings and looking at fps.", "ups": 6, "depth": 2, "author": "Pazret", "replies": []}, {"selftext": "Well then that's a good time to learn.", "ups": 1, "depth": 2, "author": "mandoxian", "replies": []}]}, {"selftext": "Nobody here knows how to adjust settings anymore. GFE optimization is way better than what 95% of people here can do.", "ups": 4, "depth": 1, "author": "Hooligans_", "replies": [{"selftext": "Thats not true, manually setting is way better, it also let you set what you want like more fps or better visual.\n\nGFE makes so bad choices in some games\n\nThe only good thing about GFE is built in game recording that you can enable", "ups": 7, "depth": 2, "author": "Vuruna-1990", "replies": [{"selftext": "I agree it's way better. I've been doing it for decades. I'm saying 95% of PC gamers on Reddit don't know how. They put everything on Ultra and complain about lazy devs.", "ups": 0, "depth": 3, "author": "Hooligans_", "replies": []}]}, {"selftext": "Doesn't GFE just set it based on your slider preference, it will just set everything to max for quality and low for performance, it doesn't tweak settings, just a max, med, low option.", "ups": 1, "depth": 2, "author": "nru3", "replies": []}]}]}, {"selftext": "Yes they do, go to the gaming tab and select the game you want, or run the game that you want. They'll show you a graph of what is going on within the game. They'll offer suggestions on trying to increase FPS based on what you have, and or upgrades if you don't truly meet requirements.", "ups": 29, "depth": 0, "author": "urlond", "replies": [{"selftext": "Advisor is just that, advises, doesn't 'do'. It would be nice if when it makes a suggestion  it actually reads the settings the game has, and can actually change those game settings for you should you choose to use the advice the Advisor has given.\n\nThat is what Experience does. Boot game one, Experience then reads the game settings, and it can be changed to recommended in one click. It is pretty much the only thing I liked about Experience software before I jumped to AMD, and probably the only thing I miss.\n\nExperience wasn't perfect, but it at least set up a basis that you could easily tweak afterwards.", "ups": 9, "depth": 1, "author": "Richo262", "replies": []}]}, {"selftext": "There was an app called Raptr which AMD helped to go on. But it was shut down a while back.", "ups": 8, "depth": 0, "author": "vitafinito", "replies": []}, {"selftext": "Kindve, amd Adrenalin will tell you to adjust settings and what to possibly adjust if you aren't getting crazy high frame rates. But if you limit your frames then that information isn't accurate. \n\nHowever AMD adrenaline is awesome I prefer it over it's nvidia counterpart", "ups": 8, "depth": 0, "author": "soisause", "replies": []}, {"selftext": "No, but I personally would not blindly trust third party recommendations and would just dial in settings manually if needed.\n\nAlso, if you still want recommendations on good settings, channels like Digital Foundry and to a lesser extent Hardware Unboxed and Gamers Nexus publish videos with recommended settings breakdown for biggest games.", "ups": 6, "depth": 0, "author": "Mungojerrie86", "replies": [{"selftext": "There's also a lesser known YouTuber called BenchmarKing that does similar optimization videos.", "ups": 3, "depth": 1, "author": "sdcar1985", "replies": []}, {"selftext": "Exactly. A huge part of PC Gaming is having many different graphical options for each game and being able to tweak them yourself to fit your own tastes and hardware capabilities.", "ups": 2, "depth": 1, "author": "faverodefavero", "replies": []}]}, {"selftext": "Not that I'm aware of.", "ups": 2, "depth": 0, "author": "Star_king12", "replies": []}, {"selftext": "It does, but it's more vague and it's just as worthless.", "ups": 2, "depth": 0, "author": "TheOctavariumTheory", "replies": []}, {"selftext": "No , Amd had this once via  a 3rd party ( but it worked for nvidia too ) named Raptr.\n\n[https://en.wikipedia.org/wiki/Raptr](https://en.wikipedia.org/wiki/Raptr)\n\nFeatures like Recording and stuff was also from Raptr Back then.\n\nTL,DR\n\nRaptr died many features got then included in the newer drivers but sadly this didnt.\n\nit also worked allways lackluster sadly as in how Raptr worked they gathered perf data from the user base instead like nvidia making actual tests in huuge data centres.", "ups": 2, "depth": 0, "author": "Evonos", "replies": []}, {"selftext": "GFE is a great way to ruin your game performance *and graphics* by flipping on a bunch of random settings (how do you feel about TAA, motion blur, DoF, and a 20% undersample? Cant see yet?), and often corrupt/break less well-tested games. most games already\n\nSo, no. no loss, either. It cant tune anything you cant do to the game already, and if you dont understand the settings and what they do, why would you trust a piece of software to decide for you?", "ups": -1, "depth": 0, "author": "LongFluffyDragon", "replies": []}, {"selftext": "Who even uses the  bloatspyware that Experience is these days? I thought people were vaccinated against this kind if predatory intrusive software already.", "ups": -3, "depth": 0, "author": "faverodefavero", "replies": []}, {"selftext": "AMD can barely get their drivers together and you want them recommending game settings /s", "ups": -2, "depth": 0, "author": "Satirical0ne", "replies": []}, {"selftext": "the adrenaline software will make performance recommendations on the last game you played.", "ups": 1, "depth": 0, "author": "Chosen_UserName217", "replies": []}, {"selftext": "I too am looking for something like this. I  know Nvidia gets it wrong some times, but this is essentially the only feature in the AMD software I'm missing.\n\nEven if it is just used as a base line. Or simply making the 'Advisor' more intelligent. Have it read the game settings, and if it advises something be done, allow Adrenaline to make the actual change. This would be amazing.", "ups": 1, "depth": 0, "author": "Richo262", "replies": []}, {"selftext": "Yes it does. Most certainly.", "ups": 1, "depth": 0, "author": "Dry-Cry929", "replies": []}]}
{"post": {"title": "Alternative to Userbenchmark?", "subreddit": "Amd", "selftext": "After a few months with an AMD system I\u2019ve just come to realize userbenchmark is a lie (yes I know, I\u2019m sorry, I just started reading the Reddit). What\u2019s a better alternative that you guys use?\n\nEdit: Thank you all for the replies!", "ups": 81, "permalink": "/r/Amd/comments/16iul4q/alternative_to_userbenchmark/", "num_comments": 58}, "replies": [{"selftext": "TechPowerUp for quick GPU comparisons, their relative performance chart is useful enough.", "ups": 121, "depth": 0, "author": "SagittaryX", "replies": [{"selftext": "If you go into TechPowerUp's individual graphics card reviews you can also see more charts that are pretty useful, including stuff like ray-tracing and individual game performance.", "ups": 19, "depth": 1, "author": "Confirmed_Retapaded", "replies": []}]}, {"selftext": "Websites like Techpowerup.\n\nGamer's Nexus or Hardware unboxed on YT. They have all the benchmarks you want.\n\nUserbechmark is a joke.", "ups": 97, "depth": 0, "author": "John_Mat8882", "replies": [{"selftext": "google need to update their search algo just so to nerf 90% for this site, so this site to be put several pages behind search result .", "ups": 24, "depth": 1, "author": "hackenclaw", "replies": [{"selftext": "You pay Google to be at the top of searches for certain words. It's almost as of a benefactor pays for it to be boosted in the Google algorithm or they really do make enough money off of ads to beat out ALL other review websites", "ups": 11, "depth": 2, "author": "Soppywater", "replies": [{"selftext": "Those sites that pay to be on top have a \"Sponsored\" put into them. UserBenchmark really just has a great name for a website that SEO puts it on top of a non-sponsored pages.", "ups": 8, "depth": 3, "author": "popop143", "replies": []}]}, {"selftext": "They are a seo scam, indeed", "ups": 7, "depth": 2, "author": "John_Mat8882", "replies": []}, {"selftext": "google's algorithm is based on who makes the most ad money for them", "ups": 1, "depth": 2, "author": "EnderOfGender", "replies": []}]}, {"selftext": "&gt; Userbechmark is a joke.\n\nIt's worse than a joke, it's fraudulent.", "ups": 6, "depth": 1, "author": "nope586", "replies": [{"selftext": "My pet theory is that it's run by a $AMD short who got burnt when AMD didn't go bankrupt in the wake of Bulldozer as expected, but instead rebounded with Zen.", "ups": 4, "depth": 2, "author": "Googulator", "replies": []}]}]}, {"selftext": "I\u2019m a big fan of 3DMark, particularly for testing and competing for overclocks. There\u2019s a free version where you get access to firestrike and timespy, which IMO are probably the only two you should bother with anyways unless you\u2019re testing RT. If you want to skip the demos for frequent runs and unlock the other ones like speedway and port royal then just wait for it to go on sale, I bought it ridiculously cheap.", "ups": 12, "depth": 0, "author": "Curious-Thanks4620", "replies": [{"selftext": "Yeah 3dmark is funny sometimes. I've had number 1 on Port Royal for the 5600g + 6700 XT combo, and someone tried so hard to beat my score that he got the 2nd to 10th place haha. I've now been relegated to 3rd, but that was funny.", "ups": 5, "depth": 1, "author": "popop143", "replies": []}]}, {"selftext": "Passmark is the closest I know of to userbenchmark without the bias", "ups": 11, "depth": 0, "author": "reality_bytes_", "replies": [{"selftext": "I use passmark too but someone pointed out that their benchmark is a bit outdated so newer gpu underperform a bit.\n\nStill not biased as userbenchmark.", "ups": 6, "depth": 1, "author": "Le_Zouave", "replies": [{"selftext": "That's why I use [Passmark CPUbenchmark.net](http://cpubenchmark.net) for CPU, and [UL 3DMark](https://www.3dmark.com/search) for GPU (looking as several of their benchmarks since some are more skewed towards certain architectures compared to game averages)", "ups": 2, "depth": 2, "author": "Pamani_", "replies": []}]}]}, {"selftext": "You can get some basic features from 3Dmark if you download the Steam demo. Works fine if you just want a quick test.", "ups": 6, "depth": 0, "author": "thewingedguardian", "replies": []}, {"selftext": "Hardware Unboxed is my go to. They seem to have the most comprehensive benchmarks, and often do comparisons between the closest in price cards from Nvidia / AMD. And often they even include pricing comparisons for different regions other than the US.\n\nAnd if you don't have time to watch the whole video you can just skip till the end to see the average FPS over a large sample at all resolutions.", "ups": 6, "depth": 0, "author": "AvengeBirdPerson", "replies": []}, {"selftext": "Anything.", "ups": 5, "depth": 0, "author": "Keulapaska", "replies": []}, {"selftext": "Notebook check is actually pretty good for CPUs in some way: https://www.notebookcheck.net/Mobile-Processors-Benchmark-List.2436.0.html\n\nThe only problems are:\n1) the search UI is not very good, especially on mobile. \n2) it's mostly laptop/mobile processors", "ups": 7, "depth": 0, "author": "kazenorin", "replies": [{"selftext": "Notebookcheck is terrible imo, the search is horrible and their gaming benchmarks are inaccurate. For synthetics they're just 'okay', but that's it.", "ups": 1, "depth": 1, "author": "KARMAAACS", "replies": []}]}, {"selftext": "Free benchmarks:\n\n  \nGeekBench has both CPU and GPU benchmarks available for free.  It nags you to purchase but you don't have to as far as I know. \n\nCompuBench for GPU\n\nFor a gaming benchmark there's Final Fantasy XIV Endwalker benchmark.\n\n7Zip - CPU/memory benchmark\n\nAIDA64 - limited functionality with the free CPU/memory benchmarks\n\nBenchMate - SuperPi, PiPrime, y-cruncher, etc. rolled into one launcher.  It's a bit out of date in terms of not having the latest version of y-cruncher and Cinebench but it will let you store and compare benchmarks with others.", "ups": 3, "depth": 0, "author": "DuskOfANewAge", "replies": []}, {"selftext": "Cinebench 24 has GPU test now so maybe that if you don't own 3DMark.", "ups": 7, "depth": 0, "author": "blackbalt89", "replies": [{"selftext": "I just tried that last night and it\u2019s a really, really weird benchmark for GPUs. It does not favor Radeon GPUs in the slightest, apart from the W6800 but I\u2019m pretty sure that was from a Mac Pro, not a windows workstation. Seems heavily Metal/CUDA reliant", "ups": 11, "depth": 1, "author": "Curious-Thanks4620", "replies": []}, {"selftext": "&gt; Cinebench 24 has GPU test\n\nEhhh... theres something screwy going on with that, i'd give it another 6 months.", "ups": 7, "depth": 1, "author": "pyr0kid", "replies": []}, {"selftext": "just use the demo version?", "ups": 5, "depth": 1, "author": "Goldenflame89", "replies": []}, {"selftext": "Glad I saw this comment. Thanks bud", "ups": 2, "depth": 1, "author": "ApprehensiveClerk723", "replies": []}, {"selftext": "It\u2019s a lot better about not bluescreening as well", "ups": 2, "depth": 1, "author": "fivestrz", "replies": []}]}, {"selftext": "A shoe would be a better alternative", "ups": 5, "depth": 0, "author": "Happiness_First", "replies": []}, {"selftext": "Look at actual benchmarks. Techpowerup, gamers Nexus, and  techspot/hardware unboxed are all good. There's several other good tech channels  and sites that do testing as well. Warning, comparing parts that are more than a gen or 2 apart is a bit more difficult as benchmarkers typically only include cards from one or maybe two generations prior in their reviews. Techpowerup has a GPU database which compares relative performance for every GPU they've ever tested, although these were tested across several different systems and should only be taken as a rough estimate.", "ups": 2, "depth": 0, "author": "lt_dan_zsu", "replies": []}, {"selftext": "CPU comparison: https://nan\\*review.net/ (replace \\* with letter \"o\"). Idk why this sub inappropriate that website", "ups": 2, "depth": 0, "author": "Hasbkv", "replies": []}, {"selftext": "I like Tom's Hardware and Techpowerup and as for YouTube channels its usually Hardware Unboxed", "ups": 2, "depth": 0, "author": "Zherkezhi-0", "replies": [{"selftext": "I still like Tom's hardware but I'm a little annoyed that their average FPS now includes Ray tracing by default. I really wish they would break it out into a separate chart", "ups": 2, "depth": 1, "author": "ThneakyThnake808", "replies": []}]}, {"selftext": "I like passmark", "ups": 2, "depth": 0, "author": "missed_sla", "replies": []}, {"selftext": "HWUnboxed, Gamer Nexus.\n\nSites are old school, YT is the way to go,if you got the right channels. \n\nStill can't believe this Userbenchmark is still running... arghhh F shills.", "ups": 2, "depth": 0, "author": "OmegaMordred", "replies": []}, {"selftext": "Write the name of modern hardware components on post-it notes, place post-it notes on wall, throw shit at the wall and the post-it note with the most shit on it will be about as reliable, probably less shitty result as what you'll get from userbenchmark. \n\nThis is an only slightly less reliable and scientific, wile most certainly less biased way to determine which hardware option is best for you.", "ups": 2, "depth": 0, "author": "heavy_metal_flautist", "replies": []}, {"selftext": "honestly.... there really isn't much out there. Many people use cinebench to compare. Others like to use 3DMark, but i know it's not free.", "ups": 2, "depth": 0, "author": "DHJudas", "replies": []}, {"selftext": "\nGpucheck", "ups": 0, "depth": 0, "author": "Gwolf4", "replies": []}, {"selftext": "tomhardware gpu ranking chart", "ups": 1, "depth": 0, "author": "dracolnyte", "replies": [{"selftext": "I used to use them pretty often but I was pretty annoyed that they've started including Ray tracing numbers as the standard for all of their averages. I really wish they split it out as it's been shown that very few people actually use Ray tracing on a regular basis.", "ups": 2, "depth": 1, "author": "ThneakyThnake808", "replies": []}]}, {"selftext": "Performance test 11.0 by passmark is a good one takes longer than user benchmark but gives you an overall of your system and you can test individual parts as well.", "ups": 1, "depth": 0, "author": "Lord_Dog46", "replies": []}, {"selftext": "Cinebench", "ups": 1, "depth": 0, "author": "HolyDori", "replies": []}, {"selftext": "[removed]", "ups": 1, "depth": 0, "author": "[deleted]", "replies": [{"selftext": "Your post has been removed because the site you submitted has been blacklisted, likely because this site is known for spam (including blog spam), content theft or is otherwise inappropriate, such as containing porn or soliciting sales. If your post contains original content, please message the moderators for approval.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*", "ups": 1, "depth": 1, "author": "AutoModerator", "replies": []}]}, {"selftext": "EzBench on Steam. Despite its low rating, it's actually used by Epic themselves for internal testing.", "ups": 1, "depth": 0, "author": "Archonites", "replies": []}, {"selftext": "Once upon a time it was Anandtech BENCH. \n\nBut that started going downhill right after Anand left and by the time Dr. Cutress left it had cobwebs.", "ups": 1, "depth": 0, "author": "ms--lane", "replies": []}, {"selftext": "I've been using GFXBench, it's pretty decent", "ups": 1, "depth": 0, "author": "Rmmhz", "replies": []}, {"selftext": "Dammit i have been using this shit for years.", "ups": 1, "depth": 0, "author": "Uchihaaaa3", "replies": [{"selftext": "We've been screaming about that shit shill site for years.", "ups": 1, "depth": 1, "author": "Scall123", "replies": []}]}, {"selftext": "notebookcheck is pretty good.", "ups": 1, "depth": 0, "author": "DoubleOwl7777", "replies": []}, {"selftext": "everything else is better than suerbenchmark", "ups": 1, "depth": 0, "author": "RBImGuy", "replies": []}, {"selftext": "Techpowerup's database is good of you need a quick comparison, of you need accurate data go for HUB or GN's videos.", "ups": 1, "depth": 0, "author": "Cave_TP", "replies": []}, {"selftext": "u/PuzzleheadedWay3530", "ups": 1, "depth": 0, "author": "A--E", "replies": []}, {"selftext": "[https://hwbench.com](https://hwbench.com) \n\nI've always found this one useful for quick glances.", "ups": 1, "depth": 0, "author": "usual_suspect82", "replies": []}, {"selftext": "There really isn\u2019t. I\u2019m surprised no one has tried to copy the premise of user benchmark just without the bias. \n\nThere are legitimate good things about user benchmark. It\u2019s just that there are also bad things and the bad things pretty much invalidate user benchmark as even a semi reliable source of information.", "ups": 1, "depth": 0, "author": "1AMA-CAT-AMA", "replies": []}, {"selftext": "I just posted about this in another forum, so excuse the copy paste. \n\nPassmark performance test has a free version that is very good, \n\nOn steam, you can download the resident evil 6 benchmark tool which is a good \"real gaming\" test to check changes, if you are playing older games. \n\nSteam VR checking tool can give you an okay idea on VR performance.\n\n3D mark Time Spy/Time Spy extreme (have to pay for extreme) are good gaming check with Dx12. (and very useful for pc to pc comparison) \n\n3D mark Night Raid is a good lower power dx12 option.\n\nFire Strike is a good DX11 option. \n\nSuper position is popular, but a bit hard on some computers, I've seen a pc not run it, but never have issues with games. \n\nCPUz has a benchmark/stress test tool built in that will check cpu overclocks. You can compare your CPU to others score.\n\nPassmark memtest86 is one of the best for ram stability testing after adjusting timings and speeds.  \n\nMSI afterburner oc check tool works decently for stability, but i have seen some GPU overclocks test fine on that that then have issues in games.", "ups": 1, "depth": 0, "author": "N_bloc_Customs", "replies": []}, {"selftext": "I don't find userbenchmark to be too bad as a system-test. it's helped me diagnose a slow ssd and a slow memory issue. it's also good to compare performance of two gpus, just close enough to know what tier they're in. Don't read any of their reviews, of course. And just search up cinebench scores to compare cpus.", "ups": 1, "depth": 0, "author": "BFCE", "replies": []}, {"selftext": "Every benchmark/workload and app will have somewhat different results so I use those:  \n\n\n**3DMark:** Just overall performance GPU/CPU comparison.  \n**UserBenchmark:** Troubleshooting also specs check ( easiest if you're helping some newbie since it also shows clocks and exact model names ).  \n**Opendata Blender:** GPU/CPU Ray-Tracing Performance comparison.  \n**V-Ray Benchmark:** Alternative GPU/CPU Ray-Tracing Performance comparison.  \n**Youtube ( Gamers Nexus, etc ):** CPU/GPU Gaming Performance.  \ne.g.:  \n**i5-13600K vs R7 5800X3D**  \nUserBench: 13600K 73% Faster  \nUB SC: 13600K 36% Faster  \nTime Spy: 13600K 45% Faster  \n3DMark CPU Profile ( SC ): 13600K 25% Faster  \nCycles CPU: 13600K 43% Faster  \nV-Ray CPU: 13600K 40% faster  \nPassMark: 13600K 36% Faster  \n\n\nSo UserBench literally added like +30% to the i5 64-Core and +10% to Single Core :D  \n\n\n**RTX 4090 vs 7900 XTX**  \nUB: RTX 45% Faster  \nTime Spy: RTX 20% Faster  \nGeekBench ( Vulkan ): RTX 14% Faster  \nUnigine ( No averages so harder to tell ): RTX 40-50% Faster  \n\n\nIn this case the UserBench and Unigine are correct for raw GPU raster performance, but Time Spy and Geek shows more realistic gaming performance where the CPU would be utilized way more.", "ups": 1, "depth": 0, "author": "ExacoCGI", "replies": []}, {"selftext": "I use pcpartpicker and compare it myself", "ups": 1, "depth": 0, "author": "BoxAhFox", "replies": []}, {"selftext": "Passmark", "ups": 1, "depth": 0, "author": "ConsistencyWelder", "replies": []}]}
{"post": {"title": "Any good resource on undervolting and OC/tweaking the 7800xt that a noob can follow?", "subreddit": "Amd", "selftext": "I see some info on the 7900xtx from a few months back, but I wonder if there\u2019s a good resource out there where some enthusiasts have figured out a good starting point for the 7800xt for undervolting and OCing", "ups": 19, "permalink": "/r/Amd/comments/16ia568/any_good_resource_on_undervolting_and_octweaking/", "num_comments": 48}, "replies": [{"selftext": "Use adrenaline, set tunings to manual. Increase power limit to 15%, max frequency 5000, minimum frequency 2800, you can go up to 3200 as you do more testing and find that you are table. Reduce voltage by 50mv, then if you want to push it further adjust by 10mv at a time. If you\u2019re impatient you can jump to say 975mv from 1000mv but no more big jumps past 975mv. If you\u2019re a gamer check stability in Starfield because that game tends to show any instability. Once you find a stable under volt bump it up by 25-50mv while you test memory clock. Bump memory clock to 2600MHz with fast timing. Adjust up 10MHz from there while slowly testing. Haven\u2019t seen any cards stable above 2650. My own card crashes after a few hours at 2610 but 2600 has been fine for days now. For reference my card is the XFX MERC 319 7800xt. I\u2019m stable at 950mv, 2600MHz, 3200 minimum GPU clock, 5000 max.Without in game testing it will appear that you can push the card a lot further than you can in my experience so don\u2019t neglect testing in game.", "ups": 7, "depth": 0, "author": "Archoniks", "replies": [{"selftext": "I used this same method (glommed from [Techpowerup.com](https://Techpowerup.com) ) on my XFX Merc 319 that came in yesterday.\n\nI'm several hours Starfield stable at 3000 min/5000 max, 950mv, 2600 memory, +15% power.  940mv would eventually crash, as would anything over 2610 memory, so our XFX results seem to jibe.", "ups": 7, "depth": 1, "author": "CroakVan", "replies": [{"selftext": "I got the same gpu!", "ups": 1, "depth": 2, "author": "JuniorGalle21", "replies": []}, {"selftext": "First of all my XFX logo on the card doesn't light up, is there something i can do about it?  \nSecond i tried your oveclock after mounting the graphic card and after some stress tests and few minutes in game i thought it was stable but today without changing anything it looks like it is not anymore because the stress test fail and the game crash.\n\nWhat the hell is happening to me?!", "ups": 1, "depth": 2, "author": "tabiooh", "replies": [{"selftext": "Every card is unique in what overclock settings will work.  Did you get the latest drivers?  The could also have an effect on your stability.\n\nAs for your XFX logo not lighting up, I have no idea, but I'm probably going to unplug mine, it bothers me.  :)", "ups": 1, "depth": 3, "author": "CroakVan", "replies": [{"selftext": "Yes i have the latest drivers because i just installed the card, i know that overclock might change from card to card but it is weird that in the morning it would work fine in game and with stress test and few hours later it crashes everytime!\n\nBtw i tried a stress test with your setting forcing the fans at 100% and i completed it, could it crash because of the temperature? It didn't seem that hot..", "ups": 1, "depth": 4, "author": "tabiooh", "replies": []}]}]}, {"selftext": "Only SF? In other Games?", "ups": 1, "depth": 2, "author": "lfcliverbird96", "replies": [{"selftext": "Not playing any other games right now.  But once again, DO NOT copy somebody else's overclock and expect it to work 100%.  You may do better, you may do worse.", "ups": 1, "depth": 3, "author": "CroakVan", "replies": []}]}]}, {"selftext": "I'm very interested in hearing how much in-game performance gain compared to stock you are getting out of that!", "ups": 2, "depth": 1, "author": "octopec", "replies": [{"selftext": "Here is a link to a chart I made of stock vs Overclocked Performance.  \nURL: [https://imgur.com/a/gbGoa2j](https://imgur.com/a/7qEKdUI)\n\nVoltage 950mV, 3200 min Frequency, 5000 max, power limit +15%, Memory: 2606Mhz, Fast Timing Enabled, Default Fan Timing (This GPU runs insanely cool in my setup).   \n\n\nAll settings were Ultra except crowd size set to low to minimize CPU limiting since I only have a 1080p monitor at the moment. FSR and VSR off.  \n\n\n% Gains: Average Framerate: 5.91% | Min Framerate: 6.55% | Max Framerate: 5.98% | 1% Low: 9.46% | .1% Low: 9.02%  \n\n\nWill do more testing when I get my 1440p monitor but I'm not 100% sure when that will be.", "ups": 3, "depth": 2, "author": "Archoniks", "replies": []}, {"selftext": "I\u2019ll do some testing either tonight or tomorrow and post the results for starfield! I\u2019ll look up how the reliable testers do their starfield fps and try to imitate that", "ups": 2, "depth": 2, "author": "Archoniks", "replies": []}, {"selftext": "Here is a link to a chart I made of stock vs Overclocked Performance.  \nURL: [https://imgur.com/a/gbGoa2j](https://imgur.com/a/7qEKdUI)\n\nVoltage 950mV, 3200 min Frequency, 5000 max, power limit +15%, Max Frequency 2606Mhz, Fast Timing Enabled, Default Fan Timing (This GPU runs insanely cool in my setup).   \n\n\nAll settings were Ultra except crowd size set to low to minimize CPU limiting since I only have a 1080p monitor at the moment. FSR and VSR off.  \n\n\n% Gains: Average Framerate: 5.91% | Min Framerate: 6.55% | Max Framerate: 5.98% | 1% Low: 9.46% | .1% Low: 9.02%  \n\n\nWill do more testing when I get my 1440p monitor but I'm not 100% sure when that will be.", "ups": 2, "depth": 2, "author": "Archoniks", "replies": []}, {"selftext": "Well, since I just got the card yesterday, can't really tell you the Starfield difference between undervolted and stock since I went right to undervolting after some benchmark runs.\n\nBut, I picked up quite a bit in 3DMark Firestrike Ultra.  Here's a comparison between the first stock benchmark run I did vs 3000/5000/950mv/2600mem/+15% power limit, which is stable.\n\n[https://imgur.com/a/xKGS7SD](https://imgur.com/a/xKGS7SD)  \n\n\n12,324 vs 13,957, about a 13.5% performance boost.  Timespy and Port Royal benchmarks reflect about the same.", "ups": 2, "depth": 2, "author": "CroakVan", "replies": []}]}, {"selftext": "But you don't need to reject an overclock because it crashes in one game, you can do that just for that game in Adrenalin.", "ups": 3, "depth": 1, "author": "Eudyptes1", "replies": []}, {"selftext": "SameGPU and all dandy in synthetic (TimeSpy) but crashes in games like Squad and NMS. I've increased the voltage to 1050mV in games and then I have better results.\n\nStable in Games?", "ups": 1, "depth": 1, "author": "lfcliverbird96", "replies": [{"selftext": "Results may vary based on your specific hardware configuration and the silicon lottery. Just slowly adjust until you find what works", "ups": 1, "depth": 2, "author": "Archoniks", "replies": []}]}]}, {"selftext": "Check out the Kitguru's last video on 7800 xt review hellhound - he give a few values.", "ups": 9, "depth": 0, "author": "kaisersolo", "replies": []}, {"selftext": "I own an Asrock phantom gaming so the results may differ depending on the AIB but here's my results.\n\nI set a max power limit of 15%\nMin clock at 2500\nMax clock at 2600 \nTension at : 1010\nAnd vram at 2542.\n\nFor some reason my GPU boosts at an average of 2660 MHZ (Not sure why it goes above the max clock but I won't complain)\n\nI get an average of 250/260 watts power draw and sometimes it'll go up to 280 watts. The fans will usually stay at 970 rpm and they'll ramp up to 1100 rpm and 1300 rpm when it's 24C in my bedroom. All of that for a 50C  edge/70C junction. And 60C/81C when it pulls 280 watts\n\nIn pure synthetic benches it's 4% below my 6950 XT. But in actual gaming it gets the same frames and often a little more. (I used to have a 6950 xt but it died and I bought a cheaper 7800 XT as a replacement after I got my refund)\n\nI hope this helps you. Tbh I think that it's a neat GPU and that many YouTubers and people are shitting on it because they only look at stock settings. It's an OC monster. I'm pretty sure I could get more out of it but I'm happy with how it works right now.", "ups": 4, "depth": 0, "author": "BestPloot", "replies": [{"selftext": "Huh, I have a red devil LE and with a mild undervolt (idk the exact mV but it was around 80%) and a stock fan curve with power budget all the way up I was able to consistently pull 2780 mhz on OC BIOS. I knew it was one of the beefier variants but I wasn't expecting that much more free clock. I'm almost certain I could get it above 2800 when I actually dial it in.", "ups": 1, "depth": 1, "author": "dougof300", "replies": []}]}, {"selftext": "[This guy gives a good starting point for most AMD cards](https://www.youtube.com/watch?v=vrlHUqBq09g&amp;pp=ygUkYW5jaWVudCBnYW1lcGxheXMgNzYwMCBiZXN0IHNldHRpbmdz) (linked to his 7600 OC/UV). He'll probably upload one for 7800 XT and 7700 XT the next week or so.\n\nFor the meantime, you can follow [this guide](https://www.youtube.com/watch?v=S5LC7rk1ujc&amp;pp=ygUxb3ZlcmNsb2NraW5nIGFuZCB1bmRlcnZvbHRpbmcgZ3B1IHVzaW5nIGFkcmVuYWxpbg%3D%3D) on how to use Adrenalin to overclock and undervolt. It'll \"only\" take you around 4-6 hours to find a stable one (though I only advise a -50 mV undervolt for the safest, most conservative undervolt. Maybe -75 mV).", "ups": 7, "depth": 0, "author": "popop143", "replies": []}, {"selftext": "It'll be the same principle as Navi 31 and 33, AncientGameplays has a good tutorial.", "ups": 3, "depth": 0, "author": "xXMadSupraXx", "replies": []}, {"selftext": "I found 1060 on the voltage the sweet spot.\n\nMost games run lower at 1000 but starfield was crashing on certain planets.\n\nWith the voltage at 1060 adrenalin reports around 180 watt +- 10", "ups": 1, "depth": 0, "author": "yulaw123", "replies": [{"selftext": "180 sounds way too low", "ups": 3, "depth": 1, "author": "Archoniks", "replies": [{"selftext": "Starfield uses low power for every GPU, AMD or NVIDIA.  This is normal for Starfield.  Remember, it's a buggy Bethesda release that will take years+ for patches to fix it.", "ups": 1, "depth": 2, "author": "DuskOfANewAge", "replies": []}]}, {"selftext": "180? What's your power tuning % at? Haven't seen a single 7800 XT review covering undervolting that gets them below 250W power draw.", "ups": 2, "depth": 1, "author": "octopec", "replies": [{"selftext": "I am getting ~225W with only undervolting to 1050mV. So depending on game and chip maybe it's possible to drop further, but 180W sounds pretty insane, probably game optimization for starfield really could do some work.", "ups": 1, "depth": 2, "author": "xeroze1", "replies": []}]}, {"selftext": "180W? The power draw should NOT be that low on a 7800XT unless the GPU isn't being fully utilized. Should be around 250W at 100% load with an undervolt.", "ups": 2, "depth": 1, "author": "RedChaos92", "replies": []}, {"selftext": "what CPU are you on and what resolution + max fps on monitor?", "ups": 1, "depth": 1, "author": "ofon", "replies": [{"selftext": "1440p 2560 144htz.monitor.\n\nBalders gate 3 tops out at 200watt as does cyberpunk, rdr2 and many others.\n\nStats taking from the adrenalin software so accuracy is dependant on that.", "ups": 1, "depth": 2, "author": "yulaw123", "replies": [{"selftext": "seems pretty solid...personally if you wanna ensure that the GPU lasts, I wouldn't do any overclocking and instead do some undervolting while shaving off a slight amount of clockspeed.  \n\n\nBut be on the lookout for \"Ancient Gameplays\" channel. He'll have a video about overclocking + undervolting on a 7700 xt soon and you can emulate the pattern and make it work with the 7800 xt", "ups": 2, "depth": 3, "author": "ofon", "replies": [{"selftext": "Hey, I was looking for someone who's a bit more cautious about these things to get an opinion on overclocking. Is it actually potentially harmful just playing around in Adrenaline? It seems like most people think it's totally fine, but I'd hate to kill the card trying to get 10% more performance. It'd seem more cost effective just buying a 7900xt at that point.", "ups": 1, "depth": 4, "author": "fookidookidoo", "replies": [{"selftext": "mild overclocks are typically fine...the thing to do is pay attention to max junction and average temperatures and one of the best ways to do that is slowly increasing your max clockspeeds while also undervolting.  \n\n\nHowever 10% extra performance isn't a good deal when you see the temperatures it takes to get there. Personally I wouldn't mess with that unless you can achieve that extra 10% performance with superb temps meaning 80C hotspot/junction temps and below especially if you're looking to keep the card for the long term meaning like 4 years or more.  \n\n\nI have a 7800 xt installed in my PC since last night and after seeing the video playback power draw and seeing it spike to 42-70 watts just browsing through youtube...I'm definitely returning this thing and going back to my 3060 ti for another generation. The 3060 ti would get occasional spikes to 40, but I'm typically hovering around 12-17 watts of usage which is pretty good. Also watching youtube on Radeon GPUs just looks worse in general. Sorry for the sidetrack...just want to mention a reason to go Nvidia instead.  \n\n\nThe gaming performance of the 7800 xt was just stellar though...only thing it's missing is an improved upscaler...however Hyper-rx or whatever it's called is super nice as it's driver level and works in all games i've noticed.  \n\n\nHopefully the rumored monolithic AMD gpus improve the idle and low usage power consumption for next generation, but back to your original question...I'd suggest just small increments of like 20 mhz increases in clockspeed to see what's stable and once you see a crash, add a little bit of voltage (5 mv or so) but pay attentino to temps.  \n\n\nGoing around 100C hotspot/junction is pretty bad no matter what people tell you...those aren't the kind of temps you want on a GPU long-term.", "ups": 2, "depth": 5, "author": "ofon", "replies": [{"selftext": "I played around with probably a fairly aggressive overclock and then got cold feet, and just set it back to stock. Lol I was worried I'd damage it. It was pretty stable for a bit at 950mv undervolt, hitting 2850mhz and the memory was at 2600. But then it started crashing even after I dropped the memory to 2550 so I got scared I'd messed something up. Haha\n\nTemps never exceeded 87 on the hotspot though. I'm a little paranoid I could have damaged it, but I'm sure a few hours benchmarking probably wouldn't hurt the memory right?\n\nMine uses about 50w for regular browsing and stuff. But cranking up the power limit 15% to 300w to get at most I saw a 12% gain seemed a bit dubious.\n\nI'll probably try a mild undervolt and maybe a 5-10% power limit and see how that does. Honestly, just a more aggressive fan curve seems to help it a lot too. My card will not allow itself to go past 85c without cutting back.", "ups": 1, "depth": 6, "author": "fookidookidoo", "replies": [{"selftext": "Yeah 50 watts is super high...but the thing is so damn good at gaming for the price that it's hard to pass up. Hyper-rx is awesome esp since you can essentially use frame interpolation and the FSR upscaling at the driver level  in DirectX 11 and 12 games. In contrast, Nvidia's frame gen (only usable on rtx 4000) can only use the frame gen in games that have specifically put it into their software which are almost exclusively AAA games and Nvidia sponsored at that.  \n\n\nAnyway both Radeon and Nvidia gpus tend to be clocked way past their efficiency curve and both benefit hugely from undervolting alone. Here's what i'd recommend...create 2 custom GPU profiles. 1 for lighter games and 1 for heavier games.  \n\n\nI highly doubt you damaged it already at those temperatures with a 950 mv undervolt as these can run very hot for short periods of time (although I wouldn't recommend it if you wanna ensure that this thing lasts several more years)  \n\n\nSee if you're satisfied with your performance with a max of around 2350-2400 mhz and that undervolt. It also lets you set the max fan curve at around 40% so you don't have to deal with the extra noise.  \n\n\nAs far as playing light e-sports games, I've got a profile where I have a max frequency of 1500 mhz and voltage set to 700mv. I left the memory on default timings and the lowest memory speeed of 2438 mhz while capped at 240 fps in league of legends. This is with hyper-rx enabled of course and I'm using a max of 81 watts from the GPU while gaming.  \n\n\nWith a profile at 950 mv like you had, I set my GPU to 2200 Mhz and get a max of about 180-200 which is pretty good considering a performance loss of about 10%, but that's part of why I'm looking to refund the GPU soon. They don't downclock or undervolt very well at the lower limits due to being MCM as well as some possible driver issues that may or may not be fixed in the future.  \n\n\nThat being said, the 7800 xt is still way more efficient at gaming than a 3080 or 3080ti which are more or less in the same region of performance as long as you don't venture into ray tracing which I personally don't care about and I'm guessing you may not either.", "ups": 2, "depth": 7, "author": "ofon", "replies": [{"selftext": "Dang, thanks for all that! So really the only issue is heat like you're saying? Because my Sapphire Pulse stays within a very reasonable temperature no matter what it seems. In Starfield, even the Hotspot temp sticks around 75c for the most part on ultra 80% render with FSR. It's a very quiet card too, on the stock fan setting I can't even hear it.\n\nUsually it idles at 30ish though. Watching YouTube goes to 50 with some spikes.\n\nJust curious, I use Radeon Chill for games I just don't need a lot of performance for. You'd recommend underclocking instead? I don't play esports games much, so the only time I don't want max performance is with games I typically don't care if they run at 60fps. You've got me thinking about one or two though that I'd like to run faster at like 90, but just don't need the GPU chugging to do it.\n\nDoes gpu clock speed effect the health of the card so long as it's cooled adequately? Really my mind is more along \"does a 20% increase in watts justify a 5-10% increase in graphics output\" and I'm not sure it does.", "ups": 2, "depth": 8, "author": "fookidookidoo", "replies": [{"selftext": "Yeah I would recommend underclocking for games and keeping a seperate profile for them. If you don't wanna trouble with yourself with it, maybe Radeon chill isn't a bad idea, but I notice that feature has some issues i get really annoyed by. It does work well when you're completely afk.  \n\n\nMain issue with Radeon Chill I've personally found is that sometimes you find yourself in a game where you're not moving the mouse or keyboard and you may be waiting for something to happen and don't necessarily want RC to reduce the current framerate. It seems to be based on recent keyboard and/or mouse movement. However if that's not a big deal for you, then by all means go for it. I haven't messed with it that much but it could be a decent forget n fire option to reduce wattage when you've gotta afk suddenly.  \n\n\nYeah it's pretty much all temperature based...just make sure you're paying attention when it's stressed.", "ups": 1, "depth": 9, "author": "ofon", "replies": []}]}]}]}, {"selftext": "if you need some help tinkering safely...just shoot me a msg!", "ups": 1, "depth": 6, "author": "ofon", "replies": []}]}]}]}, {"selftext": "just curious...what CPU are you running?", "ups": 1, "depth": 3, "author": "ofon", "replies": []}]}]}, {"selftext": "I have 200w in starfield with the clocks going from\n2330-2400mhz. My settings are max freq of 2430mhz en 1125mV. I also noticed starfield using low power", "ups": 1, "depth": 1, "author": "Macit1921", "replies": []}]}, {"selftext": "As xXMadSupraXx mentioned, i also recommend AncientGameplays on youtube, gives a good understanding of everything Radeon &amp; Adrenaline related", "ups": 1, "depth": 0, "author": "Drubban", "replies": []}, {"selftext": "I'd be curious to see a guide from someone with experience in this sort of thing.  I just purchased an ASRock 7800XT Phantom model specifically for the high-end cooling and wow, that decision paid off.  I fiddled with the Adrenaline settings to drop to 1050mV, put a moderately aggressive setting on my fans, and I'm running mid-50C at full load.  Really impressed, and I know someone with more experience can do better.", "ups": 1, "depth": 0, "author": "GrouchoManSavage", "replies": []}, {"selftext": "I have a gigabyte RX 7800 XT.\n\nI have set my voltage to 1070 mV and max Core Clock to 2650.\n\nGives me good gaming results while having the max. Hotspot Temp at 65\u00b0C", "ups": 1, "depth": 0, "author": "ui_FellFrog", "replies": [{"selftext": "Just received my gigabyte 7800 xt today.\nCan you get any lower in voltage? Have mine at 1075mV and had crashes at 1050 and 1060mV.\nI am looking for stock performance at lower noise/temp/watt and currently try at 1075mV, -10% power, and limit max mhz to get something like 220w max in time-spy.", "ups": 1, "depth": 1, "author": "AndersSchmanders", "replies": []}]}]}
{"post": {"title": "FSR 3 - Is Fortnite Going to Add it?", "subreddit": "Amd", "selftext": "Title, I\u2019m really excited for FSR 3, but I\u2019ve only seen that it\u2019s going to new games being released this month, so it might take a while to implement it to existing games. What I\u2019ve also noticed is that Fortnite isn\u2019t \u201ctechnically\u201d using FSR at the moment, and I haven\u2019t seen Epic games in that picture with supported developer companies. Should we expect it?", "ups": 0, "permalink": "/r/Amd/comments/16iuiun/fsr_3_is_fortnite_going_to_add_it/", "num_comments": 55}, "replies": [{"selftext": "If it's as easy as a plugin as AMD say then I dont see why not", "ups": 16, "depth": 0, "author": "hahaxdRS", "replies": []}, {"selftext": "I doubt it, doesn't even have FSR2", "ups": 15, "depth": 0, "author": "billbr0baggins", "replies": []}, {"selftext": "Well since DLSS 3 is getting added later this year they will likely add FSR 3 as well", "ups": 9, "depth": 0, "author": "dnb321", "replies": [{"selftext": "They added DLSS2 a long time ago, could have added FSR at the same time and never did", "ups": 2, "depth": 1, "author": "Pretty-Ad6735", "replies": [{"selftext": "They already have a temporal upscaler built in as well, they don't have frame gen built in.", "ups": 1, "depth": 2, "author": "dnb321", "replies": [{"selftext": "That TSR is new, late last year and it's part of UE5 not a Fortnite specific feature. UE5 does not have frame gen in any of its pipelines, only DLSS3FG or upcoming FSR3 titles", "ups": 1, "depth": 3, "author": "Pretty-Ad6735", "replies": [{"selftext": "It had its own temporal upscaler before DLSS 2, and I know it doesn't have any frame gen, which is why I'm saying its likely going to get FSR 3 support when it gets DLSS 3 support.", "ups": 1, "depth": 4, "author": "dnb321", "replies": []}]}]}]}]}, {"selftext": "It wouldn't really benefit Fortnite, you would deffo feel the latency in a battle royale game.", "ups": 18, "depth": 0, "author": "switchwise", "replies": [{"selftext": "Though most people probably wouldn't want to use in Fortnite, more options is generally a good thing.", "ups": 9, "depth": 1, "author": "jm0112358", "replies": []}, {"selftext": "FSR has latency? How bad is it?", "ups": -5, "depth": 1, "author": "AceXOA", "replies": [{"selftext": "FSR 3 frame Gen will add very noticeable latency, Just use DLSS frame Gen for reference. Also worth noting that DLSS Frame Gen uses Machine learning so it should probably be better the FSR latency wise (and visual wise) and people still complain about latency.\n\nAlso worth noting that if you\u2019re getting say 144 fps with frame Gen, you\u2019re only getting 72 fps as far as input latency goes, the generated frame doesn\u2019t take input.\n\nThis isn\u2019t poop pooping frame Gen as a technology though, I\u2019m waiting for FSR 3 to come out to decide on my next GPU, it\u2019s a game changer for story based/non competitive gaming, it\u2019s just bad for competitive shooters", "ups": 2, "depth": 2, "author": "TheCheckeredCow", "replies": [{"selftext": "Thanks! Not sure why I got downvoted but this is useful", "ups": 1, "depth": 3, "author": "AceXOA", "replies": []}, {"selftext": "People so badly overestimate how bad the latency is. Sure it's not optimal to be adding latency on a competitive shooter but if you already have FPS in the 100s, my guess is that the latency is not going to be noticable based on my own experiences comparing say 60 FPS native vs 120 FPS with FG which is theoretically 60 FPS. Regardless to me it seems like they're trying to make Fortnite more than just a FPS game and at the end of the day nobody will be forced to turn it on so who really cares if it is worse for competitive play(just turn it off).", "ups": 1, "depth": 3, "author": "tilted0ne", "replies": [{"selftext": "Fortnite is Epic\u2019s testing grounds for the Unreal Engine, much like how TF2 and CS:GO were and are for the Source engine and Valve. It wouldn\u2019t surprise me if Epic added FSR 3 as an option, even if it won\u2019t get utilized much in practice.", "ups": 0, "depth": 4, "author": "handymanshandle", "replies": [{"selftext": "Dude the game doesn't even have FSR2... Testing grounds my ass.", "ups": 3, "depth": 5, "author": "KARMAAACS", "replies": []}]}, {"selftext": "The whole point of high FPS is to *reduce* latency in games like Fortnite.", "ups": 1, "depth": 4, "author": "clicata00", "replies": []}]}, {"selftext": "You say use dlss 3 for reference suggesting it had a massive impact on latency. Do you struggle with games currently that don't have reflex because that's all the impact of enabling frame gen is half the time.", "ups": 0, "depth": 3, "author": "swear_on_me_mam", "replies": [{"selftext": "No not at all, I\u2019m a huge supporter of frame Gen. I don\u2019t personally have a 40 series card but I\u2019ve tried it at a local computer store and the first thing I said to myself was \u201cwell fuck me this is the future, this is incredible\u201d. \n\nThe problem with twitchy competitive shooters is that everyone wants the most minimum input latency as possible with the most frames as possible. The amount of people playing CSGO competitively for example at everything low, 800x480 stretched is crazy. It\u2019s all about frames and reaction time in those games, and because theirs no input on the Generated frame it becomes worthless for them.", "ups": 1, "depth": 4, "author": "TheCheckeredCow", "replies": []}]}, {"selftext": "ML would do sweet FA for latency, it's just another tool used in some parts of DLSS to help visual quality.\n\nIt's still interpolating frames, so it needs the \"next\" frame to be ready when generating the new in between frame, meaning that it delays showing that next frame and increases end to end latency.\n\nFrom a latency point of view fsr3 looks like it behaves exactly the same. The only differences would be on implementation specifics, rather than anything fundamental to how they work.", "ups": 1, "depth": 3, "author": "Jonny_H", "replies": []}]}]}, {"selftext": "Depends, if you're at 200fps and you have a 500fps monitor then the latency isn't much at all. Yes, super niche example.", "ups": -1, "depth": 1, "author": "Accuaro", "replies": []}, {"selftext": "good thing fortnite isn't a battle royale....\n\nfortnite battle royale... is a battle royale though", "ups": -23, "depth": 1, "author": "DHJudas", "replies": [{"selftext": "Most people referring to Fortnite generally mean the Battle Royale component of the game. It\u2019s not like all parts of the game don\u2019t share the same underlying tech anyways.", "ups": 11, "depth": 2, "author": "handymanshandle", "replies": [{"selftext": "What are you doing here with good sense?", "ups": 3, "depth": 3, "author": "Jonthan93", "replies": []}, {"selftext": "just because one is more popular than the other doesn't negate the fact of the matter that one is fortnite.. the other is a battle royale, specially in the case that one predates the other. However regardless of that, it make switchwise's statement false since even you acknowledge they utilize the same engine and basically identical assets.", "ups": -7, "depth": 3, "author": "DHJudas", "replies": [{"selftext": "Semantics with no point. People refer to Fortnite explicitly relating to the Battle Royale mode. Please don't act like the reddit stereotype.", "ups": 5, "depth": 4, "author": "Omegachai", "replies": [{"selftext": "No people don't, only the ignorant do.\n\nPlenty that know better define it. No idea what stereotype you're refering to.", "ups": -6, "depth": 5, "author": "DHJudas", "replies": []}]}]}]}, {"selftext": "Fortnite Save The World and Fortnite Battle Royale are subsets of the overall Fortnite ecosystem which also includes Fortnite Creative. You can't have one without the other so it doesn't really matter at all", "ups": 1, "depth": 2, "author": "Pretty-Ad6735", "replies": [{"selftext": "again false.... fortnite \\~&gt; save the world.... is literally the base game.... neither BR or creative would exist without it.", "ups": 0, "depth": 3, "author": "DHJudas", "replies": [{"selftext": "Separate entirely. You can have BR and Creative without Save the world. Save the world hasn't been the base for years. It's Fortnite and you choose to install Save the world or just BR/Creative. Calling something a base means the rest can't exist or be played without it, like DLC.", "ups": 1, "depth": 4, "author": "Pretty-Ad6735", "replies": []}]}]}]}]}, {"selftext": "You don't wanna use FS3/FG in games like Fortnite\n\nThat's not the intended use", "ups": 7, "depth": 0, "author": "Confitur3", "replies": [{"selftext": "Why not? I thought the purpose of it was to improve FPS, can you explain what I'm missing please?", "ups": -3, "depth": 1, "author": "AceXOA", "replies": [{"selftext": "It doesn't just increase frames, it increases frames but you'll have added latency, the real frames do have input, while the in between frame doesn't, so if for every 3 frames one is generated by frame gen, that one frame won't have any input information.", "ups": 3, "depth": 2, "author": "twhite1195", "replies": []}, {"selftext": "The point of FG/FMF is to increase smoothness. The backside of this, is that input lag goes up, because the generated frame (every other frame with FG or FMF turned on, is generated without taking user input into account) gets inserted between two traditionally rendered frames, that need to be rendered first. Thus, as a tradeoff for the increased smoothness, input lag goes up.\n\nThe point of increasing FPS in games, apart from the increased smoothness, is to reduce the input lag. Lower input lag means it feels more responsive to play, and it's easier to actually get anything done.\n\nThus, because Fortnite is a competitive game, where it is an advantage to play on a more responsive system, it's counter-productive to turn on technologies that increase smoothness at the cost of input lag.\n\nhttps://gpuopen.com/wp-content/uploads/2023/03/fsr2-3-diagram.png\n\nIn the above illustration, FSR 3 with FMF (frame interpolation) is on the bottom row. The pink/red Present markers are generated frames, the grey ones are traditionally rendered frames. Notice how much later the first grey frame is presented in the FSR 3 row, vs the FSR 2 row.", "ups": 2, "depth": 2, "author": "Slafs", "replies": []}, {"selftext": "L-A-T-E-N-C-Y", "ups": 3, "depth": 2, "author": "Medical-Tomorrow7727", "replies": [{"selftext": "If you already have good framerate from the start the latency will not be increased much (example 144 to 240). But sure, if you start from 60 and ramp up to 144 the latency will be as bad as it was when you were at 60, no escape from that.", "ups": 1, "depth": 3, "author": "PastSatisfaction7995", "replies": [{"selftext": "Going from 144 to 240 isn't worth using frame generation anyway, you will hardly perceive the increased smoothness but you might notice the increased input lag. Frame generation is best suited for going from 60 to 120. And in that case there will be a latency penalty, at least one frame worth, which is fine for a single player game but isn't insignificant for a competitive game.", "ups": 2, "depth": 4, "author": "Sipas", "replies": [{"selftext": "Lol if you achieve anything above 144fps then frame gen is just a plus.\n\nFrame gen is good for ppl that plays with controllers where latency of your controller and pc may be on a similar ms hence it is less noticeable.\n\nAnyhow. Any new tech to help with current demand is good.\n\nI'm on 7900xtx and ran fsr 2 whenever possible as the image quality and performance trade off is good enough for me. And playing on shitty res long enough, I adapt to it so can't really give a shit about the differences between native and upscaled. I don't zoom in to play lol", "ups": 1, "depth": 5, "author": "Medical-Tomorrow7727", "replies": []}, {"selftext": "I'll use FG to play at 4K 120 with lumen, I already do 120 at 4K epic settings but with lumen on it's down to 90ish FPS.", "ups": 1, "depth": 5, "author": "Pretty-Ad6735", "replies": []}]}]}]}, {"selftext": "there is extra processing in between the real frames to generate and insert fake frames therefore you add latency with any frame gen tech doesn\u2018t matter if it is dlss 3 or fsr 3. In single player games the higher smoothness is sometimes worth the extra latency, in multiplayer it is just bad extra latency and all the other issues like ghosting that come with frame gen.", "ups": 1, "depth": 2, "author": "BausTidus", "replies": []}]}]}, {"selftext": "Squad is getting it", "ups": 2, "depth": 0, "author": "bloodstorm666", "replies": []}, {"selftext": "Frame gen is not good for multiplayer games.", "ups": 2, "depth": 0, "author": "sackblaster32", "replies": []}, {"selftext": "Likely it will be one of the first to add it", "ups": 0, "depth": 0, "author": "Intercellar", "replies": []}, {"selftext": "its a compe title as much as you want dont use it", "ups": 1, "depth": 0, "author": "prisonmaiq", "replies": []}, {"selftext": "Why would you need a frame gen tech in a competitive shooter where latency is king?!\n\nFPS can't be low since you play those games min settings, not for the eye candy. Unless you have an absolute potato pc.", "ups": 1, "depth": 0, "author": "David0ne86", "replies": [{"selftext": "I honestly just play the game with full graphics at around 70fps, Unreal Engine 5 is too beautiful to ignore lol", "ups": 1, "depth": 1, "author": "AceXOA", "replies": [{"selftext": "Every FG tech add latency. You can use FG but your input latency will go up tiny bit. Like few ms, you can ignore it, it is fine. If you are a  \"sweatlord\", you want every tiny bit advantage, you will not use it.  Also if your FPS droping down below 60 it will be sub optimal experience.\n\n&amp;#x200B;\n\nIn the end, you play how ever you want. As long you are having fun it is okay.", "ups": 1, "depth": 2, "author": "CheekEnough2734", "replies": []}]}, {"selftext": "on a 4090 and run the game at low settings to get 165", "ups": 1, "depth": 1, "author": "xxademasoulxx", "replies": []}]}, {"selftext": "The question is: Is AMD going to add it?  \n\n\nThey have been talking about it for so many months and still nothing", "ups": 1, "depth": 0, "author": "smegmacow", "replies": []}, {"selftext": "Why would you want to run FSR3 on a competitive multiplayer game? If you just want to handicap yourself play with your eyes closed.", "ups": 1, "depth": 0, "author": "snotpopsicle", "replies": []}, {"selftext": "No, the reason why you want more FPS in shooters is to lower input latency and react faster, the extra latency coming from frame generation is going to give you the opposite effect.", "ups": 1, "depth": 0, "author": "Cave_TP", "replies": []}, {"selftext": "Why would you add it to a game where latency is a big factor to performance? Sure, for Flight Simulator or something like that where it's more of an experience predicated on slower paced gameplay, it's a great feature. But adding it to a game like Fortnite where you need to be a sweaty builder/editor with millisecond peeks to even survive the average lobby makes no sense. It would be like adding it to Street Fighter, no point.", "ups": 1, "depth": 0, "author": "KARMAAACS", "replies": [{"selftext": "I personally just play casual no build so reaction time isn't too big of a deal for me here, though how bad is the latency? Is it as bad as using Geforce Now?", "ups": 1, "depth": 1, "author": "AceXOA", "replies": []}]}, {"selftext": "No. Fortnite is an Epic game and they have their own competitor called TSR that's already in the game.", "ups": 1, "depth": 0, "author": "ronoverdrive", "replies": []}, {"selftext": "because esports games would suck with it\n\nit increases delay\n\nso basically your going from a good experience to a trash experience because ai would suck predicting esports plus fortnite is very delay based", "ups": 1, "depth": 0, "author": "ZainullahK", "replies": []}, {"selftext": "The game doesn't even have normal FSR yet so who knows.. but I reckon they won't. They probably use their TSR to refine it and it works on everything so it wouldn't surprise me if they have their own frame-gen stuff coming eventually too and that'll be their logic to not add FSR again. \n\nIs DLSS frame gen even in fortnite? I have no idea.", "ups": 1, "depth": 0, "author": "ZeroZelath", "replies": []}]}
{"post": {"title": "ROG Crosshair VIII Hero w/ 3900x way stable @ 4.3ghz (EVGA 3080TI) - New to OC", "subreddit": "Amd", "selftext": "Currently undervolted @ 1.28V with DDR4 3200MHz\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nCinebench (Multi Core):\n\nScored:      19,396  \nMax Temp: 77c \n\n(CPU-Z: 8606.7)\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-MP Ratio 15.4x\n\nCinebench (Single Core):\n\nScored:       1,260\nMax Temp:  50c\n\n(CPU-Z: 522.0)\n\nDoes anyone have any advise on how to push it further? Or should I leave it where it\u2019s at? First time trying to overclock so just looking for advice. \n\nMore importantly: What would I adjust at this point to try and get a better score?", "ups": 3, "permalink": "/r/Amd/comments/16idspn/rog_crosshair_viii_hero_w_3900x_way_stable_43ghz/", "num_comments": 17}, "replies": [{"selftext": "Not sure what your using to test stability, but learning how to use something at least like OCCT would be an order of magnitude better than Cinebench. Cinebench mainly serves as a verification of performance (and just CPU and not even really RAM) like Geekbench or AIDA64's RAM latency test.\n\nHeavy all core loads (OCCT), and core-cycling (OCCT), and then lighter single threaded testing over time (various games and web browsing) will yield stability results before claiming \"way stable.\" Nothing personal, just a heads up if you had not gone down this kind of route yet.\n\nAssuming you have not manually touched the FCLK (infinity fabric) yet, make sure if you do to read online how to check FCLK stability, too.", "ups": 3, "depth": 0, "author": "HateToShave", "replies": [{"selftext": "Thank you for all of the information. I claimed stability based on using my computer in its normal atmosphere, where it runs cooler than it did prior to the overclock. Specifically, I gained 40-50 fps while playing Modern Warfare. Previously, I was getting 120-130 fps, but now it sits between 170-180.\n\nI am completely new to overclocking and was too scared to do it when I first built this PC in 2020.\n\nCurrently, I am getting familiar with MSI Afterburner. I am thinking that after I dial in the GPU settings with Afterburner, I will slightly increase the core voltage and then run comparisons using Cinebench, CPU-Z, and playing MW2.\n\nI'm going to look into the things you mentioned (Geekbench and Aida). Are they pretty user-friendly or more advanced? I am very much a novice but really enjoy tinkering around. So, if you have a suggestion of where I should start first, I'm all ears! Don\u2019t hesitate to correct me if I\u2019m saying things that sound dumb. I definitely came here to get educated.", "ups": 2, "depth": 1, "author": "ParadoxSquid", "replies": [{"selftext": "I'd use 3DMark and something like FireStrike and/or Timespy to judge your GPU (*and CPU, too, as there is also a CPU score when you run the tests*) performance in gaming. It's not going to be the be-all/end-all for this, but, like Cinebench, it is a point of verification from settings adjustments compared to defaults. If you get a boost from settings adjustments in a 3DMark score then further in-game testing to ensure stability would be the next step there (like longer sessions).\n\nAIDA's only real use, these days for the consumer, is for testing theoretical RAM latency and bandwidth when overclocking that separately from everything else. Yeah, it's a suite of tools, sure, but there are other things (like 3DMark and in-game benchmarks for GPUs or even video encoding for CPUs) one can use to check performance that are more commonly referenced online than AIDA's other suite of tools. Geekbench, too, is not too shabby to use for CPU/RAM comparisons.\n\nAll of these programs can be had for free, but you can usually find 3DMark on sale for like \\~$5-7US from various sellers online and sometimes Steam during Steam Sales events. This would give you more detailed and specific testing. One of the ways to use the paid version of 3DMark's Firestrike, for example, to do a quick and dirty test of FCLK (infinity fabric clock) is to do a looped Combined CPU/GPU test with max settings (*1080p windowed is fine, though, so you can few stats in another program*) to look for WHEA errors (*which would be corrected, but will lead to performance drops*). For example, if you had RAM set to 3800 speed (so 1900Mhz actual frequency because Double Data Rate) in the BIOS and the FCLK at the 1900Mhz to match in 1:1 ***and*** some RAM test suites show stability, you could still have FCLK instability causing crashes/WHEA errors (or being the culprit you couldn't find). *(EDIT: Running this Firestrike test, as indicated, should assume a good 90 minute run.)*\n\nI'd also look at undervolting videos for MSI Afterburner on YouTube. They'll give you a good idea about how to limit voltage (even if just a little bit) in order to help keep a GPU's clock at either stock or even slightly above mainly because you're trying to limit voltage and therefore heat. The latter basically is an attempt (and not all GPU silicon can do this) to undervolt and overclock at the same time, temperatures permitting.\n\nAs always, though, when doing any performance ***or*** stability testing (***initially***) it is a very good idea to kill background applications that you can safely do so with to help get consistent test results. Close things you don't need in the System Tray and open up task manager and turn off background apps that you know you can do this with. As for *long term stability* testing (*not performance verification*), it's better to have the usual suite of background apps ***running*** so that you an ensure your system's daily config is stable, too. This will allow you to turn off/kill applications one-by-one if stability suddenly becomes an issue until you either find the unstable program/background app or you find that your system, after the fact, is not as stable as you thought.\n\nThis can all take a while, of course. Good luck!", "ups": 2, "depth": 2, "author": "HateToShave", "replies": [{"selftext": " I appreciate all of information! I\u2019ll be well occupied diving into all of this!", "ups": 2, "depth": 3, "author": "ParadoxSquid", "replies": []}]}]}]}, {"selftext": "for gaming you are better off with PBO, it will boost up to 4.6GHz\n\nanyway 4.3GHz is average all core OC on Zen 2, pushing higher is kinda hard (Zen 1 and 2 barely scale above 4.3 - 4.4 for all core, some rare single ccd CPUs can do 4.5 - 4.6)", "ups": 1, "depth": 0, "author": "maze100X", "replies": [{"selftext": "I've seen a lot of mixed opinions on PBO and actually went that route first. Can you be more specific about why that would be advantageous? My CPU was a bit erratic hopping all around 3.4GHz - 5.7GHz and it was lagging when dragging my windows around my screen. So I pretty much instantly restarted and loaded optimized defaults. I've had to rely on just a handfuls of videos, articles, opinions, etc., and while I can conceptualize everything, I'm by no means educated enough to make an \u201cinformed\u201d decision by myself. \n\nEdit: 3.4 - 4.7GHz (5.7Ghz above was a typo)", "ups": 1, "depth": 1, "author": "ParadoxSquid", "replies": [{"selftext": "For Ryzen 3000, also make sure you're using the latest Windows Chipset Drivers from AMD and ensure you're, *at least*, using the Balanced Power profile to start.\n\nBut PBO and even setting LLC to like \\~20% in the BIOS can help. If you still have stuttering then make sure you RAM is not at defaults (2133) and that you at least have XMP/DOCP on (and 1.35volts for the DRAM voltage).\n\nEDIT: All-core on Ryzen is not the most efficient, but isn't terrible either. It's just that you're limiting the processor's ability to boost in single core situations (like a game, for example, but also potentially moving windows around the desktop from a \"feel\" perspective...)", "ups": 1, "depth": 2, "author": "HateToShave", "replies": [{"selftext": "I do have the latest chipset, also did a fresh install and upgraded to windows 11. The encryption and phone link is what got me to tip to the new OS. While I was at it I updated my bios as well. And I don\u2019t believe my setting are all core as when monitoring them in game and while under volting my GPU in afterburn they have different values. \n\nBios settings:\n\nTarget CPU Speed : 4300MHz\nTarget DRAM Frequency: 3200MHz\nTarget FCLK Frequency: 1800MHz\n* \ufeff\ufeffOverlocking PresetsAi Overclock Tuner \u2014 D.O.C.P. StandardD.O.C.P. \u2014 D.O.C.P DDR4-3200 16-18-18-31BCLK Frequency \u2014 100.0000Performance Enhancer \u2014 AutoMemory Frequency \u2014 DDR4 3200FCLK Frequency \u2014 1800MHzCore Performance Boost \u2014 AutoCPU Core Ratio \u2014 43                                                       CPU Core Voltage (1.28)                                                         CPU Core Voltage \u2014 Override 1.3                                             CPU SOC Voltage \u2014 1.2                                                             DRAM Voltage \u2014 1.35.                                                              VVDG CCD \u2014 1.                                                                         VVDG IOD \u2014 Auto.                                                                     CLDO VDDP \u2014 Auto                                                                  1.00 SB Voltage \u2014  1.                                                                  1.8V PPL \u2014 1.8", "ups": 1, "depth": 3, "author": "ParadoxSquid", "replies": [{"selftext": "If you haven't manually changed the CPU's multiplier then you're at a default CPU state assuming nothing else was changed other than DOCP. Just double check that \"1.2 DRAM Voltage\" as it should be at 1.35v for the DOCP setting with the 3200 RAM.\n\nAlso, just set your FCLK to 1600Mhz for stability if you've not played with IO voltages manually at this point. Unless you changed it to 1800Mhz yourself for some reason. Lastly, that 1.3v for SOC is way to high for Ryzen 3000. It should be around 1.1v at this point if, again, you're not actively trying to get your FCLK stable with a matching DRAM frequency. 1.3v is probably going to damage the CPU in shorter order than you'd like if left that way.", "ups": 1, "depth": 4, "author": "HateToShave", "replies": [{"selftext": "I\u2019m going to DM you pictures of my bios.", "ups": 1, "depth": 5, "author": "ParadoxSquid", "replies": [{"selftext": "Yeah, if you having an issue and I see it right away I can do my best, but no guarantees. ;)", "ups": 2, "depth": 6, "author": "HateToShave", "replies": [{"selftext": "I sent them over to you. What I\u2019m trying to solve is are those settings bad or will they destroy my PC. Another person commenting here he believes I\u2019m going to destroy my PC. I do want to note he said that with only the information from when I started this Reddit thread.", "ups": 1, "depth": 7, "author": "ParadoxSquid", "replies": []}]}]}]}]}]}]}]}, {"selftext": "&gt; Currently undervolted @ 1.28V\n\nThat is dangerously high voltage, actually. Far above the normal or safe load voltage for Zen2, which is below 1.2 by a good bit.\n\nFIT voltage is *not* safe for long term load usage, contrary to some early, now mostly shame-deleted Zen2 testing.\n\nYou should really not be doing fixed overclocks on any modern CPU architecture, the inability to adjust voltage dynamically cripples them, and anything approaching even stock boost performance requires unsafe voltage. The wide range of voltages used under different conditions means setting a fixed one will either rapidly destroy the CPU, absolutely ruin light load/windows scheduler (frantic core sleep/wake) performance, or both.", "ups": 1, "depth": 0, "author": "LongFluffyDragon", "replies": [{"selftext": "So I should leave it @ 1.42? I think you are a bit mistaken. The Stock CPU Voltage of the 3900x is literally 1.42 out of the box which I\u2019ve had mine for 3 years now. 1.28 is well within safe perimeters. I\u2019ve actually read a lot on this technology because well I bought a ton of AMD stock. The 3900x was the reason I invested, it\u2019s a 7nm chip running 24x12. That is insanely small and being it\u2019s all in a tiny package these CPU\u2019s are bound to run higher temps. 7nm is pretty much twice as dense as intel CPUs. I ran mine stock up till yesterday @ 1.42V stock.", "ups": 0, "depth": 1, "author": "ParadoxSquid", "replies": [{"selftext": "No, i am suggesting you should learn how voltage on Zen2 actually works before fiddling with it in a dangerous manner. You clearly dont understand how modern CPUs work and are trying to treat it like something from a decade ago.\n\nLeaving it on stock is certainly much safer than what you are doing, though.\n\nThe stock voltage is a range of roughly 1.1 to 1.5, varying greatly depending on load and amperage. CPUs have not had fixed voltage levels for generations now. Voltages over about 1.32 are only seen when waking a core rapidly, and under load and higher power draw, voltage will drop to about 1.125-1.15v, which is the maximum safe load voltage on most chips.\n\nIf you reset everything and observe the voltage of each core with hwinfo while doing various tasks, you can see it fluctuating across that range based on load. 1.45-1.5v will never show up long enough to be visible, but the brief spikes when switching/waking cores should still be recorded in the maximums.\n\nAnything over about 1.2v *will* cause measurable silicon degradation under heavy load, something both AMD engineers and independent testers have confirmed repeatedly.", "ups": 1, "depth": 2, "author": "LongFluffyDragon", "replies": [{"selftext": "\nI most definitely didn\u2019t just make that one voltage change. And I did my do diligence reading forums, watching videos from JayzTwoCents, and Gamers Nexus. I\u2019ve watched many of these videos countless times over the past three years and obviously lost count on how many articles I\u2019ve read. I really don\u2019t think that those channels and sources would have led me wrong.", "ups": 0, "depth": 3, "author": "ParadoxSquid", "replies": [{"selftext": "They would not, since they absolutely dont give you misinformation of the level you have stumbled over. You just misunderstood it, admittedly being \"new to OC\".\n\nWell, GN wont. JayzTwocents is an amateur that has been repeatedly called out (and at least apologized and corrected himself) for harmful misinformation, but i strongly doubt he would promote anything that dangerous. He certainly does not know better than AMD's own engineers or professional hardware overclockers and testers.\n\nNobody can stop you from destroying your hardware *and* performance at the same time, if you want to dig in your heels to \"avoid\" being wrong. And i can tell when i am wasting my time.", "ups": 1, "depth": 4, "author": "LongFluffyDragon", "replies": []}]}]}]}]}]}
{"post": {"title": "Any mention of FSR 3 improving up-scaling quality?", "subreddit": "Amd", "selftext": "Just as the title says, has anyone mentioned anything regarding an increase in up-scaling quality with FSR 3?\n\nEDIT:\n\nAnother redditer mentioned XeSS being available in CyberPunk 2077 and looking better than FSR 2.1.  This is my comment back. \n\nI did check this out.  I agree XeSS is better when comparing the quality preset to both XeSS and FSR 2.1.\n\nI  did just install the FSR 2.2 mod and FSR 2.2 is almost as good as XeSS.   There is an ever so slight difference with XeSS barely taking the  lead.\n\nI'd really like it if FSR went the DLL route to allow for easier in place upgrades like DLSS.\n\nAnother  thing I noticed is that FSR 2.2 has slightly better (1-4 FPS) increase  in the 99th percentile for FPS compared to FSR 2.1.", "ups": 93, "permalink": "/r/Amd/comments/16h5yh7/any_mention_of_fsr_3_improving_upscaling_quality/", "num_comments": 150}, "replies": [{"selftext": "I sure hope so. FSR2 on anything below Quality looks so ass. Some games aren't too bad on Balanced but I prefer not to use that setting.", "ups": 71, "depth": 0, "author": "geko95gek", "replies": [{"selftext": "True, especially any resolution below 1440p sucks ass if you use anything but quality preset.", "ups": 27, "depth": 1, "author": "raifusarewaifus", "replies": [{"selftext": "Even quality kind of sucks at 1440p\u2026 \n\nI struggle to see a difference with DLSS quality but I definitely see it with FSR. Mostly the notorious shimmer on trees and other foliage.", "ups": 54, "depth": 2, "author": "Rizenstrom", "replies": [{"selftext": "There's a mistake in Starfield's FSR2 implementation where Bethesda did not use negative mip-map bias to allow for better texture quality when upscaling. Oops.", "ups": 25, "depth": 3, "author": "JasonMZW20", "replies": [{"selftext": "\\+1 To the list of things Bethesda forgot to do. No worries though, the unpaid Bethesda Developers on Nexus Mods will take care of it.", "ups": 9, "depth": 4, "author": "C3H8_Tank", "replies": []}, {"selftext": "Also anisotropic filtering is set to 4x with no ingame option to modify it. Lol.", "ups": 13, "depth": 4, "author": "Darkomax", "replies": [{"selftext": "I fixed both of these in the Custom.ini and it made a very notable difference in visual quality. Specifically the texture resolution portion", "ups": 6, "depth": 5, "author": "sticknotstick", "replies": []}, {"selftext": "That\u2019s quite an oversight lol. I can\u2019t remember the last time I used anything but 16x. Is there even a performance difference?", "ups": 4, "depth": 5, "author": "Arickettsf16", "replies": [{"selftext": "Not really on any GPU made in the last 10 years.", "ups": 2, "depth": 6, "author": "Jagerius", "replies": []}, {"selftext": "There is a performance difference if the resources are already scarce. I remember forcing it at 16x on my GTX970 in The Witcher 3 made the average frame rate worse.", "ups": 1, "depth": 6, "author": "KingArthas94", "replies": []}]}, {"selftext": "I find that strange as it's one of the lightest things in terms of performance drops. I don't think I ever notice if performance drops, but I definitely notice the visual difference.", "ups": 1, "depth": 5, "author": "sdcar1985", "replies": []}]}, {"selftext": "I saw some mod enhancing textures on all upscalers, maybe it's this problem you're talking about!", "ups": 5, "depth": 4, "author": "MrPapis", "replies": []}, {"selftext": "This is actually quite common unfortunately.", "ups": 3, "depth": 4, "author": "hairycompanion", "replies": []}, {"selftext": "Is that why the train car in New Atlantis looks like it came from Minecraft?", "ups": 1, "depth": 4, "author": "Dehdstar", "replies": []}]}, {"selftext": "Yeah the shimmer doesn't bother me as much but the actual blurriness as you zoom into further away is horrible.", "ups": 5, "depth": 3, "author": "geko95gek", "replies": []}, {"selftext": "Even on a 4K monitor like I have, it can still be ass at quality. The ghosting can be nightmarish even with FSR 2.2", "ups": 2, "depth": 3, "author": "Dexter2100", "replies": [{"selftext": "Cyberpunk right?", "ups": 3, "depth": 4, "author": "FormalIllustrator5", "replies": [{"selftext": "Warframe for me, the ghosting on particles and water drives me nuts. I have a laptop with an Nvidia card, when I play on that while I\u2019m at work I get to use DLSS and all those issues go away.", "ups": 3, "depth": 5, "author": "Dexter2100", "replies": []}]}, {"selftext": "Same issue with DLSS", "ups": -4, "depth": 4, "author": "MrWeasle", "replies": [{"selftext": "Not with Warframe it isn\u2019t at least.", "ups": 3, "depth": 5, "author": "Dexter2100", "replies": [{"selftext": "It\u2019s game dependent, Dead Space had some rough ghosting with DLSS and not FSR. Of all the things DLSS is better at, I\u2019d say ghosting is one of the least significant advantages (DLSS basically always has an advantage in terms of image stability in motion, for example)\n\nI think FSR2 in quality looks great at 4k in most games (I have a 4080, haven\u2019t tested lower resolution). It\u2019s not as stable as DLSS, but it\u2019s much better than non-temporal upscalers and I think the performance gain is easily worth the tradeoff. DLSS is just\u2026 better than that lol. \n\nI\u2019ll have to try out XESS just for funsies, never had the occasion to yet.", "ups": 2, "depth": 6, "author": "HiCustodian1", "replies": []}]}]}]}, {"selftext": "On 4k quality suck hard, not on FSR 2.2, really hope that at least on \"Quality\" settings for FSR3 will be really good, at least = to DLSS 3.0 not 3.5 \n\nmy 5 cents...", "ups": 2, "depth": 3, "author": "FormalIllustrator5", "replies": []}, {"selftext": "Depends on the game, but starfield its pretty massive the quality vs quality between dlss mod and fsr2.", "ups": 3, "depth": 3, "author": "countpuchi", "replies": []}, {"selftext": "FSR2.2 isn't that bad but Any game that still uses fsr2.1 is just a nope for me.", "ups": 1, "depth": 3, "author": "raifusarewaifus", "replies": [{"selftext": "FSR 2.2 in Starfield is crappy. Texture shimmer all over the place. Upscaling to 1440p looks blocky and blurry. Sharpness filter is out of control. It is just so much worse than native rendering it's not even funny.", "ups": 16, "depth": 4, "author": "Enigm4", "replies": [{"selftext": "Just Fsr things", "ups": 4, "depth": 5, "author": "AdStreet2074", "replies": [{"selftext": "No its because they lock it to 4x anisotropic filtering and fucked up the mipmaps.", "ups": 4, "depth": 6, "author": "MrWeasle", "replies": []}]}]}]}]}, {"selftext": "Yup Quality is the absolute minimum I tend to go.", "ups": 7, "depth": 2, "author": "geko95gek", "replies": []}, {"selftext": "The funniest thing is that even on 4K the ghosting with FSR is horrible. I'll admit, I'm a huge Nvidia Fan boy, but I really want AMD to improve their tech so there's a healthy amount of competition. Many of the build setups I recommend to friends/colleagues are AMD builds since they are generally better on the prices and I really want those guys to benefit from Frame Generation and upscaling that's on par with DLSS.\r  \n\r  \n\r  \n\r  \nAI Upscaling tech is phenomenal and 100% the future of gaming (fingers crossed the next switch actually uses it). Seriously though, not having DLSS on the switch 2 will be a huge missed opportunity.", "ups": 2, "depth": 2, "author": "C3H8_Tank", "replies": [{"selftext": "AI upscaling is the future indeed but I hate to see it become too good either. When I turn on upscaling, I want 120fps at the very least on ultra settings at my desired resolution. Now the reality is games are so reliant on them getting 60fps on a slightly higher resolution is all the upscaling can do.  It becomes a crutch for lazy ass developers or greedy developers to push deadline forward instead.", "ups": 2, "depth": 3, "author": "raifusarewaifus", "replies": []}]}, {"selftext": "Because you shouldn't use upscaler on any resolution below 1440p", "ups": -1, "depth": 2, "author": "MrWeasle", "replies": [{"selftext": "It depends. On my ROG Ally on its 7\" 1080p screen FSR looks good. It's difficult to see the flaws on a screen that small and the performance gains are good. I can't same the same about FSR on my 32\" 4K display though, I can't stand it and rarely use it due to the shimmering and other crap.", "ups": 4, "depth": 3, "author": "random_reddit_user31", "replies": [{"selftext": "It has something to do with ghosting and inverse ghosting", "ups": 0, "depth": 4, "author": "MrWeasle", "replies": []}, {"selftext": "Yep. I have an ROG Zephyrus 14\"\" laptop with a 6700S and if FSR2 is available I'm using it. Not noticeable at all. If it's slightly blurry, I'll use Radeon Image Sharpening to touch up the final image.", "ups": 1, "depth": 4, "author": "chsambs_83", "replies": []}, {"selftext": "I set my Ally to 960x540 resolution in Starfield, lowest settings, max FSR scaling and it looked like a pixelated mess.  Almost like a retro game!  It was getting close to 60fps at like 20 watts though.  Haha", "ups": 1, "depth": 4, "author": "XXNameAlreadyTakenXX", "replies": []}]}]}, {"selftext": "[removed]", "ups": 1, "depth": 2, "author": "[deleted]", "replies": [{"selftext": "Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*", "ups": 1, "depth": 3, "author": "AutoModerator", "replies": []}]}, {"selftext": "True. However even with the quality present its tolerable but the image qualilty downgrade is too obvious already and further lower options is atrocious (balanced,performance). I really support FSR because is platform agnostic so no matter which card you've IT RUNS but...... it really needs a big lift and thats the expectation i guess for everyone with FSR 3.", "ups": 1, "depth": 2, "author": "Thorwoofie", "replies": [{"selftext": "TBH, compared to your typical upscaling FSR is probably the best out there. It only loses to ML-based upscasling so far. There is also TSR by unreal engine which is also pretty good but it also suffers from the same problem as FSR2 in creating thin lines such as Electric wires on the road.", "ups": 0, "depth": 3, "author": "raifusarewaifus", "replies": []}]}]}, {"selftext": "Quality on fsr2 is bad as well.  I'm rly hoping fsr3 pans out to be good otherwise I'm biting the bullet with nvidia", "ups": 7, "depth": 1, "author": "GPopovich", "replies": [{"selftext": "I think it's decent at 1440p. On 4k, I think quality is really good and balanced is again only decent (in a vacuum, that is. I haven't compared it to DLSS). \n\nOn anything less than quality or when using it at 1080p at all, it is pretty terrible.", "ups": 0, "depth": 2, "author": "sparkythewildcat", "replies": [{"selftext": "If you haven't compared with DLSS remain in the dark or you will ruin it.", "ups": 7, "depth": 3, "author": "swear_on_me_mam", "replies": [{"selftext": "I mean, is dlss so good that a 4060ti can give better image quality than a 7800xt at the same fps due to dlss looking that much better? \n\nLike if 1440p scaled from a low enough res that a 4060ti could match the frame rate of a 7800xt and still look better than 1440p quality fsr then I'd regret going AMD, but I highly doubt that's the case.", "ups": -1, "depth": 4, "author": "sparkythewildcat", "replies": [{"selftext": "DLSS at the same setting will always look better than FSR and I will hapily claim that DLSS balanced will look better than FSR quality 99% of the time and running those settings there may be instances where the 4060ti is faster. Would depends entirely on the game tho.", "ups": 5, "depth": 5, "author": "swear_on_me_mam", "replies": []}, {"selftext": "At lower resolution, that can actually be the case.", "ups": 3, "depth": 5, "author": "Knjaz136", "replies": []}]}]}]}]}, {"selftext": "If u use fsr balanced on rdr2 the game is literally broken and crash every time you enter a building", "ups": 2, "depth": 1, "author": "lil_hajzl_smejd", "replies": []}, {"selftext": "If fsr quality is not enough for what fps u need, just activate RSR, upscale from 1440p to 4k and u will get a beautiful image quality and a huge boost in fps.", "ups": -3, "depth": 1, "author": "Evening_Sir_6320", "replies": [{"selftext": "That just shows how much of a broken mess FSR is right now lol", "ups": 5, "depth": 2, "author": "geko95gek", "replies": []}]}, {"selftext": "it looks poor at quality and even native, especially the temporal artifacting around moving things ruins it", "ups": 1, "depth": 1, "author": "Alien_Cha1r", "replies": []}]}, {"selftext": "The people that have seen it already can't go into details but seem to be impressed.\n\nI don't even want it to upscale better (not that I'd obviously mind if it does) they just need to get rid of the artifacts. \n\nThough as much as dlss is better at low resolutions people seem to forget it was also awful with artifacts until it's more recent versions. So they've only just got on top of it too.\n\nIt wouldn't take much to make it better at 4K either. At 4K it's usually comparable to dlss or only slightly worse.\n\nBoth companies could really do with trying to make it driver based though at least partly so it can be easily upgraded for older games. At least Nvidia owners get to just swap a DLL.", "ups": 11, "depth": 0, "author": "paulc19802", "replies": [{"selftext": "&gt;At least Nvidia owners get to just swap a DLL.\n\nThat isn't always possible. For example, in MW2, Battle.net auto replaces the new DLL with the old one. As soon as you replace it, it auto runs a file verification cycle and overwrites the new DLL.", "ups": 4, "depth": 1, "author": "Worldly_Pound_7625", "replies": [{"selftext": "Aye. Like I said should just be driver level. A drop down where the default is labeled and you can change it if you wish with a note it can cause issues in some games.", "ups": 3, "depth": 2, "author": "paulc19802", "replies": [{"selftext": "You shouldn't be able to change it, because some dummy will accidentally use an old version and then complain. It should be like app updates, with no option to downgrade. Besides, why would you want to downgrade anyway?", "ups": 1, "depth": 3, "author": "Worldly_Pound_7625", "replies": []}]}]}, {"selftext": "Yeah I'm really just talking about artifacts when I mention quality.  I really hope there are some good improvements with FSR 3.\n\nWith that said, do we think it'll be another year until we get FSR 3.5 RR?!  haha", "ups": 2, "depth": 1, "author": "brumsky1", "replies": []}, {"selftext": "&gt;Though as much as dlss is better at low resolutions people seem to forget it was also awful with artifacts until it's more recent versions. So they've only just got on top of it too.\n\nYep, the only *just* got on top of it with DLSS 2 released 3 and a half years ago.", "ups": -7, "depth": 1, "author": "BinaryJay", "replies": [{"selftext": "When DLSS 2 first came out it had awful ghosting and shimmering.\n\nThus why people replace the DLL of older games.", "ups": 6, "depth": 2, "author": "paulc19802", "replies": []}]}]}, {"selftext": "You don't think AMD would have mentioned this if it were the case. They would have spent 5 whole minutes talking about enhanced image quality if it existed.", "ups": 10, "depth": 0, "author": "KARMAAACS", "replies": [{"selftext": "Well if it is just an update to FSR 2.2 as in going to 2.3, maybe not...", "ups": 1, "depth": 1, "author": "brumsky1", "replies": [{"selftext": "FSR 3 is just the marketing name, following the DLSS marketing and naming. No doubt if AMD ever make a \"ray reconstruction\" competitor, it'll be called FSR 3.5. In all honesty this \"FSR 3.0\" is just FSR 2.2 + Frame Generation.", "ups": 9, "depth": 2, "author": "KARMAAACS", "replies": []}]}]}, {"selftext": "AMD has said FSR3 \"also includes the latest version of our temporal upscaling technology used in FSR 2 which has been optimized to be fully integrated with the new frame generation technology\".", "ups": 21, "depth": 0, "author": "CatalyticDragon", "replies": [{"selftext": "They also added, \"However, our main focus with AMD FSR 3 research and development has been on creating high-performance, high-quality frame generation technology that works across a broad range of products.\"\n\nFSR is still on revision 2.2  \nI'm hopeful that AMD will update FSR 2, they were radio silence all year, now we know why, they were focusing on FSR 3... but FSR 2 upscaling, for sure needs an update.", "ups": 15, "depth": 1, "author": "Harbi117", "replies": [{"selftext": "FSR3 and FG are different features, on \n\nFSR3 we must get quality improvement and FPS boost (otherwise will not make sence) \n\nFG - will be a additional button to switch, to start the actual frame generation. \n\n&amp;#x200B;\n\nSo ....we hope for better results in terms of quality..", "ups": -5, "depth": 2, "author": "FormalIllustrator5", "replies": []}]}, {"selftext": "Latest version of our temporal upscaling technology used in FSR 2\n\nand \n\nOptimised for frame gen\n\nThis really sounds like they took FSR 2.2 and incorporated it in the render queue with frame gen in an optimised way. \n\nIf there were a visual update AMD would have spoken about it on gpuopen. Additionally, they mention FSR 2 specifically and that indicated that there hasn't been any real progress with image reconstruction as the focus goal was frame gen.\n\nFSR 3 is now a term just like DLSS 3 and consists of multiple technologies.\n\nI hope I'm wrong, I really really do. FSR image reconstruction is in need of an update.", "ups": 7, "depth": 1, "author": "Accuaro", "replies": []}]}, {"selftext": "My current understanding is that they'll have a version bump on traditional FSR upscaling, optimizing it with generated frames, but I wouldn't expect a drastic change in quality. I think artifacting and shimmering will be a thing sadly due to AMD not wanting to utilize AI in pursuit of wider support. Hope I'll be proved wrong and we'll get a DLSS-like experience but feels far-fetched.", "ups": 36, "depth": 0, "author": "sad7900xtxuser", "replies": [{"selftext": "I wonder if they are going to use the \"AI Cores\" for anything...  Or if those are just a \"We have it too\" feature.\n\nI could see and understand a version of FSR that uses AI to fix those issues though.  FSR 2 for cards without AI and FSR 3.x that uses the AI cores.", "ups": 10, "depth": 1, "author": "brumsky1", "replies": [{"selftext": "I think it's there for gpgpu applications like stable diffusion and llama, I'd love to have a RDNA 3 \"FSR AI\" alternative to standard FSR 2 upscaling for the cards that have the AI cores, but I doubt they'll opt for that. Hope it helped, so I can focus on my work now!", "ups": 14, "depth": 2, "author": "sad7900xtxuser", "replies": [{"selftext": "I meeeeeeaaan there is XeSS and at least in cyberpunk it looks much better than FSR, doesn\u2019t have that annoying noise all over the screen.", "ups": 10, "depth": 3, "author": "CeladonBadger", "replies": [{"selftext": "I did check this out.  I agree XeSS is better when comparing the quality preset to both XeSS and FSR 2.1.\n\nI did just install the FSR 2.2 mod and FSR 2.2 is almost as good as XeSS.  There is an ever so slight difference with XeSS barely taking the lead.\n\nI'd really like it if FSR wen the DLL route to allow for easier in place upgrades like DLSS.\n\nAnother thing I noticed is that FSR 2.2 has slightly better (1-4 FPS) increase in the 99th percentile for FPS compared to FSR 2.1.", "ups": 3, "depth": 4, "author": "brumsky1", "replies": []}, {"selftext": "As far as i know XeSS cost 10 times more to compute than FSR. So quality setting wont really improve frame times.", "ups": 6, "depth": 4, "author": "Yilmaya", "replies": [{"selftext": "That\u2019s a given since it is using AI, if there was AI based FSR it would also be inherently heavier. Since RDNA3 natively supports DP4a instruction set XeSS is running pretty well on 7000 series cards, it definitely is for me, gives good gains in quality mode too.", "ups": 4, "depth": 5, "author": "CeladonBadger", "replies": []}]}]}]}, {"selftext": "Those AI cores are used for running and training AI models. They are also used during AV1 encoding to increase/maintain image quality.", "ups": 5, "depth": 2, "author": "punished-venom-snake", "replies": []}]}, {"selftext": "You don't need AI, XeSS DP4a is a lot more stable than FSR 2. I'm actually using it in Starfield.", "ups": 5, "depth": 1, "author": "Darkomax", "replies": []}]}, {"selftext": "FSR 2 is 4th in upscaling\n\nDLSS, XeSS and even Apple's Metal FX Upscaling looks better. All of them using ML-based upscaling.\n\nI do not know how AMD is proud that their upscaling model doesn't use AI and is the worst of the lot.", "ups": 38, "depth": 0, "author": "From-UoM", "replies": [{"selftext": "UE5's upsscaler and even Insomniac's internal upscaler are also both better than FSR.", "ups": 7, "depth": 1, "author": "Jon-Slow", "replies": []}, {"selftext": "It's actually not even the best in terms of non-ML upscaling either. XeSS quality is lower in non-intel gpus but it is still better than fsr2. lol", "ups": 2, "depth": 1, "author": "raifusarewaifus", "replies": [{"selftext": "No XeSS on non-intel GPUs is still ML-based. It runs on a different pathway. DP4A instead XMX.\n\nThis makes it less accurate and slower.\n\nYou know how you can emulate games on a pc? Its sort of like that (vastly oversimplifying here) but running differently causing it to be slower and less accurate.\n\nBut that less accurate version of XeSS 1.2 is still better than FSR in quality terms which says a lot", "ups": 24, "depth": 2, "author": "From-UoM", "replies": [{"selftext": "It does, and there's a lot of hopeful people on this sub blaming developers for not being at the level of DLSS saying that it is a bad optimisation of FSR 2.\n\nWhile that may hold true for some games, it is also a fact you just can't make FSR 2 any better or close to DLSS. It's like trying to draw blood from a stone, it's all she's got.", "ups": 9, "depth": 3, "author": "Accuaro", "replies": []}, {"selftext": "I modded XeSS 1.2 into Starfield and while it reduces flickering on thin objects there are significantly more ghosting/trailing on characters. I wonder if future native implementation would be better.", "ups": 2, "depth": 3, "author": "Dos-Commas", "replies": []}, {"selftext": "Agreed to this. Intel Xess is better on image quality wise than FSR 2. Intel XeSS is also better on CPU limited scenarios FPS wise more than FSR 2. Though FSR 2 seems better on GPU limited scenarios. I'm using RX 7600 and a Ryzen 5 5600 and I prefer using Intel XeSS. Hope AMD FSR 3.0 would make use of RDNA 3 AI cores.", "ups": 0, "depth": 3, "author": "jdrysrmrz", "replies": []}]}]}, {"selftext": "I would say it highly depends on a game. Last game where I tested it was Atomic heart and FSR looked better then XeSS", "ups": 1, "depth": 1, "author": "MrTytanis", "replies": [{"selftext": "I suggest trying the new XeSS 1.2\n\nCyberpunk got it and it looks quite good", "ups": 1, "depth": 2, "author": "From-UoM", "replies": [{"selftext": "XeSS 1 was already better with it being accelerated via XMX, and 1.1 made it better than FSR 2.2 albeit with less performance gain.\n\nReally happy with how Intel is continually improving its image reconstruction technology. It's nice to see a 3 player GPU market, even if Intel is small right now.", "ups": 2, "depth": 3, "author": "Accuaro", "replies": []}]}]}]}, {"selftext": "AMD marketing is absolutely crap. FSR3 should have been launched within a week of the announcement but instead we get some vague timescale of 'early fall'. The hype will have died down by the time they add it to Forspoken or any other game.  Maybe it's not that good hence the poor handling of the 'launch'.", "ups": 18, "depth": 0, "author": "nas360", "replies": [{"selftext": "AMD\u2019s failure to include FSR3 with Starfield was such a missed opportunity. I have to agree with Tom and Dan from MLID when they recently stated on the Broken Silicon podcast that AMD\u2019s marketing team needs to be sacked. They\u2019ve had so many opportunities to pull ahead of the competition and gain market and mind share but they\u2019ve routinely failed to capitalise on them.", "ups": 20, "depth": 1, "author": "TheContingencyMan", "replies": [{"selftext": "AMD just needs to hire their critics as their new marketing team.", "ups": 3, "depth": 2, "author": "PsyOmega", "replies": [{"selftext": "I've said it for years, I'd do it but... too bad I'm not \"smart\" enough or worked at Alienware to be hired.", "ups": 2, "depth": 3, "author": "KARMAAACS", "replies": []}]}]}, {"selftext": "&gt; AMD marketing is absolutely crap.\n\nAlways has been, always will be. I mean is this surprising from the same company that brought you \"Radeon Rebellion\", \"Poor Volta\" and \"Two RX 480's is more powerful than one GTX 1080\". I mean... c'mon, this is par for the course for AMD.\n\n&gt; FSR3 should have been launched within a week of the announcement but instead we get some vague timescale of 'early fall'. \n\nIt's because so many people have been asking them \"where is FSR3\" and so they had to give some sort of update, they also promised it last year with the RDNA3 launch to come out sometime this year. Considering Hyper-RX is late, this isn't exactly shocking. AMD almost never brings out a feature on time.\n\n&gt; The hype will have died down by the time they add it to Forspoken or any other game. Maybe it's not that good hence the poor handling of the 'launch'.\n\nBingo. It's AMD tech so expect it to be poor at the beginning and to finally work it way up to being usable maybe 3 years later. No doubt games will implement it, but don't expect it to be any good just like FSR2. I hope it doesn't become vaporware or EOL like CAS became where no one uses it.", "ups": 5, "depth": 1, "author": "KARMAAACS", "replies": []}, {"selftext": "A lot of people think of FSR as being AMD's DLSS. Windows gaming media, the typical PC gamer, often even AMD's marketing department. But the reality is more complicated than can be explained in a press release or a marketing presentation whose main goal is to sell OEMs on a GPU by giving them another feature bullet point.\n\nDLSS is easy to understand. It is fully owned and controlled by Nvidia, it works on their modern hardware, with their driver, which makes it a major selling point for them. It satisfies Windows OEMs selling systems with Nvidia's GPUs that they have a bullet point to add.\n\nXeSS is currently similar with the exception that it has a fallback that works on other hardware, at least on Windows. Again, it satisfies OEMs shipping Windows systems with Intel GPUs.\n\nFSR by contrast is complicated. AMD sponsors the open source GPUopen project, but that doesn't really mean FSR is an AMD product, or that its developers treat updates to it as a selling point for specific AMD hardware releases.\n\nAMD hasn't made their own DLSS or XeSS because they have so little of the Windows OEM market, and making one wouldn't really gain them anything. OEMs care about the bullet point, and not much beyond it. FSR is enough to satisfy that. Think of all the Intel systems sold with 'DirectX 12 capable' stickers despite having outdated GPUs that can't really run modern games. \n\nSo much of what AMD is doing can be understood when you look at how small their GPU business really is, and know the only real way to change that is getting OEM contracts. Attempts to appeal to enthusiast system builders has gained them less marketshare in recent years, than a single mainstream low-end Linux gaming device.", "ups": 3, "depth": 1, "author": "mad_mesa", "replies": [{"selftext": "When you account for the console chips from AMD, they control close to half the graphics chip market for gaming in general. \n\nThat said, it explains why AMD is interested in development of open upscaling tech that is applicable to existing modern gaming console chips. (as well as discrete pc graphic cards).\n\nAMD makes far more money from their console partners than PC gpus. Anything they come up with will likely be applicable to the PS5 and Xbox.", "ups": 4, "depth": 2, "author": "Ecredes", "replies": []}]}, {"selftext": "The only options are \"AMD promised FSR3 and hasn't said anything new\" or \"AMD talked some more about FSR3 but it's not here yet\". Regardless of what AMD picks from these, people will complain.", "ups": 1, "depth": 1, "author": "ET3D", "replies": [{"selftext": "Because it should've been here a year ago, if they actually were serious about it. But they clearly aren't, they're slow to the market with features copypasted from Nvidia - except done worse.\n\nPeople have the right to call it out and complain, maybe one day AMD will pick themselves up and actually get serious about their graphics cards' feature set.", "ups": 4, "depth": 2, "author": "heartbroken_nerd", "replies": []}, {"selftext": "Looking how DLSS 3.5 is, i cant imagine what DLSS 4 will be.... I am start thinking i made a mistake as clearly N$vidia is improving with big steps...", "ups": 5, "depth": 2, "author": "FormalIllustrator5", "replies": [{"selftext": "DLSS4 could be anything and that is quite exciting.", "ups": 2, "depth": 3, "author": "heartbroken_nerd", "replies": [{"selftext": "Nvidia ships LSD directly to every user of their 4000 or 5000 series cards and you trip your balls off thinking those 54 frames are actually 300+", "ups": -1, "depth": 4, "author": "ZeinThe44", "replies": []}]}]}]}]}, {"selftext": "What I don't understand - is the quality deficit of FSR compared to DLSS due to a failing of AMD, or is it a consequence of not using any specialised AI hardware - a compatibility/quality tradeoff?\n\nIf it's the latter, then surely it'll never be able to achieve quality parity with DLSS in its current form.", "ups": 7, "depth": 0, "author": "Bostonjunk", "replies": [{"selftext": "Not just ai hardware, it's the ai models Nvidia has trained on their super computers.", "ups": 8, "depth": 1, "author": "NewestAccount2023", "replies": [{"selftext": "Which is why image reconstruction tech such as DLSS can* be better than native. Games can sometimes suffer from moire patterns and DLSS can effectively fix that. Something that FSR doesn't currently do.", "ups": 6, "depth": 2, "author": "Accuaro", "replies": []}]}, {"selftext": "In general, yes, these days it can be easier to throw a task at AI, find out how to train it effectively, and get something that's better than a handcrafted algorithm.\n\nThat said, it's a ton of work, both by people and by computers. NVIDIA probably invests several times more money than AMD on this. So even if it's technically possible for AMD to catch up, it probably won't if it doesn't decided to make it a priority.\n\nIt was said at one point (about two years ago) that Microsoft is working on AI upscaling for Direct3D. Sadly nothing came of it, but that would have been a good way to go about it.", "ups": 8, "depth": 1, "author": "ET3D", "replies": [{"selftext": "&gt;It was said at one point (about two years ago) that Microsoft is working on AI upscaling for Direct3D. Sadly nothing came of it, but that would have been a good way to go about it.\n\nMost likely they ran into the same issues Nvidia did with AI upscaling (as in DLSS1) and it was abandoned, like again, nvidia did (but they pivoted to reconstruction instead).", "ups": 3, "depth": 2, "author": "JarlJarl", "replies": []}]}, {"selftext": "I think it probably is a combination of the two. The difference in the method they use to achieve the upscaling, combined with a possible resource under-investment are the most probable explaination as I see it. But after all, I am no expert.", "ups": 2, "depth": 1, "author": "Thesadisticinventor", "replies": []}, {"selftext": "&gt; What I don't understand - is the quality deficit of FSR compared to DLSS due to a failing of AMD, or is it a consequence of not using any specialised AI hardware - a compatibility/quality tradeoff?\n\nAlmost certainly. FSR2 uses hand-tuned heuristics to upscale, this means that the model will inherently have faults or shortcomings, as good as humans are, they don't have the level of compute capability of an evolving AI model which is built to look at tonnes of images and constantly improve itself. A human can only use so much data to make a good looking algorithm. The AI model can compare pixel-to-pixel of billions of  reference images and use that to improve its model. An AI model can see way more data and this will inherently make it far more accurate at upscaling to almost native quality. Eventually DLSS will reach a level where it's basically native quality or better, with better temporal stability than today where it's almost got no ghosting.\n\nAMD will have to swallow the pill eventually and move to an AI based approach, but NVIDIA's already so far ahead with DLSS that I doubt AMD will ever catch up. Not to mention NVIDIA's whole company focus is AI, they've been doing the whole AI thing for over a decade, so I just can't see AMD making a better approach than DLSS ever, not unless AMD somehow start making better hardware to train an AI model faster than NVIDIA's hardware. Even then NVIDIA might just throw more money at the problem and keep their superiority in this area.", "ups": 1, "depth": 1, "author": "KARMAAACS", "replies": []}]}, {"selftext": "They said that this wasn't their main goal with fsr 3. So maybe slightly better, but don't expect dlss level", "ups": 5, "depth": 0, "author": "MrTytanis", "replies": [{"selftext": "I'm hoping they can reduce the artifacts more than a quality improvement.", "ups": 1, "depth": 1, "author": "brumsky1", "replies": [{"selftext": "Shimmering is my main issue with fsr. Apart from that I would use it even at 1080p", "ups": 2, "depth": 2, "author": "MrTytanis", "replies": []}]}]}, {"selftext": "From what I recall in a recent interview Scott Herkelman said that they focused on the frame generation aspect more rather than the upscaling part...", "ups": 3, "depth": 0, "author": "Archer_Gaming00", "replies": []}, {"selftext": "I hope so\nThat\u2019s all I care about, I don\u2019t even want FrameGen", "ups": 3, "depth": 0, "author": "schimmlie", "replies": [{"selftext": "I look forward to FG to help smooth out animations, assuming the quality is good.", "ups": 1, "depth": 1, "author": "brumsky1", "replies": []}]}, {"selftext": "I hope so, shimmers are way too bad", "ups": 3, "depth": 0, "author": "Magnar0", "replies": []}, {"selftext": "I wish devs would update their FSR versions as well. Yes, I mean you Capcom.\n\nRE4 looks better with even XeSS than their implementation of FSR 2.1.", "ups": 3, "depth": 0, "author": "LeiteCreme", "replies": []}, {"selftext": "Yeah, it would be nice.  With FSR1 being tapped out and bad, and FSR2 seemingly not going to improve much if at all at this point, with AMDs market share my current concern is that developers won't even bother to implement FSR once the buzz around it is over if FSR3 doesn't pane out to be at least as good as DLSS. \n\nDLSS is superior and a vast majority of users can use it, and this will only increase over the next generation or two as people upgrade non-supported Nvidia cards.  AMD needs to either have a superior product (at least on their own hardware), start slinging video cards at cost to increase their market share or do something at the driver level / make it as easy as a developer spending 5 seconds to click a toggle to enable it without needing to test.", "ups": 3, "depth": 0, "author": "red_dog007", "replies": []}, {"selftext": "AMD shouldn't say anything about it until/ unless they can get it actually working well. No point in building up any hype until they know that their software people have got something good that's working and not too difficult to implement in games. \n\nUnder-promise, over-deliver is a good motto to live by for marketing.", "ups": 3, "depth": 0, "author": "UninstallingNoob", "replies": []}, {"selftext": "Pixel peeping XeSS and FSR2 in 3DMark's Frame Analysis feature, XeSS is much higher quality and can take an obscenely noisy image and clean it up. This has a higher processing cost on non-Intel GPUs though. DLSS doesn't offer the Frame Analysis feature.\n\nAMD is trying to keep FSR's wide compatibility, but I do think that will hold it back, in terms of ultimate image quality. Moving to RDNA1+, Turing+, and Xe1+ support only might allow AMD to use algorithms better tuned to modern GPUs. \n\nNewer GPUs should add the ability to do targeted upscaling where the base framebuffer is kept at native resolution, but the scene is broken up into resolution tiles. This isn't too different from VRS, but instead of shading rate, it allows piecemeal resolution within the rendered scene. Could be better option than dynamic resolution scaling which changes the entire frame to a lower resolution.", "ups": 3, "depth": 0, "author": "JasonMZW20", "replies": []}, {"selftext": "That's the idea.\nBut they can talk all they want. We won't know for sure until it's out.\nHopefully before 2030 \ud83d\ude02", "ups": 10, "depth": 0, "author": "marvinnation", "replies": [{"selftext": "Wow someone is clearly a glass half full kinda person!  Haha", "ups": 3, "depth": 1, "author": "brumsky1", "replies": [{"selftext": "\ud83d\ude02 I expect the best but I'm ready for the worst", "ups": 6, "depth": 2, "author": "marvinnation", "replies": []}]}]}, {"selftext": "No", "ups": 4, "depth": 0, "author": "azael_br", "replies": []}, {"selftext": "if you have forspoken report back lmao", "ups": 5, "depth": 0, "author": "prisonmaiq", "replies": [{"selftext": "Forspoken have free demo so you can check it now and after fsr 3 came out", "ups": 2, "depth": 1, "author": "MrTytanis", "replies": [{"selftext": "I'm sure the five people playing the game will be happy when they include FSR3.", "ups": 4, "depth": 2, "author": "KARMAAACS", "replies": [{"selftext": "Sure thing it's not so good of a choice, but I guess these developers wanted to include it as fast as they can. Maybe they belive that performance is the issue", "ups": 2, "depth": 3, "author": "MrTytanis", "replies": []}]}, {"selftext": "The one time I tried the demo I couldn\u2019t even play it. The game ran fine but the controls were all over the place and the camera was bugged so I couldn\u2019t stop looking at the sky lol", "ups": 1, "depth": 2, "author": "Arickettsf16", "replies": [{"selftext": "That's peak of a gaming experience", "ups": 1, "depth": 3, "author": "MrTytanis", "replies": []}]}, {"selftext": "I don't think they update the demo.", "ups": 1, "depth": 2, "author": "DoktorSleepless", "replies": []}]}, {"selftext": "Haha I don't but good point.", "ups": 0, "depth": 1, "author": "brumsky1", "replies": [{"selftext": "If you have Forespoken report to prison.", "ups": 1, "depth": 2, "author": "Jon-Slow", "replies": []}]}]}, {"selftext": "If it was improving upscaling quality it would be pointed out in the presentation.", "ups": 1, "depth": 0, "author": "dmaare", "replies": []}, {"selftext": "It\u2019s adding a new AA method that will utilize compute shaders.  This will hopefully improve shimmering, which is FSRs weakness right now.  RDNA 4 will likely add some new method in compute to improve the new aa method.", "ups": 0, "depth": 0, "author": "XXNameAlreadyTakenXX", "replies": [{"selftext": "&gt;It\u2019s adding a new AA method that will utilize compute shaders\n\nWhat are you talking about? Source?\n\nOr are you talking about Native Anti-Aliasing? Well, that's just FSR2 with 100% resolution scaling. Some games have been doing it in the past, it's just not common. One example recently is Starfield with 100% resolution factor and FSR2. Nothing special.", "ups": 6, "depth": 1, "author": "heartbroken_nerd", "replies": [{"selftext": "Native aa is the name.  It is not in fsr2.  It\u2019s coming as part of fsr3 in response to dlaa which gives nvidia a more stable picture without shimmering.  It also comes at a slight performance cost because it uses compute.", "ups": -2, "depth": 2, "author": "XXNameAlreadyTakenXX", "replies": [{"selftext": "NativeAA is FSR2 with 100% resolution scaling.\n\n&gt;It also comes at a slight performance cost because it uses compute.\n\nSo does FSR2 in general compared to just dropping down the resolution target - a little overhead. Same thing.", "ups": 4, "depth": 3, "author": "heartbroken_nerd", "replies": [{"selftext": "It\u2019s FSR3 exclusive.\n\nhttps://gpuopen.com/fsr3-announce/\n\nmode\nWe\u2019re excited to introduce Native AA to FSR 3, which we know both devs and gamers have been asking for. Unlike our other modes, gamers can use FSR 3 without applying any upscaling, but they\u2019ll still get the high-quality AA and sharpening from FSR. There is a modest performance cost when using this new mode.\n\nNative AA can also be combined together with FSR 3 frame generation, providing a substantial performance boost3 .", "ups": 1, "depth": 4, "author": "XXNameAlreadyTakenXX", "replies": [{"selftext": "Going forward it's becoming standardized as a distinct feature for marketing purposes, is all.", "ups": 3, "depth": 5, "author": "heartbroken_nerd", "replies": [{"selftext": "Gonna have to agree to disagree.", "ups": 1, "depth": 6, "author": "XXNameAlreadyTakenXX", "replies": []}]}, {"selftext": "You can do this in any game with a res slider. Its not new. DLAA is also just DLSS at native.", "ups": 1, "depth": 5, "author": "swear_on_me_mam", "replies": [{"selftext": "Yeah, I could be wrong on this one.  Guess we\u2019ll have to wait and see.", "ups": 1, "depth": 6, "author": "XXNameAlreadyTakenXX", "replies": []}]}]}]}]}]}, {"selftext": "It's rendering at native with AMDs temporal AA, I damn hope it improves shimmering. That's not all though, particles are bad, disoclusion fizzle leaving trails, moire patterns, transparencies and especially ghosting.\n\nThis leaves nothing for the base tech that is image reconstruction though, so I'm hopeful it has improved somewhat.", "ups": 1, "depth": 1, "author": "Accuaro", "replies": []}]}, {"selftext": "yes they can do as the rebranding scheme used in RDNA3, so:\n\nFSR3 quality will be just like as native rendering\n\nFSR2 quality will be FSR3 balanced\n\nFSR2 balanced will be FSR3 performance\n\nFSR2 performance will be FSR3 ultra performance\n\n\"hey guys, we improved visual quality a bit\"\n\n![gif](emote|free_emotes_pack|sweat_smile)", "ups": -1, "depth": 0, "author": "pecche", "replies": [{"selftext": "&gt;FSR3 quality will be just like as native rendering\n\nLol no, you are so wrong. FSR 3 Quality, IQ wise will be the same as is FSR 2 Quality now, they are not changing upscale ratios (Quality is and will be 0,67x your native res.), they are just adding a native mode (FSR2 algorithm at 1x native res, so basically DLSAA counterpart).", "ups": 1, "depth": 1, "author": "Oxygen_pls", "replies": []}]}, {"selftext": "Only maybe if AMD stoped spending money paying off publishers to omit DLSS and minimize or remove RT implementations. That's a lot of money that could've gone into R&amp;D and hiring the right people. Maybe make a version of FSR that like DLSS runs on AI hardware and uses deep learning to improve over time.\n\n&amp;#x200B;\n\nBut in reality, no, and I don't think you should expect it to. I'm sorry but there has been enough time (4 years almost) for FSR to catch up with DLSS. DLSS once did a major jump going from 1 to 2 and then again with 2.5.1. And I think it got incrimentally better recently as well because the performance mode in Starfield looks incridibly close to Quality. This is while FSR looks garbage even at 4K quality mode and any modes below quality is just bad PR for FSR at this point.\n\n&amp;#x200B;\n\nIf FSR were to make that jump, it would've done so by now. I think the code not using the hardware for AI like DLSS and XESS on INTEL cards is the reason and I don't think AMD would spend any more money on it.", "ups": -1, "depth": 0, "author": "Jon-Slow", "replies": []}, {"selftext": "Will it be compatible with the ROG Ally?", "ups": -4, "depth": 0, "author": "Electrical_Dare7632", "replies": [{"selftext": "The base requirements for FSR3 are quite high, so I don't expect it to work well on integrated graphics, even though technically there's nothing preventing it from running.", "ups": 1, "depth": 1, "author": "ET3D", "replies": []}, {"selftext": "Yes", "ups": -2, "depth": 1, "author": "SquidDaddy81", "replies": [{"selftext": "So frame generation will be possible then?", "ups": -5, "depth": 2, "author": "Electrical_Dare7632", "replies": [{"selftext": "Possible, yes, but likely will have a performance impact that may not be worth it.", "ups": 2, "depth": 3, "author": "dirthurts", "replies": []}, {"selftext": "My understanding is that it should work across every GPU make and model\u2026 including AMD\u2019s competitors. That being said, I also wouldn\u2019t be holding out for a big bump in quality. Would be nice though!", "ups": 2, "depth": 3, "author": "SquidDaddy81", "replies": []}, {"selftext": "it uses hardware found in GPUs since 2014 so yes", "ups": 1, "depth": 3, "author": "EnderOfGender", "replies": []}, {"selftext": "if you can hold a 60fps minimum input to framegen, sure.  gonna be a silky smooth 144p image!", "ups": 1, "depth": 3, "author": "capn_hector", "replies": []}]}]}]}, {"selftext": "Really hope so :) Haven't seen it being mentioned anywhere though.. FSR can look pretty good in some games, but it can't compare to DLSS &amp; XeSS in image stability ;(", "ups": 1, "depth": 0, "author": "Drubban", "replies": [{"selftext": "Yeah agreed!  I'd also like to see the DLSS 3.5 RR feature added.  That looks amazing!", "ups": 2, "depth": 1, "author": "brumsky1", "replies": []}]}, {"selftext": "so where is fsr 3", "ups": 1, "depth": 0, "author": "Normandy_sr3", "replies": []}]}
{"post": {"title": "Ryzen 7 5800x - Overclocking", "subreddit": "Amd", "selftext": "Hey Everyone, \n\nI am curious if I enable PBO in BIOS does that mean I should not use Ryzen Master PBO and vice versa? I was thinking at first that I needed to enable it in BIOS so that it would work in Ryzen Master but then my PC crashed today. Looking into it I believe that they are separate ways to PBO your CPU. Which one would you recommend for someone that is just looking for a safe AUTO feature. I like to play CODMW2 \"competitively\" so I sacrifice most visual graphics to push frames and reduce as much input lag as possible. \n\nSpecs\n\nCPU: Ryzen 7 5800x\n\nGPU: 4070 AERO OC\n\nMobo: Gigabyte B550 Vision D-P\n\nRAM: 2x16gb 3600MHZ C18 Corsair Vengeance \n\nThanks in advance, ", "ups": 0, "permalink": "/r/Amd/comments/16hxakn/ryzen_7_5800x_overclocking/", "num_comments": 20}, "replies": [{"selftext": "Every chip is different PBO with a -20 to -30 works on most 5800x some can do +150 to +200 mhz offset. I can hit 5ghz but no higher or it crashes. Those are the settings from the bios.", "ups": 3, "depth": 0, "author": "xNIC0Nx", "replies": [{"selftext": "Yea, I did some Cinebench after setting all to -30 but I don't know what to do from there. There is also so many bios options that I am lost. Like should Turbo Mode be on as well? Its not even called Turbo but im not looking at it atm.", "ups": 1, "depth": 1, "author": "StandardBrute", "replies": []}]}, {"selftext": "I have not installed Ryzen master and do pbo settings in bios. Im using - 15 negative curve Optimizer and 150 MHz offset.\nFor some reason 200 MHz offset perform worse even though it's stable. One was talking about clock stretching, but thats my experience", "ups": 3, "depth": 0, "author": "RedhawkAs", "replies": [{"selftext": "yea, I'm still trying to learn what to do to even attempt the OC. I did set all my cores to -30 and then ran Cinebench.. It was stable. I was trying to follow a guide but it seemed like the guy was all over the place with his instructions and had no rhyme or reason. I was doing what he was doing and not getting any results.", "ups": 1, "depth": 1, "author": "StandardBrute", "replies": []}]}, {"selftext": "I use the auto CO feature in Ryzen master but I apply the result in the BIOS.", "ups": 2, "depth": 0, "author": "Le_Zouave", "replies": []}, {"selftext": "Not worth the headache for a 3% gain. No point going 200+FPS when the COD servers are running at 30 tick rate.", "ups": 2, "depth": 0, "author": "Dos-Commas", "replies": [{"selftext": "Yea, I agree that COD servers are trash but I do think I can get more than 3% out of the CPU and unfortunately, not only do I not have fiber and I still have cable internet due to availability, but I also play MnKB. lol So I'm trying to get everything I can. \n\nI did get a controller recently though playing it some here and there.", "ups": 1, "depth": 1, "author": "StandardBrute", "replies": [{"selftext": "I got 7% out of the bios auto OC.", "ups": 1, "depth": 2, "author": "quotemycode", "replies": []}]}]}, {"selftext": "I enabled pbo but as an undervolt. Stock performance at lower temps with a 200mhz boost. Just look it up on my. Enable pbo with a negative offset of 30", "ups": 2, "depth": 0, "author": "slicky13", "replies": [{"selftext": "Been trying to look it up but bios have so many extra settings from updates and then many videos and guides are conflicting etc.", "ups": 2, "depth": 1, "author": "StandardBrute", "replies": [{"selftext": "What board? Varies from vendor to vendor", "ups": 2, "depth": 2, "author": "slicky13", "replies": []}]}]}, {"selftext": "There is no such thing as \"safe auto OC\", unless you want meaninglessly tiny gains for a lot of extra heat. You can just turn on PBO and get a couple percent performance gain with a bunch of extra heat and noise..\n\nEither learn to do it properly, dont bother, or just give in and get a 5800X3D, increasing your performance maybe 10-20x as much as an overclock of a modern CPU could.", "ups": -3, "depth": 0, "author": "LongFluffyDragon", "replies": [{"selftext": "&gt; increasing your performance maybe 10-20x\n\nMore like 10%.", "ups": 5, "depth": 1, "author": "Dos-Commas", "replies": [{"selftext": "Luckily we have a wide range of well-respected hardware reviewers to test stuff like this, so we can ignore people like you who get upset enough about random inoffensive comments to say something inane and apply multiple downvotes to it in a span of 30 seconds.\n\nNo time was wasted.", "ups": -1, "depth": 2, "author": "LongFluffyDragon", "replies": [{"selftext": "&gt;Luckily we have a wide range of well-respected hardware reviewers to test stuff like this, so we can ignore people like you who get upset enough about random inoffensive comments to say something inane and apply multiple downvotes to it in a span of 30 seconds.\n\nWell, you know he's right. OC'ing since the whole Ryzen series has bin terminated pretty much as both AMD and Intel figured out ways to \"maximize\" their chips, on a automatic scale. The percentage gains you get these days is nothing compared to the chips of the past.\n\nI mean you buy a FX 3.5Ghz and you could easily stamp 4.6Ghz or even 5GHz out of it if you where lucky. That was 40% performance increase on a chip that was known to be \"slow\" in a era where single threading was still king.\n\nThe same was with Intel - many chips would be pushed over a GHz \"for free\" while today's avg OC is in the spectrum of only hundred of MHz. PBO pretty much seeks the maximum your chip can do or is allowed to.\n\nI mean you can go for a all core OC - and have less single threaded clock speeds - or you let PBO do it for you which is far more precise then All core is, and lose on some max all core OC's. Pick your side.\n\nI'd say if you want the best \"PBO\" just slap a big cooler on it. I've learned that most Ryzens like it when they are kept below 60 degree. With the newer series that will be, quite difficult. But you get the best of the whole free boosting thing. It just sticks longer and thus faster.", "ups": 1, "depth": 3, "author": "Jism_nl", "replies": []}, {"selftext": "&gt;Luckily we have a wide range of well-respected hardware reviewers to test stuff like this,\n\nWell go read some actual reviews then? 9% average increase on gaming performance at 1440P when moving from 5800X to 5800X3D. \n\nhttps://www.techspot.com/review/2451-ryzen-5800x3d-vs-ryzen-5800x/", "ups": 1, "depth": 3, "author": "Dos-Commas", "replies": []}]}]}]}, {"selftext": "Not sure I would trust the BIOS from a Gigabyte board. But that's just me...", "ups": -8, "depth": 0, "author": "HomePCRepair", "replies": [{"selftext": "Cryptic, unhelpful, and with a hint of elitism.  Thank you.", "ups": 10, "depth": 1, "author": "ProbeToUranus", "replies": []}]}, {"selftext": "&gt;2x16gb 3600MHZ C18 Corsair Vengeance\n\nJust about everything you do on the CPU side will be offset by the RAM you're running, unfortunately. It's very likely using Samsung C-Die IC's, which just is *not* good.\n\nYou might gain 3-4 FPS in the 1% and 0.1% lows with a good PBO tune and frequency override (keep the override in the +50MHz to +100MHz range to avoid clock stretching).\n\nHowever, if you traded that RAM out for either a confirmed 2x16GB dual rank kit, or a guaranteed dual rank 4x8GB setup, either at C14 or C16, you can add another 10 FPS to your 1% and 0.1% lows on top of the 3-4 FPS from the PBO tune, and add another 3-5 FPS more if you can get your FCLK to 1900MHz or higher and the ram to 3800MHz stable with tightened secondaries.\n\nThat being said, you can save yourself the trouble of buying and tuning better RAM if you instead sell the 5800X, swap it out for a 5800X3D, which will end up costing roughly the same as changing out the RAM for a better kit, but will bring up your 0.1% and 1% lows by 15-25 FPS without doing anything other than slotting in the X3D.", "ups": 1, "depth": 0, "author": "Rockstonicko", "replies": [{"selftext": "Much appreciated for your input thank you! I\u2019ll keep that in mind", "ups": 1, "depth": 1, "author": "StandardBrute", "replies": []}]}]}
{"post": {"title": "Anyone reached 6000mhz or close with Kingston Fury 6000Mhz (4x16)", "subreddit": "Amd", "selftext": "Hey guys, I saw a bunch of post about the subject but none about this specific memory. Im using BIOS 1654 here at my motherboard and tryed to manual set the EXPO configurations for my memory but when the system restarts I get stuck before the BIOS, so I have to CMOS. (I read one post explaning a step by step to maybe avoid that, but didn't work. It was a good try)   \n\n\nI know that sometimes the EXPO is a mess right now but want to figure if anyone have reached close to 6000mhz with manual set for the KF560C36BWEAK2-32  \n\n\nFunny fact, I was able to use the EXPO with 32GB but with 64 GB isn't \"working\".  \n\n\nRyzen 7 7800x3D   \nTUF GAMING X670E-PLUS  BIOS: 1654  \n Kingston Fury 6000Mhz KF560C36BWEAK2-32\n\nThanks", "ups": 12, "permalink": "/r/Amd/comments/16hb5yn/anyone_reached_6000mhz_or_close_with_kingston/", "num_comments": 23}, "replies": [{"selftext": "Sell all 4 modules get 2x32GB, problem solved.", "ups": 26, "depth": 0, "author": "speznatzz", "replies": []}, {"selftext": "4x anything is a lot harder on the IMC.  You have the choice of trying to loosen your timings compared to the EXPO profile, settling for a slower speed, or go down the rabbit hole of adjusting voltages like VDDP, VDDG, VSOC, VDDIO, etc.", "ups": 15, "depth": 0, "author": "DuskOfANewAge", "replies": [{"selftext": "I always see people say this but I think it has a lot to do with the CPU and MoBo. I\u2019m using 4 sticks of DDR5 (5600 CL40). I just used MSI\u2019s \u201cTest it\u201d function at 6000 CL36 in the BIOS and it\u2019s fine. I even stressed with MemTest86 and it looks good. Seems like &gt;6000 is where 4 DIMM configurations start having problems... or maybe I just hit the silicon lottery I dunno.", "ups": 2, "depth": 1, "author": "jekpopulous2", "replies": [{"selftext": "The CPU doesn't matter (apart from matching infinity fabric but that doesn't change the overclockability of the RAM) but the trace layout of the motherboard does matter. Sadly, the last T-topology (best for 4 DIMMs) motherboard was the X570M Pro4 (which is why I bought it)", "ups": 1, "depth": 2, "author": "RedTuesdayMusic", "replies": []}, {"selftext": "What motherboard do you have? I have b650 carbon wifi and 2x16 corsair vengeance and have been considering buying another kit.", "ups": 1, "depth": 2, "author": "Yubelhacker", "replies": []}]}]}, {"selftext": "I've got 4x16GB of the Trident Z Neo 6000mhz paired with a 7950X3D and ROG Strix X670-E F, and I'm running stable at 6400mhz CL32 with a slightly altered version of Buildzoid's Hynix 6000mhz timings.\n\nThe system was a PITA to get stable though, even at 6000mhz. The main advice I would give to you is that if you're judging RAM stability by having Event41 issues, then it's highly possible that it's instability on the CPU side. Or...it was in my case at least (CPU instability got notably worse when dialling in my RAM OC).\n\nEven when Core Cycler stopped picking up errors, I ultimately needed to back things off by 2 clicks on my Curve Optimiser per core settings to actually get properly stable.\n\nAnnoyingly for me, one of my non v cache cores needs +9 just to be stable at stock frequency, so turning off PBO / Curve Optimiser altogether and running at stock wasn't stable either.\n\nIf you're getting RAM test errors, then that's different obviously.\n\nAnyway, the voltages / settings which worked for my were setting FCLK at 2133mhz (or 2033 for 6000mhz), UCLK =MCLK (i.e., 3000mhz for 6000mhz RAM frequency), SOC voltage to 1.275v, MCIO to 1.4v, misc voltage to 1.1v, VDDP at 0.950v, DRAM VDD at 1.5v (1.4v should be fine for 6000mhz), DRAM VDDQ at 1.4v (1.3-1.35v should be fine for 6000mhz) and use the sync all PMIC voltages option to take care of the advanced voltages for you.\n\nI left memory context restore on (although YMMV), and dropped my Prod ODT to 28ohms (may not be necessary for 6000mhz, but always good regardless if your sticks / mobo can do it). \n\nThere's some additional settings hidden away in one of the Asus menus that claim to provide extra stability at high memory frequencies, so I enabled them too (albeit I'm not sure they're actually doing anything due to me already having manual timings entered).", "ups": 3, "depth": 0, "author": "BigGirthyBob", "replies": []}, {"selftext": "I run 4x16 at 6000 EXPO profile on 7950X no problem since the last two AGESA releases (Gskill).", "ups": 3, "depth": 0, "author": "BinaryJay", "replies": []}, {"selftext": "I don't think you can run 4 sticks at speed on intel either. If you want to run fast ram at 6000 or better, stick to 2 sticks.", "ups": 3, "depth": 0, "author": "joeh4384", "replies": []}, {"selftext": "On AM4, I was running 4x8gb ddr4 3600mhz with timing set to 16-19-19-forgot. Pc boots at 1.4v but I pumped 1.45v and left it. No stability issues and ran memtest for half a day.\n\nI guess you can try upping the voltage to 1.45 and be happy if it works.\n\nCurrently using 6000mhz ddr5 on 7800x3d as well but with only two dimms at 32gb total.", "ups": 2, "depth": 0, "author": "Medical-Tomorrow7727", "replies": []}, {"selftext": "Same ram here but 2x16GB on msi pro b650a wifi with 7600X and i have 6000mhz(30-36-36-38) with improved timings working flawlessly since last bios update!", "ups": 2, "depth": 0, "author": "johnspan89", "replies": [{"selftext": "So can you change timings even if the kit is rated at higher CL?\n\nI got an ASUS TUF B650 Plus, 7600x and 2 x16 Kingston Fury Renegate Silver RGB (rated 6400mhz @CL32) and been able to use them at 6000mhz stable for months with DOCP profile and just increasing CPU Voltage a bit at around 1.25V. I did try once to get to the advertised 6400mhz but it won't even boot i get bsod.\n\nAlso i am not using the latest BIOS, did they make any improvements? I am using the one before the last one.", "ups": 1, "depth": 1, "author": "SonOfAnarchy91", "replies": [{"selftext": "Not all Ryzen 7xxx I/O Die can handle 6400 MT/s with MCLK and UCLK at 1:1.  Mine can't go above 6000 whatever I try.  Dropping down to MCLK/2=UCLK will get you stable again but it's not worth the performance loss  unless you go all the way up to 7800+MT/s.", "ups": 3, "depth": 2, "author": "DuskOfANewAge", "replies": [{"selftext": "I understand. Maybe i'll try the latest BIOS and see if i can make 6400 work with it, if not i'll just keep 6000, it's not a big deal anyways.\nI am just lazy to update bios atm because i ll have to redo all settings again...", "ups": 1, "depth": 3, "author": "SonOfAnarchy91", "replies": [{"selftext": "agesa 1.0.0.7c is pretty stable for RAM oc so i would suggest that you update the bios you wont regret it", "ups": 1, "depth": 4, "author": "johnspan89", "replies": []}]}]}]}]}, {"selftext": "I guess I won at the silicon lottery. I got the same ram 4x16gb and it runs stable at 6000mhz.\nI run them with a 7950x3d and x670e steel legend from asrock 1000 watt psu and 7900xtx.\n\nEdit: I tweaked timings according to buildzoid video", "ups": 2, "depth": 0, "author": "Help_Effective", "replies": [{"selftext": "&gt; I tweaked timings according to buildzoid video\n\nHey, do you still have this video??", "ups": 1, "depth": 1, "author": "Retalha", "replies": [{"selftext": "https://youtu.be/dlYxmRcdLVw?si=ekc9RSUPdBQ51Wt0", "ups": 1, "depth": 2, "author": "Help_Effective", "replies": []}]}]}, {"selftext": "Yeah, barely an inconvenience.\n\nhttps://www.reddit.com/r/Amd/comments/10qxcr2/7950x_4x16gb_sr_hynix_adie_cinebench/", "ups": 2, "depth": 0, "author": "riesendulli", "replies": [{"selftext": "Tks man", "ups": 1, "depth": 1, "author": "Retalha", "replies": []}]}, {"selftext": "You had terrible trouble with Corsair Vengence 6000 on an MSI Motherboard. I found that going into overclocking and setting the Expo profile worked.", "ups": 1, "depth": 0, "author": "[deleted]", "replies": []}, {"selftext": "You're having the same experience as everyone else on the AM5 MSI boards with no real resolution.  Check the MSI forums and you'll see many similar complaints.\n\nI had the same issue on the X670E Tomahawk wifi with gkill memory.\n\nI tried the steps often listed on their forums with no luck then realized I had not disabled the integrated GPU.  Wierdly, after that it booted with the expo profile.", "ups": 1, "depth": 0, "author": "Viking999", "replies": []}, {"selftext": "You fucked up by getting 4x16", "ups": 1, "depth": 0, "author": "No_Factor2800", "replies": [{"selftext": "No, because one pack I buyed and the other one was a gift from Kingston. This is the reason for the testing.", "ups": 1, "depth": 1, "author": "Retalha", "replies": []}]}]}
{"post": {"title": "Native RSR?", "subreddit": "Amd", "selftext": "I feel like this could be an awesome addition to adrenaline and hyper-rx, we're already getting NAA so why not. It only expands the driver-level features", "ups": 17, "permalink": "/r/Amd/comments/16h5pdf/native_rsr/", "num_comments": 19}, "replies": [{"selftext": "You can set a custom resolution, making it 99% of your screen basically gives you native quality. I do this for Overwatch 2, looks good", "ups": 5, "depth": 0, "author": "TheHybred", "replies": [{"selftext": "people in the comments are saying that there is no use for rsr while on native res, would you agree? Im asking because in my mind it makes sense but people are saying otherwise, and Im not really well read on the difference between Spatial and temporal upscaling.", "ups": 1, "depth": 1, "author": "InZaneTV", "replies": []}, {"selftext": "Yep. Set IN VSR your monitor to whatever resolution, 5K, 8K, doesn\u2019t matter, and then use RSR to run the game at 1/2 resolution.\n\nI like running at slightly above native myself", "ups": 1, "depth": 1, "author": "Buris", "replies": [{"selftext": "This would mean you are not using RSR I tried this just Yesterday doing VSR and then trying to use RSR to do the upscaling to that higher resolution, but RSR won't turn ON unless using less than native resolution and would only upscale up to that native resolution. :(", "ups": 1, "depth": 2, "author": "Concert_Affectionate", "replies": []}]}]}, {"selftext": "native rsr is just ris", "ups": 10, "depth": 0, "author": "Lyajka", "replies": [{"selftext": "No it's not, RSR is based on FSR 1, RIS is based on CAS.\n\nFSR 1 is an evolution of CAS.\n\nhttps://gpuopen.com/fidelityfx-superresolution/", "ups": 15, "depth": 1, "author": "SecreteMoistMucus", "replies": []}]}, {"selftext": "Radeon Image Sharpening is your answer. And it does look fantastic IMO.", "ups": 5, "depth": 0, "author": "Demy1234", "replies": [{"selftext": "Is it? I would assume RSR works differently than image sharpening, but im going to test it tomorrow", "ups": 3, "depth": 1, "author": "InZaneTV", "replies": [{"selftext": "RSR is a static upscale algorithm combined with a CAS sharpening pass. You can't really do a static upscale on an already-native image, so you have just the sharpening pass, which RIS would give you.", "ups": 1, "depth": 2, "author": "Demy1234", "replies": []}]}, {"selftext": "Yes RIS was a real game changer when I discovered it. Everything look way sharper and more detailed withoit being oversharpened. And it costs nearly no performance at all.", "ups": 1, "depth": 1, "author": "Julia8000", "replies": [{"selftext": "Whats a good value for image sharpening? I run it at 30% should i pump it up?", "ups": 1, "depth": 2, "author": "Funkdoobiest64", "replies": [{"selftext": "It should be in a per game basis for best results.", "ups": 2, "depth": 3, "author": "murderouskitteh", "replies": []}, {"selftext": "Depends on your tolerance. A lot of people do 60-80, I personally have it at 40.", "ups": 1, "depth": 3, "author": "popop143", "replies": []}, {"selftext": "Really depends on the game. Some games 50% is right and anything higher looks harsh. Some 30% and some 80% or higher.", "ups": 1, "depth": 3, "author": "AMD718", "replies": []}]}]}]}, {"selftext": "FSR1 doesn't have an antialiasing component, it relies on game's existing antialiasing component to begin with.\n\nWhat are you even talking about? You can't use FSR1 at 100% resolution scaling (native) to antialias the image, because FSR1 doesn't do that at all.", "ups": 2, "depth": 0, "author": "heartbroken_nerd", "replies": []}, {"selftext": "Do you mean being able to use RSR whilst on native resolution? If so, Radeon Image Sharpening exists for that. It uses the contrast adaptive sharpening algorithm (which you\u2019ll see built into some games like MW2 and Starfield as CAS) but on a driver level so it can work for all games. AFAIK you can get some weird artifacts with traditional temporal upscaling which is what RSR/FSR is, so I assume that\u2019s why they opt for CAS/RIS for native resolutions.", "ups": 3, "depth": 0, "author": "Paccypacpac", "replies": []}, {"selftext": "RSR is FSR 1.0 which is a spatial upscaler. Doesn't make sense to use it at 100%, it doesn't do anything (no temporal reconstruction as in FSR 2)", "ups": 1, "depth": 0, "author": "Darkomax", "replies": [{"selftext": "My thought process is, since fsr 1 acts more like a filter (correct me if I'm wrong) would it not be good for replacing TAA for some games?", "ups": 1, "depth": 1, "author": "InZaneTV", "replies": []}]}, {"selftext": "Isn't that a oxymoron op ?", "ups": 1, "depth": 0, "author": "Big_River_8264", "replies": []}]}
{"post": {"title": "Adrenaline Hotkeys are terrible, pls fix!", "subreddit": "Amd", "selftext": "I dont want to have to press ctrl + shift to clip, or to take a screenshot, or bring up metrics. Why is this even a thing? I have to use tools like powertoys to make my insert button press 3 buttons to save a clip! It works fine but it can cause me to execute unwated actions while in game.", "ups": 0, "permalink": "/r/Amd/comments/16icm5o/adrenaline_hotkeys_are_terrible_pls_fix/", "num_comments": 42}, "replies": [{"selftext": "You can customize almost all your hotkeys fyi.\nSettings gear on the top right --&gt; Hotkeys\n\nEdit: Make sure your mouse is within the red box if you were to assign one of your mouse buttons", "ups": 45, "depth": 0, "author": "phasedscum", "replies": []}, {"selftext": "Fix it yourserlf. It is customizable.", "ups": 43, "depth": 0, "author": "Atecep", "replies": []}, {"selftext": "Did you check the app? if you did you'll see you can customise.\n\nAt least do due diligence before spouting  \"Amd terrible blah blah blah\".\n\nUser error", "ups": 39, "depth": 0, "author": "kaisersolo", "replies": [{"selftext": "I love amd, I just want them to stay on top. And minor inconveniences should be fixed.", "ups": -28, "depth": 1, "author": "InZaneTV", "replies": [{"selftext": "Like they already are by allowing you to set whatever the hell you want as the shortcut?", "ups": 11, "depth": 2, "author": "droptheectopicbeat", "replies": [{"selftext": "No, a while ago they changed it so you can't bind single keys, but you need ALT/CTRL too.\n\nInstead of blocking that they should give a warning that it might interfere with other inputs. Nvidia doesn't do that, they let you bind whatever key you want no problem. Instead for Adrenaline you need to fuck around in the Registry if you want to use 1 single key for example take a screenshot.\n\nThis is what OP is talking about but everyone is being a smartass.", "ups": 3, "depth": 3, "author": "Ion_is_OP_REEEEEEE", "replies": [{"selftext": "[removed]", "ups": 1, "depth": 4, "author": "[deleted]", "replies": [{"selftext": "Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*", "ups": 1, "depth": 5, "author": "AutoModerator", "replies": []}]}]}]}, {"selftext": "Nothing to fix. You just need to learn to read.", "ups": 11, "depth": 2, "author": "jwilde8592", "replies": []}]}]}, {"selftext": "you can edit them", "ups": 16, "depth": 0, "author": "Emirimeme", "replies": []}, {"selftext": "Okay, so go change your settings.", "ups": 14, "depth": 0, "author": "EndCritical878", "replies": []}, {"selftext": "You can change it", "ups": 9, "depth": 0, "author": "definatly-not-gAyTF", "replies": []}, {"selftext": "I found this obscure post about it a while back, has useful info: https://community.amd.com/t5/drivers-software/hotkey-assign-limitations-must-be-addressed/m-p/613534", "ups": 5, "depth": 0, "author": "borger_borger_borger", "replies": [{"selftext": "This is exactly what I've been looking for!  nonetheless it should be fixed by amd. Thanks for sharing", "ups": -1, "depth": 1, "author": "InZaneTV", "replies": []}, {"selftext": "Woah. Super useful. I wonder if that overrides on driver update.", "ups": 1, "depth": 1, "author": "bobalazs69", "replies": [{"selftext": "I've done several up- and downgrades. It stays!", "ups": 1, "depth": 2, "author": "borger_borger_borger", "replies": [{"selftext": "Good. And does display driver uninstaller remove it?", "ups": 1, "depth": 3, "author": "bobalazs69", "replies": []}]}]}]}, {"selftext": "If you don't want to use the built in method use your keyboard or mouse software to change a key to the function you want to use.", "ups": 2, "depth": 0, "author": "Alternative_Wait8256", "replies": [{"selftext": "This is what I do, and would also suggest.\n\nHowever, for some reason the trend of minimalist keyboards and mice means people just rarely ever have macro buttons these days. \n\nI really don't understand the people who prefer minimalist KB+mouse, you lose so much functionality.\n\nI personally use a G.Skill KM780 keyboard and a Logitech G600 MMO mouse, and with the function key buttons on both, it gives me 40 available macros on my mouse, and 18 on my keyboard, for a total of 58 macros between both hands.\n\nRelive is one button for me, bound to G12 on my mouse, and Radeon UI is G6 on my keyboard.", "ups": 1, "depth": 1, "author": "Rockstonicko", "replies": []}]}, {"selftext": "Skill issue.", "ups": 2, "depth": 0, "author": "taco_blasted_", "replies": []}, {"selftext": "I love dunking on AMD and their drivers but all these hotkeys are configurable, user error \n\n\nP.S. I haven't used an AMD GPU in ~10 years.", "ups": 1, "depth": 0, "author": "Star_king12", "replies": [{"selftext": "I love dunking on nVidia and their drivers, it goes both ways \ud83d\udc4c", "ups": 0, "depth": 1, "author": "Faceh0le", "replies": []}]}, {"selftext": "For those saying \"user error\" are either stupid or just don't have the same issue as me, which proves my point that amds hotkeys are terrible. Today I learned that instead of using shift + ctrl I could use a combination like alt + insert and then bind insert key to alt + insert in power toys. It helps with game inte reference but this should still not be the way to do it. Also some user suggested to change values in regedit which I will definitely have to try out. So props to all of you that are not assholes and actually want amd to succeed! This community needs to do better.", "ups": 1, "depth": 0, "author": "InZaneTV", "replies": [{"selftext": "yeah they suck ass, also they're always global and not ingame only ? who the fuck wants that", "ups": 0, "depth": 1, "author": "LTyyyy", "replies": [{"selftext": "I own both a nvidia system and amd system, and hotkeys are the only thing I prefer on nvidia.", "ups": 2, "depth": 2, "author": "InZaneTV", "replies": []}]}]}, {"selftext": "Lol change it", "ups": 1, "depth": 0, "author": "Crptnx", "replies": []}, {"selftext": "...", "ups": 1, "depth": 0, "author": "Amstradcpc664", "replies": []}, {"selftext": "First World Problems :P", "ups": 1, "depth": 0, "author": "KythornAlturack", "replies": []}, {"selftext": "I applaud the audacity; when realizing there are keybind settings you can freely change, you call everyone assholes and idiots instead of going quietly back under your bridge.\n\nNice no karma account, by the way. Come here often?", "ups": 1, "depth": 0, "author": "LongFluffyDragon", "replies": []}, {"selftext": "Yall are brutal. I've used amd for a while now and have not been able to change it. Of course I've checked adrenaline, but it does not let me get rid of ctrl + shift in my hotkey. I will admit however that I have not checked if it's possible in 23.9.1. I will go try again and if I'm wrong I revoke my statement.", "ups": -5, "depth": 0, "author": "InZaneTV", "replies": [{"selftext": "Just tried on Andrenalin. You need either Shift + &lt;key&gt;, Alt+&lt;key&gt;, Ctrl &lt;key&gt;, or both Shift + Alt + &lt;key&gt; or Ctrl + Alt + &lt;key&gt;\n\nAnd not a hotkey already in use of another setting", "ups": 5, "depth": 1, "author": "ItsImNotAnonymous", "replies": [{"selftext": "Yes exactly, I only want one key but they are not letting me do that.", "ups": 1, "depth": 2, "author": "InZaneTV", "replies": [{"selftext": "Because single keys can be used for other inputs and they don't want users to have a AMD setting accidentally be toggled.", "ups": 4, "depth": 3, "author": "ItsImNotAnonymous", "replies": [{"selftext": "But say keys like insert, pg down and other inputs that are barely ever used. Why waste functionality? Nvidia does this no problem", "ups": 1, "depth": 4, "author": "InZaneTV", "replies": [{"selftext": "I use keyboard software (Razer Synapse) to help with this issue. My F12 button is assigned as alt + F12. When I hit the F12 key it shows up as alt+F12. I then Assign Alt + F12 in adrenaline as my screenshot key. I hope this helps. This is just an example of a way to get around this annoyance .", "ups": 1, "depth": 5, "author": "Reixmi", "replies": []}, {"selftext": "Program autohotkey to fix your  problem, to make control shift e equal to insert. Is easy", "ups": 1, "depth": 5, "author": "bobalazs69", "replies": []}]}, {"selftext": "I use a single button for clips/screenshots, I\u2019m not understanding the problem y\u2019all are having with Adrenalin hot keys", "ups": 1, "depth": 4, "author": "Faceh0le", "replies": [{"selftext": "It doesn't let you use a single key. It needs to be a combo with CTRL/ALT.\n\nReset it to default and try pressing a single button, nothing happens.\n\nIf you want a single key you have to manually edit the Registry.", "ups": 1, "depth": 5, "author": "Ion_is_OP_REEEEEEE", "replies": [{"selftext": "It let me \ud83e\udd37", "ups": 1, "depth": 6, "author": "Faceh0le", "replies": []}]}]}]}, {"selftext": "I use only one key to make my clips and screenshots with Adrenalin, not sure why you can\u2019t", "ups": 0, "depth": 3, "author": "Faceh0le", "replies": []}]}]}]}, {"selftext": "Agreed. You cant clip or record with only one hotkey", "ups": 1, "depth": 0, "author": "Ridix786", "replies": []}]}
{"post": {"title": "Several measurements fluctuating in BIOS even after locking down the usual", "subreddit": "Amd", "selftext": "I am new to am5, and haven't run an AMD system in three decades. But 7800x3d so...\n\nFinally got it to boot in dual channel with a Corsair kit that was giving me headaches. On an x670e board, I saw no fluctuations in BIOS on optimized defaults or with Expo enabled.\n\nHowever on B650, I do. Specifically the Aorus Elite AX, which is updated to the latest bios.\nI turned off spread spectrum manually set CPU clock control, and the following are still fluctuating:\n\nCPU Frequency, BCLK, and Voltage.\nMemory Frequency. DDR_VDD voltage is occasionally flipping from .250A to .375A and back (not as fast as the other measurements).\n\nHow do I stop, that, and do I really need to?", "ups": 8, "permalink": "/r/Amd/comments/16h7yti/several_measurements_fluctuating_in_bios_even/", "num_comments": 13}, "replies": [{"selftext": "All those measurements will fluctuate normally, whether under load in an idle state.  Even if I put 100% load on my RAM and IMC for example the RAM VDD will fluctuate.  As long as it is stable you should be satisfied.", "ups": 5, "depth": 0, "author": "DuskOfANewAge", "replies": []}, {"selftext": "Those fluctuations are completely irrelevant. I don\u2019t know how you come to think they are.", "ups": 3, "depth": 0, "author": "gonirad", "replies": []}, {"selftext": "Complete noob here, but why are you even looking at those measurements in the first place? And you swapped boards just because? Or am i missing something. \nAsus x670e-e here, bought, up and running since last november, with zero issues, tho, i wasnt looking at voltage, being amperage.  Not sure what the question or the issue is here. But good luck with what you want or need.", "ups": 2, "depth": 0, "author": "Koyote7676", "replies": []}, {"selftext": "To start, voltage cant be amps. That is not how anything works. Those numbers are absolute nonsense and kind of concerning.\n\n&gt; How do I stop, that, and do I really need to?\n\nGet a motherboard not made by gigabyte (or asus), most likely.\n\nYou should not be trying to manually set the clockspeed of any modern CPU, though, that will cause all sorts of problems.\n\nI would suggest checking the voltage in windows using hwinfo64, meanwhile.", "ups": 1, "depth": 0, "author": "LongFluffyDragon", "replies": [{"selftext": "Just tired, I know the difference but on that vdd line there were three entries on the row the first two remain static and the third one was flopping between those numbers in amps\ud83e\udd37\u200d\u2642\ufe0f", "ups": 1, "depth": 1, "author": "Bagslapadin", "replies": [{"selftext": "Those are extremely low values, and amperage is usually not what you need to care about.\n\nMaybe start by explaining what sort of problems you are having to begin with. It is unclear what you are trying to solve.", "ups": 2, "depth": 2, "author": "LongFluffyDragon", "replies": [{"selftext": "Notice the ram speed fluctuating from 6000 stepping up to 6048.xxx (with corresponding or because of fluctuations in BCLK).\n\nOn Intel systems that was usually because of spread spread spectrum being on or set to auto, turning that off and sometimes manually setting BCLK, stop fluctuation.\n\nTalking about fluctuation at Auto and bios not normal stepping under load.", "ups": 1, "depth": 3, "author": "Bagslapadin", "replies": [{"selftext": "Not an issue, typically. Just clear CMOS and dont mess with stuff you dont fully understand unless you have actual performance or stability problems, otherwise you will just create new problems.", "ups": 6, "depth": 4, "author": "LongFluffyDragon", "replies": [{"selftext": "I understand what EXPO is, BCLK, and Spread Spectrum. That is why, when I changed the relevant CPU value (Auto -&gt; 100.0) and Spread Spectrum Control (Auto -&gt; Disabled) and there was zero difference in the fluctuations, I wanted to investigate.\n\nWhat is really happening is the BCLK value is fluctuating, so the CPU speed and RAM speed will fluctuate of course along with it. To me, that is instability. Maybe it is of no consequence on this system.\n\nIt is absolutely not true, that those values will fluctuate always at idle, on every system.", "ups": -1, "depth": 5, "author": "Bagslapadin", "replies": [{"selftext": "That has been normal for a good while now and not generally an issue or cause of instability. The opposite, actually. spread spectrum is increasingly needed as processor speeds increase over time.", "ups": 2, "depth": 6, "author": "LongFluffyDragon", "replies": [{"selftext": "That makes sense, I hadn't thought of it that way.\nI have a lot of reading to do. I appreciate that insight.", "ups": 1, "depth": 7, "author": "Bagslapadin", "replies": []}]}]}]}]}]}]}, {"selftext": "&gt;To start, voltage cant be amps.\n\nYou really should take the time to study Ohms Law. You cannot have one without the other in any given electrical circuit.  Yes, the OP appears to be confusing the two in post possibly, but in any electrical circuit with a voltage applied there will be a current flow that is measured in Amperes. Voltage should remain constant, but amp load can change with the load applied to the circuit. All modern CPU do this as a normal function.", "ups": 1, "depth": 1, "author": "rod6700", "replies": [{"selftext": "Cant be reddit without useless driveby pedantry.", "ups": 0, "depth": 2, "author": "LongFluffyDragon", "replies": []}]}]}]}
{"post": {"title": "Make colors better for recording", "subreddit": "Amd", "selftext": "I just got the new 7800xt 2 days ago &amp; im loving it so far , however in games it does look dull / washed out , not really any color , but using amd software i can add color so it\u2019s perfect now but was curious how can i record clips with the color filters on ? Cuz when i record or stream it\u2019s just the dull gameplay", "ups": 9, "permalink": "/r/Amd/comments/16gzqt0/make_colors_better_for_recording/", "num_comments": 35}, "replies": [{"selftext": "1: Make sure you do not enable 10bit pixel format in the gaming tab.... WAY WAY too many people are enabling this and not realizing what it's intended for... \n\n2: If your colours are washed out, then either you're not running the proper colour space for the display, or your display is improperly set/configured. The output from the amd gpus are spot on for colour accuracy out of the box. Honestly there are far too many people utilizing filters that they really should be tuning their monitors/displays for instead. Though considering the nightmare i've seen some users thinking grossly oversaturated to the point of obvious bleeding and banding occuring, may as well be playing with 256 colour or maybe less, hell i've seen some old gifs that look better than their preferred visual settings implemented, so could be just really terrible eyes?", "ups": 17, "depth": 0, "author": "DHJudas", "replies": [{"selftext": "So is there something i should enable or disable? I\u2019ve only had Nvida gpus", "ups": 1, "depth": 1, "author": "ResistConsistent4552", "replies": [{"selftext": "nvidia had digital vibrance as a feature to combat nvidia's history of lacking in in colour saturation, often overshooting. ATI aka AMD doesn't suffer from this and honestly you shouldn't need to mess around with anything unless your display is improperly configured/calibrated. Many monitors come with their colour preset set to cold instead of normal/standard. Some may prefer warm even. set everything to default in windows/display driver, and then adjust your monitor first.", "ups": 9, "depth": 2, "author": "DHJudas", "replies": [{"selftext": "At default driver settings Radeon and GeForce produce exactly the same colors. There were differences like 15 years ago", "ups": -1, "depth": 3, "author": "dmaare", "replies": [{"selftext": "still getting situations where nvidia's still default to limited 16-255 output by default...", "ups": 7, "depth": 4, "author": "DHJudas", "replies": []}, {"selftext": "There is some difference in the color settings because the image was much more vivid for me with 6800XT than 3090 in Cyberpunk on C2. Not the RGB limited issue.", "ups": 1, "depth": 4, "author": "bctoy", "replies": [{"selftext": "Then amd driver set color saturation higher for some reason", "ups": 0, "depth": 5, "author": "dmaare", "replies": [{"selftext": "Digital vibrance of 55% in nvcp seems to match AMD's default color profile. But it clips, not sure if that happens with AMD.\n\nhttps://www.reddit.com/r/nvidia/comments/58eqay/anyone_use_digital_vibrance_in_nvidia_control/\n\nSearching for it with AMD brings up u/DHJudas posts from few years back,\n\nhttps://www.reddit.com/r/Amd/comments/hjagq9/is_there_a_conversion_from_nvidia_digital/fwmdvid/", "ups": 2, "depth": 6, "author": "bctoy", "replies": []}]}]}]}]}, {"selftext": "If you're on Windows 11, press Win + Alt + B. This will disable HDR and should revert to an 8-bit color space.\n\nCapturing HDR rendered applications is still a little weird with HDR on PC kind of being in it's infancy, captures look washed out across the board.", "ups": 4, "depth": 2, "author": "derangedhaze", "replies": [{"selftext": "XBox Game Bar does have good screenshots for HDR. It saves two formats, and one shows the HDR screenshot. Command is Win + Alt + PrtScrn", "ups": 2, "depth": 3, "author": "popop143", "replies": [{"selftext": "Hey, great info! Makes sense MS's baked-in tools would be quicker to correct this than third parties.", "ups": 1, "depth": 4, "author": "derangedhaze", "replies": []}]}]}]}, {"selftext": "1: Make sure you do not enable 10bit pixel format in the gaming tab.... WAY WAY too many people are enabling this and not realizing what it's intended for...\n\n&amp;#x200B;\n\nDoes this video looks bad or anything else ? \n\n[https://www.youtube.com/watch?v=pPuIh2nS6oU](https://www.youtube.com/watch?v=pPuIh2nS6oU)\n\n&amp;#x200B;\n\n10bpc full rgb.", "ups": 0, "depth": 1, "author": "Molda_Fr", "replies": [{"selftext": "i have no point of referrence. some of it looks flat to me.. but that could be the game.\n\nrunning 10bpc via the display tab is good, enabling 10 bit pixel format in the gaming tab is bad.", "ups": 1, "depth": 2, "author": "DHJudas", "replies": []}]}, {"selftext": "Just to clarify you're talking about the \"10-bit pixel format\" under the Graphics tab right?", "ups": 1, "depth": 1, "author": "Fluff42", "replies": [{"selftext": "Yeah, OP was referring to that one; that Graphics option is mostly for apps that support 10-bit color in OpenGL; this setting breaks HDR now too. It shouldn't be enabled.\n\n10-bit colors via display panel (under \"Gaming -&gt; Display\" is fine for ~1B colors, mostly to prevent color banding with gradients.", "ups": 3, "depth": 2, "author": "JasonMZW20", "replies": []}, {"selftext": "Yes,but 10 bit color doesn't make the colors look any more vibrant. It's mainly for vibrance. And Nvidia drivers automatically have 8 bit plus frc enabled on all \"10 bit\" monitors", "ups": 1, "depth": 2, "author": "SaintPau78", "replies": [{"selftext": "There are two separate settings, if you have a 10-bit capable monitor you should enable 10-bit Color Depth in the Display section but not 10-bit pixel format under the Graphics tab.", "ups": 2, "depth": 3, "author": "Fluff42", "replies": [{"selftext": "Tf are you talking about", "ups": 0, "depth": 4, "author": "SaintPau78", "replies": []}]}]}]}, {"selftext": "&gt; Make sure you do not enable 10bit pixel format in the gaming tab.... WAY WAY too many people are enabling this and not realizing what it's intended for...\n\nWhy not, what does it do/what is it intended for?\n\nHow do I ensure that the display is actually using 10bit? I thought that was the toggle.", "ups": 1, "depth": 1, "author": "heartbroken_nerd", "replies": [{"selftext": "under the gaming graphics tab... absolutely do not, it's intended for capability mode for specific programs. Enabling it will break a ton of things.\n\nAnything in the display tab is fine, just do not enable that specific feature in the gaming/graphics tab", "ups": 1, "depth": 2, "author": "DHJudas", "replies": []}]}]}, {"selftext": "Dull and washed out? Enable RGB 4:4:4 Pixel Format (PC Standard) in the Display Settings of the Driver Control Panel...", "ups": 3, "depth": 0, "author": "Dunkle_Geburt", "replies": [{"selftext": "In the amd software?", "ups": 1, "depth": 1, "author": "ResistConsistent4552", "replies": [{"selftext": "Yes. Press \"Alt\" + \"R\", open the Gaming-&gt;Display Tab, set \"Pixel Format\" to RGB 4:4:4 PC Standard. That's it.", "ups": 3, "depth": 2, "author": "Dunkle_Geburt", "replies": [{"selftext": "A *lot* of displays only support half-chroma 4:2:2, like mine.\n\nI'm not able to confirm if changing to full-chroma fixes HDR capturing, but if it does not you may need to disable HDR entirely, or edit your captures to boost saturation after the fact.", "ups": 1, "depth": 3, "author": "derangedhaze", "replies": []}]}]}]}, {"selftext": "Are you playing games with hdr?", "ups": 3, "depth": 0, "author": "wingback18", "replies": []}, {"selftext": "Sounds like HDR is turned on by default but your monitor doesn't deliver.\n\nDisable HDR in windows and on your monitor, that should fix it.", "ups": 2, "depth": 0, "author": "PotentialAstronaut39", "replies": []}, {"selftext": "Press Win + Alt + B to turn off HDR. I\u2019ve had problems with recording software in the past struggling to capture HDR without over-saturation.", "ups": 2, "depth": 0, "author": "TheContingencyMan", "replies": [{"selftext": "Is this for windows 10? Cuz i did that and Xbox game bar opened", "ups": 1, "depth": 1, "author": "ResistConsistent4552", "replies": [{"selftext": "Yeah, that\u2019s the Windows 10 shortcut. Might be different for Windows 11.", "ups": 1, "depth": 2, "author": "TheContingencyMan", "replies": []}]}]}, {"selftext": "Swap out cable?", "ups": -2, "depth": 0, "author": "kaisersolo", "replies": [{"selftext": "U mean display cable?", "ups": 1, "depth": 1, "author": "ResistConsistent4552", "replies": []}, {"selftext": "Also, restart his router, right?", "ups": 1, "depth": 1, "author": "octopec", "replies": [{"selftext": "No need to be sarky.\n\nCables can lead to such problems hence the hdmi check functionality built into Radeon settings.", "ups": 1, "depth": 2, "author": "kaisersolo", "replies": []}]}]}, {"selftext": "Start by fixing the base issue, which is likely incorrect color format being used? In most cases it should be using 8-bit RGB 444.\n\nIf that still looks wrong, try to compare it against an OLED phone or similar that has a brightly colored image.", "ups": 1, "depth": 0, "author": "LongFluffyDragon", "replies": []}, {"selftext": "Don't record hdr content (just go in your monitor/pc settings and turn hdr off), only xbox gamebar can record with hdr on without the washed out colors but its still weird", "ups": 1, "depth": 0, "author": "TruzzleBruh", "replies": []}]}
{"post": {"title": "Overclocking R7 2700 and RX580", "subreddit": "Amd", "selftext": "I'm saving for a better CPU and GPU  so in the meantime I thought it'd try OC'ing a Ryzen 7 2700 and RX 580 but this isn't something I've ever done before so would like some advice as to how to go about it to squeeze as much juice out of them before I replace them in a few months.\n\nAFAIK it seems to all be done from the BIOS. Is that so or am I missing something else?", "ups": 10, "permalink": "/r/Amd/comments/16grb9j/overclocking_r7_2700_and_rx580/", "num_comments": 42}, "replies": [{"selftext": "Not worth it. Let's say you won the \"silicon lottery\" and your parts OC for a total of 15% more performance (combined). \n\n Let's put that in actual game numbers: your BG3 game used to run at 30 fps, now it runs at 34.5 FPS. Not really life changing for something that makes more noise, generates more heat and increases your power consumption. \n\nJust keep them as it is until you can buy new parts.", "ups": 18, "depth": 0, "author": "bestanonever", "replies": [{"selftext": "Thank you. I'll do that!", "ups": 5, "depth": 1, "author": "OutpostThirty1", "replies": []}]}, {"selftext": "Don't bother with both...\n\nThe 2700x is already \"the best bin\" there is out there. What you can do to keep longer boost states or conditions - is to cool the chip enough. Keep it below 60 degree mark and it will boost as long as possible. You can also try to increase BCLK - it will give you a higher boost clock (i.e 4480Mhz) but other then that - the 2700X is already clocked to it's highest limit.\n\nAs for the RX580 - it's the same story. The chip does not scale well anymore after 1200Mhz. It's due to the limiting memory bandwidth the card has. Ive had both chips and really the best i could get out of a RX580 was 1660Mhz using watercooling. But the performance uplift was barely a few percent.\n\nI replaced the RX580 with a RX6700XT - that blasts the whole RX580 out of the water while being far more efficient. Basicly 2.5x faster at the same power enveloppe. If you want to replace the CPU - pick a 5800X or 5800X3D. The whole 5800x series is pretty much 50% faster then a 2700x.", "ups": 4, "depth": 0, "author": "Jism_nl", "replies": [{"selftext": "That's true if he has the 2700x, but just in case he actually has the 2700(non-x) that CPU can give an extra 15% or so, the non-x 2700 has a single thread clock limit of 4.1ghz, but all core limit of 3.3, so overclocking it isn't completely pointless, it's free after all.\n\nStill might be time to upgrade though, I see great deals on the 5600 and 5700 CPUs regularly, and they will work in the same boards.\n\nAgree with the GPU though, there's no OC headroom on those polaris cards to speak of.", "ups": 6, "depth": 1, "author": "TheNoseHero", "replies": [{"selftext": "The best you can do would be 4.1 to 4.2Ghz all core. Anything above is like 1% of best chips out there. You cannot feed more then 1.34v 24/7 onto these chips - degradation is real and ive seen many users fry their chips because the majority of review websites ramp in a 1.4V voltage and start OC'ing.\n\nAnd this degradation can happen in even weeks - i'm not joking. Max clocks cannot be held, or even at stock the thing chrashes. A manual downclock is all that helps. It's irreversible damage done to the CPU.\n\nAt best you can go for BCLK - that would ramp up to a maximum of 105Mhz (4300Mhz) but it depends on how good the rest of your system is. Even 101Mhz gives a free 41Mhz boost without even having todo anything for it.\n\n2700x's can run 3200Mhz or even 3466Mhz - There's chips out there doing 3600Mhz which in games can be beneficial. So start with Memory OC'ing first. My 2700X would do 3466Mhz with no issues. Ive lured 4.5Ghz boost clocks out of it and sustained 4.25Ghz on all core with simply PBO and keeping the chip below 60 degrees. Above 60 degrees it starts to downclock. It's really simple.", "ups": 2, "depth": 2, "author": "Jism_nl", "replies": [{"selftext": "Perhaps,\n\n[https://www.reddit.com/r/PUBATTLEGROUNDS/comments/9gslxf/pubg\\_optimization\\_guide\\_rx580/](https://www.reddit.com/r/PUBATTLEGROUNDS/comments/9gslxf/pubg_optimization_guide_rx580/)\n\nThis might be of help. The RX580 is just a 1080p card with some extend doing 1440p, but likely with settings turned down. It is a OK card for it's time but there's simply better stuff out there by now. If you want best FPS &gt; 1080 resolution and fine tune per game. OC'ing will yield only a few percent,\n\nYou need to flash it's bios with a unlocked, 250W bios at first. Additionally you need to apply the Ubermix 3.1 Memory Timings into the bios, and flash it. From there on you OC the memory which will be approx 2100Mhz because of the tighter timings. The GPU itself can be undervolted, or even OC'ed, and ive experience 1660Mhz myself on water. But the heat output is huge at those levels. Just by increasing the power limit on the card you can gain some extra performance in heavy scenes. Above does yield you some extra performance, but OC'ing these days is minimum.\n\nIve hit 350W on a RX580 lol.. That was insane and all over one 8 pin power cable.", "ups": 2, "depth": 3, "author": "Jism_nl", "replies": []}, {"selftext": "Oh yeah, I wouldn't go for absolute peak clocks, but I have seen a few 2700(non-x) users claim they managed 3.8-3.9 on 1.1v, that's still 10% higher multi thread than stock, but apparently also runs colder than stock, something like that seems like it's probably a better plan.  \nNot sure how easy that would be to achieve though.\n\nFor the 2700x, just PBO auto usually does better than manual OC in most cases, and \"should\" not degrade the chip.", "ups": 1, "depth": 3, "author": "TheNoseHero", "replies": [{"selftext": "PBO does not degrade the chip. It won't be going all core at 4.3Ghz. It's more like 4.1 to 4.2GHz if kept below 60 degree.\n\nIt's near impossible to get a 2700(x) at 4.3Ghz all core on air or water. You'll be running against a wall and it does'nt matter which voltage you cram into it. And even if you get it it's likely with a higher voltage then what's recommended 24/7 (1.34v) without hurting it on the long term.\n\n2700x's like as much as high possible memory bandwidth. They are rated for 2900Mhz but some models do 3466Mhz and beyond. Games seem to scale very well with higher memory clocks. The IF seems to max out at around 1800Mhz and it's locked.\n\nI've played around with it - at some point reached over 200W of power consumption on the 2700x. The performance gains of going over stock is'nt there. These chips are already binned to the maximum. If you get a 5800x it's already 50% faster in everything.", "ups": 1, "depth": 4, "author": "Jism_nl", "replies": [{"selftext": "Yeah, I used to have a 2700x, I turned on PBO, it gave me 4.35 single thread and 4.2 all core, and I couldn't hit that stable manually, overclocking it was just pointless.\n\nI do however have a 2700 non-x in my media center, and overclocking that chip is tempting, sadly the ITX case does not have space for good coolers, but that chip drops to 3.3ghz when hit with a multi threaded load, and it can't PBO, so if I was running that in a desktop I would probably overclock it.\n\nUnlike it's big brother, the 2700 non-x is a 65w chip, which in some situations holds it back somewhat.", "ups": 1, "depth": 5, "author": "TheNoseHero", "replies": []}]}]}]}]}, {"selftext": "RX580 wont do 1660MHz, the best RX580s and RX480s will top at around 1.5GHz (core clock)\n\nRX590 is a different chip (12nm node and not 14nm) and it will do 1.65 - 1.7GHz", "ups": 1, "depth": 1, "author": "maze100X", "replies": [{"selftext": "What?\n\nI had a RX580 with a modified bios and watercooled - the max clocks i was able to obtain at around 1.23v was 1660Mhz, and furmark stable that was yes.\n\nIt burned only 350W at the proces, but i managed todo it. However, the gain from going from 1380Mhz to 1660Mhz was only in the percentages - the memory bandwidth is lacking and holding back the GPU.\n\nI'd say this card passes it's best potential beyond 1200MHz. It just does'nt scale anymore beyond those clocks.\n\nYou really need a modded bios (not the stock) which increases the allowed power limit. It's not a good thing when you do run at 350W - the 8 Pin header is getting taxed and easily pulls 26 amps from the 12V line.\n\nGood quality cables should have no issues with 26 Amps. A good yellow gauge is capable of doing 12A without burning. So 3 wires makes 36amps. It's far beyond PCI-E spec but who cares - there's other older Radeon X2 cards out there pulling 600W or even more from just 2 8 pin cables.\n\nThe only thing i noticed is that, degradation on swapping PCI-E power cables is \"real\" - i noticed to start having lower 12V input voltages (at load 11.2V) which was corrosion coming from those connectors. Once i swapped it out a clear 12.1V was back again.", "ups": 1, "depth": 2, "author": "Jism_nl", "replies": [{"selftext": "im not sure how you \"measured\" 1660MHz\n\nbut a 580 wont do 1660MHz, either you had incorrect sensor reading or a RX590\n\nit has nothing to do with power, the silicon itself cant do it (V/F curve)", "ups": 1, "depth": 3, "author": "maze100X", "replies": [{"selftext": "Sigh.\n\n[https://www.reddit.com/r/overclocking/comments/be1fvj/rx\\_580\\_1600\\_mhz\\_superposition\\_1080p\\_extreme\\_score/](https://www.reddit.com/r/overclocking/comments/be1fvj/rx_580_1600_mhz_superposition_1080p_extreme_score/)\n\nThere's so much people exceeding the 1600Mhz clocks on those cards. 1660Mhz on 1.23v is'nt unreal. Just cool the chips properly.", "ups": 1, "depth": 4, "author": "Jism_nl", "replies": []}]}]}]}]}, {"selftext": "I\u2019ll be honest I doubt you\u2019ll get that much extra juice out of overclocking those components. But I\u2019m just someone that prefers to keep it at stock.\n\nI\u2019ve owned both that CPU and GPU. They just are very underwhelming. \n\nIf you\u2019re on a budget just save up for the 5800x3D and then get an AMD GPU in your budget. It will save you a lot more money in the long run.", "ups": 3, "depth": 0, "author": "CarLearner", "replies": [{"selftext": "Ah maybe worth keeping at stock then \ud83e\udd37\u200d\u2642\ufe0f I'm desperate to play BG3 \ud83d\ude05 ideally I'd like to buy CPU and GPU together to negate any bottlenecking I know I'll get if I replace GPU or CPU first", "ups": 2, "depth": 1, "author": "OutpostThirty1", "replies": [{"selftext": "Replace GPU first cause that GPU is old. Then the 2700 will bottleneck you cause that CPU bottlenecked hard. Even if you\u2019re on a strict budget get the 5600 and you\u2019ll still see a good performance gain because Ryzen got good at 3000 and 5000 series releases.", "ups": 1, "depth": 2, "author": "CarLearner", "replies": [{"selftext": "Thought about getting a Ryzen 5600 but I'm sure I read AMD is only gonna support 5700 onwards with FSR3.0 \ud83e\udd14\n\nI've checked [PC Bottleneck](https://pc-builds.com/bottleneck-calculator/result/10p18t/3/graphic-card-intense-tasks/1920x1080/) and with a 5800X and RX6700XT I get 0% bottleneck and seems I can run what I want @ 60fps+ 1080p, perhaps except Starfield but I'm sure it'll get more and more optimised as time goes on \ud83e\udd37\u200d\u2642\ufe0f", "ups": 0, "depth": 3, "author": "OutpostThirty1", "replies": [{"selftext": "Apples and oranges,\n\n5700 as in the GPU\n\n5600 as in the GPU", "ups": 1, "depth": 4, "author": "exterminuss", "replies": []}, {"selftext": "If you're going to upgrade CPU on AM4, only go for 5800x3d otherwise you're gonna regret getting anything else. As for gpu, get whatever your budget allows I guess. \n\nFsr3 won't even be a thing late into 2024 given devs will need to update their games to support it.", "ups": 1, "depth": 4, "author": "Medical-Tomorrow7727", "replies": []}, {"selftext": "FSR 3.0 is for AMD GPUs.. you\u2019re confusing it with the 5700 XT and that line of AMDs GPUs..\n\nNot their Ryzen CPUs.. a Ryzen 5800x vs a Ryzen 5600 is not really that much of a difference beside core count. \n\nIf you\u2019re considering the 5800x just pay more for the 5800x3D.\n\nI\u2019m only recommending the 5600 because as Black Friday approaches it should go back on sale to $115-130 again USD and I don\u2019t know how much you\u2019re willing to spend. \n\n5800x3D is the endgame for AM4 which you should save for. Then just get a 6700 XT or see if the 7700 XT drops in price but doubtful. \n\nAlso I wouldn\u2019t trust PC Bottleneck websites but that\u2019s just me.", "ups": 1, "depth": 4, "author": "CarLearner", "replies": []}]}]}, {"selftext": "I\u2019d prioritize getting the CPU if you\u2019re staying on the same motherboard since it\u2019s completely possible that they\u2019ll stop producing Ryzen 5000 series CPUs for it sometime in the near future. I feel like you\u2019d notice a CPU bottleneck more since you\u2019ll have high frames that stutter and dip low very frequently", "ups": 1, "depth": 2, "author": "UnsureAssurance", "replies": []}]}]}, {"selftext": "The extra heat produced and increased power consumption is not worth the minor performance increase.", "ups": 1, "depth": 0, "author": "iAmGats", "replies": [{"selftext": "Thank you.", "ups": 1, "depth": 1, "author": "OutpostThirty1", "replies": []}]}, {"selftext": "Depends on where your goal is at \n\nOVERCLOCKING these parts might be worth it.\n\nDon't expect any huge improvements,\n\nAt least check Ram speeds\n\nTry if the GPU might run the same/better with lower voltages", "ups": 1, "depth": 0, "author": "exterminuss", "replies": []}, {"selftext": "Buy 6650 XT, it's cheap,\n2700 is still alright.\n\nShould be enough for Starfield 1080p ~60fps.\n\nOverclocking 580 isn't going to do you anything.", "ups": 1, "depth": 0, "author": "SkorbiF1", "replies": [{"selftext": "r7 2700 wont run starfield at 60fps, maybe 30", "ups": 1, "depth": 1, "author": "petron007", "replies": [{"selftext": "I have 2700x, 6650 XT, I play Starfield at 60fps.\nCPU usage % is below GPU usage %.\n\nHigh settings, 1080p, fsr on.\n\nA bit lower fps on that biggest city.", "ups": 1, "depth": 2, "author": "SkorbiF1", "replies": [{"selftext": "1080p with fsr on, how many pixels are we looking at?", "ups": 1, "depth": 3, "author": "petron007", "replies": [{"selftext": "I'm currently playing 1440p. getting somewhere around 40-60 fps.  \n1080p should be 60.  \nThat's still not the point. Point is, 2700x is not bottlenecking my system as GPU usage is still higher than CPU. I was about to upgrade this CPU to something newer, but I can't see the benefits. Might get couple more fps, but that's it.", "ups": 1, "depth": 4, "author": "SkorbiF1", "replies": [{"selftext": "I am surprised that you are seeing 60fps with 2700, friends of mine are really struggling with 2nd gen ryzen on starfield.\n\n30-40fps max at 1080p medium, with similar rigs.", "ups": 1, "depth": 5, "author": "petron007", "replies": [{"selftext": "I do have a new SSD drive and 32gb of fast RAM. Maybe they don't have XMP enabled, and their RAM is limited to 2400mhz, which is choking the system?  \n\n\nUpdate drivers, update bios, xmp on, game boost on, sam on.  \nshould help.", "ups": 1, "depth": 6, "author": "SkorbiF1", "replies": [{"selftext": "could be that they are just spending a lot of time in cities or something, that will probably tax cpu a lot more.", "ups": 1, "depth": 7, "author": "petron007", "replies": [{"selftext": "Yea, that too.", "ups": 1, "depth": 8, "author": "SkorbiF1", "replies": []}]}]}]}]}]}]}]}]}, {"selftext": "I\u2019d just enjoy the ride until you get a 5800X3D or something newer even, the frames you would gain are negligible compared to the effort to get them.", "ups": 1, "depth": 0, "author": "UnsureAssurance", "replies": []}, {"selftext": "You're not going to get much out of the 580 because that was already a 480 kind of push to its limits", "ups": 1, "depth": 0, "author": "jrhowrey", "replies": []}, {"selftext": "Just save for a 5600x or a 5800x3D, update.the bios of your motherboard , drop either in and add a better GPU.", "ups": 1, "depth": 0, "author": "John_Mat8882", "replies": []}, {"selftext": "The 580 isn't going to get much more wrung out of it realistically, and the same can be said for the 2700, only thing you could do there is improve cooling to get better boost, or longer boost actually. Either way, save your shmeckels for the upgrade, live with what you have and it will make the upgrade that much sweeter.", "ups": 1, "depth": 0, "author": "freeroamer696", "replies": []}, {"selftext": "The old days of getting meaningful gains by overclocking are gone. Good old days Operon 165 from 1.8ghz to 2.7 on good air or Celeron 300a from 300mhz to 450mhz. I could go on, but will say that tuning modern hardware nets small improvements.", "ups": 1, "depth": 0, "author": "zPacKRat", "replies": [{"selftext": "It's still a big difference in between smooth and smoother. It works on lower end systems still - not so for higher end. 2700(x) is still solid - just won't cut it against a 5800X3D for example.", "ups": 1, "depth": 1, "author": "Jism_nl", "replies": []}]}, {"selftext": "You overclock the GPU in AMD Software. You should be able to find plenty of guides on YouTube about how to overclock it. Ancient Gameplays is a good YouTuber and might have a video on it.", "ups": 1, "depth": 0, "author": "Demy1234", "replies": []}, {"selftext": "CPU overclocking is done via the BIOS or via software that communicates with it, basically.\n\nGPUs have their own onboard BIOS you cant access config for directly, so you need to use software (part of the AMD adrenaline software that includes the driver) to configure them.\n\nBut unless it is just for fun, there is not much point in overclocking newer hardware. Tends to be very tiny gains for a lot of extra heat.", "ups": 1, "depth": 0, "author": "LongFluffyDragon", "replies": []}, {"selftext": "You could OC the Ryzen 7 2700 to achieve an all-core boost of 3.8-4GHz, overclock the memory if it's below DDR4-3000, and as for the RX 580 you could try 1400MHz on the core.\n\nNeither is worth it if the system becomes unstable/hot/noisy. Not much to be gained.", "ups": 1, "depth": 0, "author": "LeiteCreme", "replies": []}]}
{"post": {"title": "6950XT GPU re-paste with PTM7950 -- went from 110C HotSpot to 83C", "subreddit": "Amd", "selftext": "The temp delta on my 6950XT (AsRock OC Formula) had been getting higher and higher lately and I knew I had to do something... was seeing it hit 110 within 30 seconds of a Speedway benchmark test drawing 350Watts.  Ordered some PTM7950 and put it on yesterday.. booted up the machine and ran the same exact test and now the hotspot only hits 83C at the same 350Watts!!  Ran the test a few times to verify and sure enough this stuff is that good.  \n\nIt was NOT the easiest stuff to work with but I couldn't be happier with the result! ", "ups": 69, "permalink": "/r/Amd/comments/16fzowo/6950xt_gpu_repaste_with_ptm7950_went_from_110c/", "num_comments": 60}, "replies": [{"selftext": "Every repaste without screenshots of the test results claims enormous gains and every repaste with full documentation and images shows very little gain.  Hmmmmmm", "ups": 104, "depth": 0, "author": "HeywoodJaBlessMe", "replies": [{"selftext": "The post could be truthful.  \nIn case when you change ***dried up*** thermal paste to ***any*** ***new*** paste, you would get such kind of results.  \nIn case when you change ***new*** thermal paste to ***another new*** thermal paste, you would get something like [this](https://youtu.be/4QWS2cLQofM?feature=shared&amp;t=644). The results are not so impressive, though.\n\nAnother thing is that just by configuring the fan curve properly, you may also have good results... but who cares :)", "ups": 24, "depth": 1, "author": "uw_cma", "replies": [{"selftext": "He said his hotspot has crept up which screams pump out to me , PTM7950 does not pump out.", "ups": 5, "depth": 2, "author": "Parking_Automatic", "replies": []}]}, {"selftext": "Everyone would just cringe if they saw how much MX-4 paste i used and claim its user error, i agree tho, clearly should've gone straight for PTM7950 :D\n\nedit: 7900 XTX is a way higher wattage chip then rdna2 so will pumpout easily in 1-2 weeks with MX-4 compared to cpu at lower wattage so yes if you do not want day 1 results to get worse you quite litterally have to repaste once every 1-2 weeks which is not viable which is why i went PTM7950 which has no pump out even Honeywell is explaining there no issues with pumpout [https://thermalmanagement.honeywell.com/us/en/products/thermal-interface-materials/phase-change-materials/ptm7000-series](https://thermalmanagement.honeywell.com/us/en/products/thermal-interface-materials/phase-change-materials/ptm7000-series)\n\nArtic MX-4 tho is meant to pumpout so it easily covers the entire die but the issue is this effect contineus on chips that are not perfectly flat or cooling surfaces that are not flat or uneven which is common with an ekwb waterblock cos their quality control has lower standards, so on the liquid devil 7900 XTX which has a waterblock from ekwb + AMD chips are not perfectly flat sometimes this heavily contributes to even more pumpout over 1-2 weeks with MX-4 in my case\n\nor 1-3 months with the stock paste that powercolor used they already admit to having issues with red devil cards, its not limited to only the red devil cards, its probably not even something you can blame powercolor for, its pure bad communication from AMD or lack of communication, something AMD could've prevented by recommending PTM7950 on their highend chips that suffer from pumpout or have uneven die and cooling surface, since their chips are quite high wattage even more so then their cpu chips\n\nSo for context you may be fine with MX-4 on lower wattage chip but you likely run into issues on a 7900 XTX, besides you should not be going for older paste anyway get a newer one preferably something that does not spread easily cos its less likely to pumpout after heat and cooling cycles and last quite a bit longer, or just go for PTM7950 cos it works.", "ups": 3, "depth": 1, "author": "Melodias3", "replies": [{"selftext": "MX-4 is [also good](https://www.youtube.com/watch?v=4QWS2cLQofM&amp;t=644s). But it should be properly used, in case there is too much of it, cooling would be worse.\n\nPTM7950 is, probably, easier to use, since there is a need to put it in a fridge (for 30 mins) and cut afterwards. And you would have proper amount.", "ups": -2, "depth": 2, "author": "uw_cma", "replies": [{"selftext": "MX-4 is only good at day 1 for me it degraded from 75c hotspot to 92c hotspot after 1-2 weeks 2 times it had literally pumped out to a point there where visible areas with no paste, anyway you should never use an old paste anyway, there better paste out there these days that probably last quite a bit longer these days, Artic tho is not part of those pastes only heard bad things about pumpout about it, not much you can do about it but get something that does not pump out, which is either hard to use or hard to get.", "ups": 5, "depth": 3, "author": "Melodias3", "replies": [{"selftext": "Yeah, that could happen.  \nHowever, I have MX-4 for Ryzen 5 5600, everything is fine for almost 11 months, but the temps are lower there.  \nI assume your graphic card's fans work with the default settings. That could also be the reason of that degradation.", "ups": 1, "depth": 4, "author": "uw_cma", "replies": [{"selftext": "Would still get something better, but cpu is having heatspreader on it does not suffer from pump out usually, different story if you delided it thermal paste not really viable", "ups": 1, "depth": 5, "author": "Melodias3", "replies": [{"selftext": "I'm just trying to explain that thermal paste does nothing except conducting heat. CPU and GPU *are chilled by coolers, not by thermal paste*.\n\nThermal paste may degrade if it's overheated (CPU and GPU too, btw).\n\nPlease look at [fan curve defaults](https://ibb.co/HFzCk78) (6800 XT).   \nCard's fans are limited with 57% at 80\u00b0C. Even if the card reaches 100+\u00b0C, the fan would not perform better. And that's the real issue.  \nWith that settings my card easily goes 100+. Regular thermal paste just can't handle that temps for a long time.\n\nBut when user [allows coolers to chill](https://ibb.co/b1GBtFK) the GPU, there are no 100+\u00b0C temperature regimes and spikes, thermal paste doesn't degrade so fast, no GPU overheating and throttling.\n\nIt looks like PTM just can handle higher temperatures longer than regular paste, but that's just a try to fix a symptom, while the root cause is the fan settings defaults.", "ups": 0, "depth": 6, "author": "uw_cma", "replies": [{"selftext": "PTM on direct die cooling is proven to be superior to thermal pastes full stop.\n\nIt is just better.", "ups": 1, "depth": 7, "author": "Parking_Automatic", "replies": []}]}]}]}]}]}]}, {"selftext": "I did the PTM on my 6950XT before Starfield. Did a rough screenshot because I was using it as personal data. When I was playing Remnant 2, I Had a 40 delta and after it changed to 20. [Before](https://snipboard.io/pbG1Zt.jpg) and [After](https://snipboard.io/wOldku.jpg). Again, this is just a rough screenshot of the same area in the game. My fans are also much better now. I think anyone with a Red Devil who has a pump out issue should use it. Some parts would hit over 106 but now no game pushes past 90. The delta is the most important number though. If your delta is over 30, that is an issue. \n\n\\*You might have only seen people who have little improvement because they don't actually need to do it. People will do a repaste at 25 degree delta and say how they only saw little improvement. There's a reason why we tell people over 35 to 40 is when it needs to be done. The hotspot alone means nothing. If your entire case is hot af, using PTM won't improve that. Some people on here use it on a card with 90 hotspot and 70 Global yet act surprised when it doesn't help at all.", "ups": 7, "depth": 1, "author": "xZethx", "replies": [{"selftext": "yes... when I pulled the cooler off I could see that the paste was not covering everything well.... I did the 7950 mod and right now im running my card at 2700 on the core ... and it's sitting at 82C.. something I could NEVER come close to accomplishing before (anything over 2400 would instantly hit 110C on the hotspot)... it's actually pretty nuts.  I understand why the poster above would be doubtful... and I didnt write a paper on it while I was doing it .. so all I can say is believe it or not it worked for me.", "ups": 3, "depth": 2, "author": "silicosick", "replies": [{"selftext": "I think we will see more issues with hotspots for the 6950xt. My Red Devil was the same way, all the paste was on side. I\u2019m happy it worked well for both of us. My brother is also going to use it because he has 108 on his 6950xt now.", "ups": 2, "depth": 3, "author": "xZethx", "replies": []}, {"selftext": "The OC Formula clocks to the fkn moon", "ups": 2, "depth": 3, "author": "chapstickbomber", "replies": [{"selftext": "yeah its insane", "ups": 2, "depth": 4, "author": "silicosick", "replies": []}]}]}, {"selftext": "Let me show [this](https://ibb.co/095s9cq).  \nCard: ASRock RX 6800 XT Phantom Gaming D 16G OC  \nGame: Forza Horizon 4, 2k ultra, dynamical optimization on  \nGPU util: 98%  \nGPU temp: 69\u00b0C  \nGPU Hot Spot temp: 73\u00b0C  \nDelta: 4\u00b0C\n\nI own this card for almost 11 months. No repasting.  \nHow is this possible? :)", "ups": 1, "depth": 2, "author": "uw_cma", "replies": [{"selftext": "Your GPU is not at 98% utilization.   \nIt is only drawing 165W and the TDP for your GPU is up to 300W.", "ups": 5, "depth": 3, "author": "Subject-Complex8536", "replies": [{"selftext": "That's a very good point, thank you!  \nActually, my card is limited by 272W. Afaik, for 6800 XT value may vary in range \\~250-300W.  \nGames usually do not fully load the card, you are right.\n\nLet's make it [harder](https://ibb.co/16pMHTV) this time.  \nApp: FurMark  \nPreset settings:  \n\\- duration: 60000 ms (1 min)  \n\\- resolution: 2560x1440  \n\\- anti-aliasing: 4xMSAA  \n\\- full screen: on\n\nGPU temp (max): 81\u00b0C  \nGPU Hot Spot temp (max): 87\u00b0C  \nDelta: 6\u00b0C\n\nStill the same ASRock RX 6800 XT, 11 months in use, no repasting.  \nI can increase test duration time and even run OCCT stability test to load the CPU in background, if you want.  \nHow is this possible? :)", "ups": 5, "depth": 4, "author": "uw_cma", "replies": [{"selftext": "Perhaps your card actually had properly applied paste, OP said their card had unevenly spread thermal paste. Also they're different cards so not really a great 1:1 comparison.", "ups": 1, "depth": 5, "author": "_vegetafitness_", "replies": [{"selftext": "Could be so.\n\nI think, the reason for high GPU temperatures and fast thermal paste degradation is poor default fan settings.  \nThe short answer how to fix that is provided [on this screenshot](https://ibb.co/b1GBtFK). And [here are](https://www.reddit.com/r/Amd/comments/16fzowo/comment/k06jkiz/?context=3) the details.\n\nAlso, there are some other actions were performed by me. I described them in [this thread](https://www.reddit.com/r/Amd/comments/16cujui/comment/jzm66mr/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3), really hope my explanations and thoughts would be useful for anybody else. Always open to discuss how to improve the solution.\n\n[Here](https://www.reddit.com/r/Amd/comments/16by1w8/comment/jzgj6l4/?context=3) I asked the guy for the FurMark test results with Upsiren U6 and Honeywell PTM7950. And they are good! I'm considering repasting my card. Just want to share my findings, 'cause repasting is not the only thing to be done in order to have lower temperatures.", "ups": 0, "depth": 6, "author": "uw_cma", "replies": [{"selftext": "Oh yeah, for sure. Having an adjusted fan curve is really important. My card (same model as OP) also has high junction temp. The delta is something like 25-30c. I have some MX-4 lying around but this 7950 stuff I'm confused about, is this meant to go on pads or the die?", "ups": 1, "depth": 7, "author": "_vegetafitness_", "replies": [{"selftext": "It is for the die only.\r  \nImo, that's worth to consider changing the other thermal pads only in case of real need.", "ups": 1, "depth": 8, "author": "uw_cma", "replies": [{"selftext": "Interesting, where I live houses are cooled mostly by central AC so cooling is super expensive. During the winter my room is much cooler and the temps dont reach so high so its only a problem for a few months of the year ![gif](emote|free_emotes_pack|joy)", "ups": 1, "depth": 9, "author": "_vegetafitness_", "replies": []}]}]}]}]}, {"selftext": "Your GPU delta is low , Your temps are still really high.\n\nYou didn't bring the hotspot temp down.....you just brought all the other temps upto its level.", "ups": 1, "depth": 5, "author": "Parking_Automatic", "replies": [{"selftext": "With PTM and fan defaults the hot spot temps would be pretty the same in case of long duration test, please check [this thread](https://www.reddit.com/r/Amd/comments/16by1w8/comment/jzgj6l4/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3).  \nIf you have additional questions, feel free to ask.", "ups": 1, "depth": 6, "author": "uw_cma", "replies": []}]}, {"selftext": "87c hotspot after a 1 minute test.....yeah that's not good.", "ups": 1, "depth": 5, "author": "Parking_Automatic", "replies": []}]}]}]}]}, {"selftext": "[deleted]", "ups": 1, "depth": 1, "author": "[deleted]", "replies": [{"selftext": "The screenshot lacks the CPU frequencies and voltages.  \nThe CPU package power was 45W in the first case and 41.8W in the second one.\n\nI would agree, that PTM improves the laptop's cooling, but it looks like in the first case frequencies were higher, and that could also be the reason of throttling.", "ups": 1, "depth": 2, "author": "uw_cma", "replies": [{"selftext": "[deleted]", "ups": 1, "depth": 3, "author": "[deleted]", "replies": [{"selftext": "Sure. I think it would be fine :)", "ups": 1, "depth": 4, "author": "uw_cma", "replies": []}]}]}]}, {"selftext": "yup. that and the fact that hotspot temps dont go down by 30\u00b0 due to a repaste unless maybe u had a huge air pocket in the thermal compound right at the hotspot sensor on the die.\nas a matter of fact repasting actually affects non-hotspot temp more than the hotspot. \nand even there an almost perfect result would be -10...-15 with -20 already being in the realm of either miracles or a mounting error / paste missing prior already.", "ups": 1, "depth": 1, "author": "Portbragger2", "replies": []}, {"selftext": "PTM is really good for keeping the same performance for a longer time than the pastes. nvidia used it on 4090FE and I bought it then for my aging 3090 which was again having hotspot isues after a repaste.\n\nA year, later the delta is still 13C on the 3090 and that with a rather botched application.", "ups": 1, "depth": 1, "author": "bctoy", "replies": []}, {"selftext": "i dont have before video of the stock temps, but i do have video of ptm7950.  And maybe mx6 but need to double check.\n\nAnyhow, 7900xt phantom gaming.  At new and 1050mv, stock pl settings, it was \\~85c with edge/hotspot delta of 20c. After 3 months the delta went up to 30c and i was getting hotspot of 95c.  Ambient has been 22-24c.\n\nPut mx6 on it, temps went back to around stock. Ran that for about a week. Then decided to sell the card as it wouldn't fit the waterblock i got.\n\nbeing paranoid about pump out and cranky buyer within 6 months, i put ptm7950 on it.  30min of heaven maxes at 80c hotspot.  All temps were compared to an open case using heaven benchmark.", "ups": 1, "depth": 1, "author": "jaymobe07", "replies": []}]}, {"selftext": "6900 xt repasted with PTM7950 and pads replaced with Upsiren U6 pro and saw significant hotspot drops as well. They say it lasts forever which is a plus I guess .", "ups": 4, "depth": 0, "author": "DiAvOl-gr", "replies": []}, {"selftext": "In what form?\n\nThermal pad or thermal paste in tubes?\n\nWhat thickness if a thermal pad?", "ups": 1, "depth": 0, "author": "PotentialAstronaut39", "replies": [{"selftext": "i did the same with my reference 6800 and it also worked really well \n\ni used the 0.2mm pad", "ups": 5, "depth": 1, "author": "IllbilItiliselI", "replies": []}, {"selftext": "pad form .2mm", "ups": 2, "depth": 1, "author": "silicosick", "replies": []}]}, {"selftext": "Can you do a follow up with screenshots of unreal4 games like Kena or the witcher and 3dmark\n\nWhere do i get it", "ups": 1, "depth": 0, "author": "wingback18", "replies": []}, {"selftext": "What the fuck 110C? Isn't that wayyyy more than the maximum? Damn asrock sure did fuck up with amd cards if you reach 110C temps.", "ups": -8, "depth": 0, "author": "BestNick118", "replies": [{"selftext": "&gt;What the fuck 110C? Isn't that wayyyy more than the maximum?\n\nExactly 110c is the max on hotspot.", "ups": 10, "depth": 1, "author": "Evonos", "replies": []}, {"selftext": "Typical temp on the hotspot.", "ups": 4, "depth": 1, "author": "HeywoodJaBlessMe", "replies": []}, {"selftext": "AMD is known for having uneven chips sometimes not sure if this is always case, also cooling surfaces aren't always flat either all this can contribute to pumpout so overtime paste is pumped out from heat and cooling cycles thermals get worse as result, especially if paste is not resistant to pumpout or pumps out easily this proces go's really quickly at high watts, especially if its high wattage for the amount of area it covers, lets say 300+ watts in tiny area vs big area in tiny area it just pumpout really fast, thermal paste is not always viable, which is why liquid metal is recommended when going for delided cpu for example.\n\nAlthough i really wanna see if PTM7950 is viable on delided CPU it probably is, just shame its not readily and easily avaible via typical computer webshops, and only via chinese resellers for example\n\nAs for 110c its within spec but its not something you wanna hit, especially considering it will nerf the boost you get out of your card.", "ups": 2, "depth": 1, "author": "Melodias3", "replies": [{"selftext": "PTM7950 is second only to liquid metal in thermal performance for any application it is used for, plus it is non-conductive and not messy,  it is available on amazon prime.", "ups": 1, "depth": 2, "author": "EolasDK", "replies": []}]}, {"selftext": "The answer to your question is no. 110c is exactly the maximum before a card begins to throttle. My Powercolor 5700 XT also hits 110 after a few months, and only stops when I repaste it.", "ups": 1, "depth": 1, "author": "CapybaraDlvry", "replies": [{"selftext": "Oh alright, I thought it was 100c.", "ups": 1, "depth": 2, "author": "BestNick118", "replies": []}, {"selftext": "Stop using toothpaste as thermal paste then. Good paste is fine for 2 years.", "ups": 1, "depth": 2, "author": "Reddituser19991004", "replies": [{"selftext": "Any recommendations? I'm using a huge thing of Arctic MX-4 I bought like 3 years ago.", "ups": 1, "depth": 3, "author": "CapybaraDlvry", "replies": [{"selftext": "New Mx-4? You might be using 5 or 6 year old paste. If you bought it 3 years ago, it's at least that old. It shouldn't be bad, but there's no way it should only be lasting a few months so I'm leaning towards it has to be.", "ups": 1, "depth": 4, "author": "Reddituser19991004", "replies": [{"selftext": "It's called pump out. Can happen to new paste.", "ups": 2, "depth": 5, "author": "jonboy999", "replies": []}]}]}]}]}, {"selftext": "That's driver issue. I have ASRock card as well.  \nThe default settings limit the fan with [57% at 80\u00b0C](https://ibb.co/HFzCk78) (RX 6800 XT). If the card reaches 110\u00b0C, the fan would still be performing on the same level as it was for 80\u00b0C.\n\nHowever, could be easily [fixed](https://ibb.co/hMvjLVj).", "ups": 1, "depth": 1, "author": "uw_cma", "replies": []}]}, {"selftext": "fun", "ups": 1, "depth": 0, "author": "Florette-Grimmyre", "replies": []}, {"selftext": "My Sapphire Nitro 6950 XT was pretty new, temps however sucked for hotspot and edge (yes, even gpu temp was high, not just hotspot) so I did a repaste with Noctua H2 and it did cut them down by about 10-15 degrees, maybe more. Went from a cooking 100+ degrees at 270w to more 90-95 degrees. Edge tends to stick around 80. Was also the first time I took it apart so maybe that helped? Either way temps went down, I keep fans quiet without roasting it now. Since then I have also added more case fans (which also stay quiet) but that wasn\u2019t why it was bad before.", "ups": 1, "depth": 0, "author": "Hayden247", "replies": []}, {"selftext": "with liquid metal i hit under 70oC at the same power, reference cooler..", "ups": 1, "depth": 0, "author": "Karma_Robot", "replies": []}, {"selftext": "Doing the same with my 6700xt , and from 100\u00b0C down to 80\u00b0C max hotspot. And this thing doesnt go pumped out at all which is way way better than any thermal paste in long run", "ups": 1, "depth": 0, "author": "MassiveCantaloupe34", "replies": []}, {"selftext": "I have noticed similar problems on my Reference XTX.  Even when OC'ing to 400 watts and running 3D Mark tests, I would hit \\~89\u00b0 junction temps back when I ran in Dec/January.  But last \\~month or so, I've noticed Junction climbing higher and higher.  It hit 101\u00b0 in Starfield the other day during a dialog scene. \n\nThat ChrisFix GPU repair guy on Youtube said that AMD's chips tend to be a bit concave and that this can happen over time.  Unsure why that would be the case, unless all TSMC chips are like this?\n\nBUT, unsure if it could be related to newer drivers too.  Haven't gone back to test other drivers yet. \n\nAlso been doing tons of Stable Diffusion renders, maybe the short term high/low load cycles have accelerated aging of paste.  Common that GPU will maxed for \\~3 mins, then normal, repeat.", "ups": 1, "depth": 0, "author": "WubWubSleeze", "replies": [{"selftext": "I suspect its a problem with the chiplet design that makes it more prone to pump out.\n\nI had a 7900XT that was getting to 100c hotspot , I used 3 different pastes , TFX , MX-6 and some deep cool generic stuff.\n\nThey all dropped the hotspot to like 77-80c but after 2-3 weeks it would climb again.\n\nSwitched to ptm7950 a month ago and the temp sits pretty happily at 60c gpu and 75c hotspot.\n\nThats with the fan speed at 1800rpm which is just before it starts to become audible without my headset on.\n\nIt's also a reference 7900XT so I imagine some beefier aib coolers would do even better.\n\nCouldn't be happier with it , There's a reason it comes stock on a 4090.", "ups": 2, "depth": 1, "author": "Parking_Automatic", "replies": [{"selftext": "Ptm7950... googling tells me this is a Honeywell brand thermal pad?", "ups": 1, "depth": 2, "author": "WubWubSleeze", "replies": [{"selftext": "Yes.\n\nCalling it a pad is a bit of a stretch though.\n\nIt's like a solid sheet of 0.2mm paste inside 2 sheets of plastic.\n\nIt differs from regular paste because it's a phase change material , It uses heat and pressure to get a very good mould on the Coldplate and whatever you are cooling.\n\nIt's really good.", "ups": 1, "depth": 3, "author": "Parking_Automatic", "replies": [{"selftext": "Oh nice, I'll keep an eye on things and may consider using it on my XTX. Was considering an RMA as  I'm still within the year warranty, but I'm not so sure they'll offer me an RMA since Junction isn't hitting 110\u00b0 like the bad batch of early cards.  Who knows, maybe they will.  If not though, I got my sights on the PMTM 7950!", "ups": 1, "depth": 4, "author": "WubWubSleeze", "replies": []}]}]}]}]}, {"selftext": "Did your boost clock increase though? If not, then there's not much of a point.", "ups": 1, "depth": 0, "author": "Dos-Commas", "replies": [{"selftext": "It won't have but some people prefer to have a cooler running card which in turn will stop the fan speed from ramping up too much.", "ups": 1, "depth": 1, "author": "Parking_Automatic", "replies": []}]}]}
{"post": {"title": "Going from a 3070ti to a 6950xt ($500 used). Is there a DLSR equivalent, or upscaling tech?", "subreddit": "Amd", "selftext": "Thank you!", "ups": 23, "permalink": "/r/Amd/comments/16g59fa/going_from_a_3070ti_to_a_6950xt_500_used_is_there/", "num_comments": 87}, "replies": [{"selftext": "probably a ~~Sony A7R IV~~ Canon 5D IV", "ups": 16, "depth": 0, "author": "ayunatsume", "replies": [{"selftext": "I had to scroll way too far to find this joke", "ups": 5, "depth": 1, "author": "Techmoji", "replies": []}]}, {"selftext": "Pretty sure VSR is an alternative to DSR.", "ups": 30, "depth": 0, "author": "CheemsGD", "replies": [{"selftext": "He said dldsr.\n\nAMD doesn't have an equivalent to that.\n\nOnly dsr which is vsr.", "ups": 37, "depth": 1, "author": "Evonos", "replies": [{"selftext": "Why was this downvoted? This is simply, literally the facts. AMD doesn't have Deep Learning VSR.", "ups": 25, "depth": 2, "author": "ArateshaNungastori", "replies": [{"selftext": "It's now up voted.\n\nWeirdly people on the amd sub often seem to confuse features.\n\nLike mention nvidias background fps limit.\n\nYou will get waves of people that will try to explain to you that chill does this ( no it doesn't) the plain existence and how a background fps limit works seems absolutely alien to many people", "ups": 10, "depth": 3, "author": "Evonos", "replies": [{"selftext": "Not many people know the exact details about fps limiters.", "ups": 5, "depth": 4, "author": "cellardoorstuck", "replies": [{"selftext": "Yes... But alone the word \"background\" should make them atleast Google if they don't know what I mean...\n\nEvery other fps limit is a foreground fps limit ( frtc, chill, rtss and stuff)", "ups": 10, "depth": 5, "author": "Evonos", "replies": [{"selftext": "You overestimate the average redditor. They are not cross-referencing facts, just responding and then being corrected (hopefully) down the line.", "ups": 8, "depth": 6, "author": "TheOkComputerGuy", "replies": []}]}]}, {"selftext": "Lots of people here can't just admit Nvidia has a superior software experience", "ups": 3, "depth": 4, "author": "AdStreet2074", "replies": [{"selftext": "I don't have to sign in to use Radeon software. That alone kills the software experience when I have to touch force. Same with razors mouse software.", "ups": -3, "depth": 5, "author": "spetanis", "replies": []}]}]}, {"selftext": "Amd doesnt have anything with deep learning as far as i know.", "ups": 6, "depth": 3, "author": "Yilmaya", "replies": [{"selftext": "They do actually. https://community.amd.com/t5/gaming/introducing-amd-noise-suppression-new-performance-optimizations/ba-p/531860\n\nEdit: Why is this downvoted??? Are you guys in denial? Did you even check the link?\n\n&gt; AMD Noise Suppression reduces background audio noise from your surrounding environment, providing greater clarity and improved concentration whether you are focused on an important meeting or staying locked-in on a competitive game. By using a real-time, deep learning algorithm to reduce background audio noise, this new feature works for both your input and output devices across any AMD-powered system, removing unwanted background noise captured on your microphone or from someone else\u2019s device.\n\nAlso from verge https://www.theverge.com/2022/7/27/23280287/amd-noise-suppression-software-release-rtx-broadcast-voice\n\nA day later edit: This comment was at -7 at some point yesterday. Thankfully it's looks like now above zero. Some people really lost their minds seeing AMD having deep learning and didn't even check the literally AMD's post about it wow.", "ups": 3, "depth": 4, "author": "ArateshaNungastori", "replies": []}]}]}, {"selftext": "[deleted]", "ups": -5, "depth": 2, "author": "[deleted]", "replies": [{"selftext": "\u201cTo reduce demand\u201d is a pretty big difference. When you can get equivalent image quality of DSR 2.25x at the performance levels of 1.78x using DLDSR it makes them quite different.", "ups": 11, "depth": 3, "author": "usual_suspect82", "replies": [{"selftext": "Reducing demand is what you just described. A big difference doesn\u2019t exactly make it invalid.", "ups": -9, "depth": 4, "author": "CheemsGD", "replies": [{"selftext": "Well, saying \u201cthey\u2019re the same\u201d, then saying \u201cAI accelerated to reduce demand\u201d is contradicting.", "ups": 6, "depth": 5, "author": "usual_suspect82", "replies": [{"selftext": "And\u2026 when did I say they\u2019re the same? What I said was basically \u201cDSR but with AI so it\u2019s better.\u201d", "ups": -3, "depth": 6, "author": "CheemsGD", "replies": [{"selftext": "So it's DSR but not DSR might as well describe what you said. Still no AMD counterpart so moot point.", "ups": 2, "depth": 7, "author": "Darkomax", "replies": []}]}]}, {"selftext": "OP still asked for DL dsr.\n\nWhich amd lacks.\n\nAMD only got DSR ( vsr)", "ups": 2, "depth": 5, "author": "Evonos", "replies": []}]}]}, {"selftext": "Actually.\n\nDldsr is awesome.\n\nI had a 3080 till 4 months ago and basically 4k dldsr is with the performance of 1440p.\n\nAnd looks even better than normal dsr.\n\n\n\nAnd yes.\n\nAMD got vsr = dsr.\n\nAMD got nothing like dldsr sadly.\n\nSo tl Dr.\n\nAMD got dsr ( vsr) but no dldsr ( none)", "ups": 6, "depth": 3, "author": "Evonos", "replies": [{"selftext": "I\u2019m not entirely sure why you\u2019re just here in this thread talking about you liking a GPU feature.", "ups": -5, "depth": 4, "author": "CheemsGD", "replies": [{"selftext": "OP asked if amd got DL dsr\n\nI answered with no.\n\nAnd explained that amd only got DSR which is correct.\n\nSimple.", "ups": 11, "depth": 5, "author": "Evonos", "replies": []}]}]}, {"selftext": "Yeah as in it's significantly more useful DSR than just brute forcing it with raster. It's a big difference.", "ups": 2, "depth": 3, "author": "Wander715", "replies": []}]}]}, {"selftext": "Correct", "ups": 5, "depth": 1, "author": "naxatra", "replies": []}]}, {"selftext": "I think the AMD equivalent is setting VSR to render at a higher resolution (same as DSR on Nvidia) and then using FSR in game.", "ups": 17, "depth": 0, "author": "Thing_On_Your_Shelf", "replies": [{"selftext": "That isn\u2019t a dldsr equivalent", "ups": 19, "depth": 1, "author": "ARedditor397", "replies": [{"selftext": "Its not the same, but it\u2019s about as close as you can get.", "ups": 13, "depth": 2, "author": "Thing_On_Your_Shelf", "replies": [{"selftext": "Since no one really answered you, it isn't the same. Reading down the chain, don't understand the infighting but, DLDSR has no upscaling function, it's purely downscaling.\n\nWhat you suggest would add a layer of upscaling which comes with it's own issues, remember you aren't upscaling a higher resolution image (what you expect the VSR to do) you're upscaling from the output resolution.\n\nSo if you use VSR to go from 8K to 4K, then FSR, you're doing FSR from 1440p (or whatever) to 4K, not 8K to 4K.\n\nWith DLDSR you go from 8K to 4K via the AI-algorithm, and it's that AI part that reduces the performance hit versus DSR 8K to 4K.\n\n(Numbers used just to illustrate my point, not going to do conversions).\n\nI'd assume by \"closet you can get\" you are implying performance hit. That also isn't true since you still got to do the VSR stage, and this is where DLDSR improves on DSR, the performance hit is lower. You're solution would add more penalties due to all the extra steps being done versus what DLDSR is doing.", "ups": 1, "depth": 3, "author": "railven", "replies": []}]}, {"selftext": "How is it different?", "ups": 4, "depth": 2, "author": "PeripheralDolphin", "replies": [{"selftext": "DLDSR essentially provides improved image quality, and isn't as demanding as traditional DSR\n\nThis video sums it up well:\n\nhttps://youtu.be/c3voyiojWl4?si=Z4uyMQO_wL9L8R03", "ups": 11, "depth": 3, "author": "TalkWithYourWallet", "replies": [{"selftext": "VSR with FSR would do the same with DLDSR than?", "ups": -6, "depth": 4, "author": "Yilmaya", "replies": [{"selftext": "VSR would be equivalent to DSR", "ups": 6, "depth": 5, "author": "TalkWithYourWallet", "replies": [{"selftext": "You clearly just read my first word and made a comment.", "ups": -5, "depth": 6, "author": "Yilmaya", "replies": [{"selftext": "I didn't, I'm telling you the technology VSR is equivalent to\n\nVSR is a down sampling method, FSR is an upscaling  method \n\nThey aren't the same technology, they do different things, and aren't comparable", "ups": 5, "depth": 7, "author": "TalkWithYourWallet", "replies": [{"selftext": "I couldnt tell my opinion clearly i guess. Let me re-phrase. Using FSR with VSR will make same effect as DLDSR.", "ups": -1, "depth": 8, "author": "Yilmaya", "replies": []}]}]}]}]}]}, {"selftext": "They are not even remotely the same tech one uses AI and is built off the same algorithm and has seperate training to DLSS", "ups": -13, "depth": 3, "author": "ARedditor397", "replies": [{"selftext": "Thanks for the downvote for me asking a benign question because I don't use AMD graphics cards! Appreciate it! Really encourages learning!\n\n&amp;#x200B;\n\nAnd secondly, I thought both DLDSR and VSR just makes you run at a higher than native res", "ups": 5, "depth": 4, "author": "PeripheralDolphin", "replies": [{"selftext": "You are referring to **DSR** (Not the same technology).\n\n**DLDSR** is a seperate technology all together.", "ups": -3, "depth": 5, "author": "ARedditor397", "replies": [{"selftext": "Just admit you don\u2019t know how the tech works, jesus.", "ups": -4, "depth": 6, "author": "TheOkComputerGuy", "replies": [{"selftext": "Dldsr is AI driven DSR and VSR are not", "ups": 4, "depth": 7, "author": "ARedditor397", "replies": []}]}]}]}]}]}]}]}, {"selftext": "AMD has a DSR equivalent, but not a DLDSR equivalent.", "ups": 7, "depth": 0, "author": "Practical-Canary8782", "replies": [{"selftext": "Intel coming in hot", "ups": -3, "depth": 1, "author": "Classic_Hat5642", "replies": []}]}, {"selftext": "No, the closest you can get is using VSR and then FSR in-game (provided it's supported).\n\nYMMV, but having used both DLDSR and VSR+FSR I would say don't expect to get the same image quality at the same image cost (i.e., if you bump up to 8k with VSR then that might change things).\n\nThat said, sometimes you will get pretty close at the same/similar cost, other times you won't (kinda depends on FSR version game side, and how good the individual FSR implementation is, as this can vary wildly).\n\nOn the whole though, image quality is good whenever you're starting with an above native resolution image. It's just not as good as DLDSR/nor is it a like for like competitor.", "ups": 5, "depth": 0, "author": "BigGirthyBob", "replies": []}, {"selftext": "No amd doesn't have an dldsr equivalent.\n\nOnly a dsr one which is called vsr.\n\nAlso if you ever used the background fps limit in the driver amd lacks that too.\n\n( no chill and frtc isn't that read the word \"background\") and if you need further explanation check my comment history all explained including Videos.", "ups": 19, "depth": 0, "author": "Evonos", "replies": [{"selftext": "Chill would limit the fps when the game is out of focus", "ups": 0, "depth": 1, "author": "plaskis", "replies": [{"selftext": "Nope.\n\nAs soon as you move your mouse on the desktop or type the fps raises.\n\nDo as I said and check my comments proof videos and everything already delivered.\n\nWith like 20 + comments debunking why amd doesn't have a background fps limit and multiple explanations why chill isn't that and will never work for that.", "ups": 0, "depth": 2, "author": "Evonos", "replies": [{"selftext": "What can't you post the comment here?", "ups": -5, "depth": 3, "author": "el_pezz", "replies": [{"selftext": "He is correct I have a 6950xt", "ups": 6, "depth": 4, "author": "coyotepunk05", "replies": []}, {"selftext": "What, can't you simply check for yourself?\n\nIt's also roughly 20 comments if I link them all reddit will just delete the comment.", "ups": 9, "depth": 4, "author": "Evonos", "replies": []}]}]}]}]}, {"selftext": "Amd does not have a DLDSR equivalent, just a DSR equivalent.", "ups": 5, "depth": 0, "author": "sackblaster32", "replies": []}, {"selftext": "Gotta love how the comment sections turns it into a fight because people can't come along on what counts as a DLDSR alternative and what doesn't.\n\nSometimes you just need to keep it simple, there is no DSLR equilavent period. AMD could benefit a lot from a feature that's like it. \n\nAs for other upscaling tech, you can use RSR which helps with improving how lower than native resolution looks in games, if FSR2 isn't an option or not preferred.", "ups": 2, "depth": 0, "author": "RCFProd", "replies": []}, {"selftext": "You Will not need it if You play at 1440p", "ups": 0, "depth": 0, "author": "Chilean_Chargaff", "replies": [{"selftext": "You probably don't understand what DLDSR is if you say it's not useful for 2560x1440.\n\nIt's useful for all resolutions, it helps clean up the image significantly for a big performance cost.", "ups": 16, "depth": 1, "author": "heartbroken_nerd", "replies": []}]}, {"selftext": "There isn't a driver-native toggle, but you can achieve similar by using VSR to run at a higher resolution and then use FSR 2 in-game to upscale to that native resolution.", "ups": 4, "depth": 0, "author": "Demy1234", "replies": [{"selftext": "It looks much worse than dldsr and isn\u2019t close or a related technology", "ups": 8, "depth": 1, "author": "ARedditor397", "replies": [{"selftext": "It actually looks much better because it is not prone to DL artifacts.", "ups": -8, "depth": 2, "author": "MAXFlRE", "replies": [{"selftext": "DL artefacts while downsampling? Do you even know how DLDSR works?", "ups": 6, "depth": 3, "author": "4514919", "replies": []}, {"selftext": "LoL nope", "ups": 4, "depth": 3, "author": "Classic_Hat5642", "replies": []}]}]}]}, {"selftext": "That seems like a fairly small, questionable value upgrade, unless you need the VRAM?", "ups": 1, "depth": 0, "author": "LongFluffyDragon", "replies": [{"selftext": "Literally fucking everyone who\u2019s into modern enough games to justify a high end machine needs more than 8GB.  It\u2019s also a pretty massive performance jump, the 6950 is the best part of a 3090 in non-RT scenarios.", "ups": 2, "depth": 1, "author": "ATrayYou", "replies": []}, {"selftext": "8gb today is a border edge of comfort.", "ups": 1, "depth": 1, "author": "MAXFlRE", "replies": []}]}, {"selftext": "No direct competition to DLSDR from AMD yet, altough they presented tgat they soon will launch Native AA, which basically would be FSR without upscaling part. Yet to see how it will perform, they said Q1 2024 if I'm not mistaken", "ups": 1, "depth": 0, "author": "mixedd", "replies": []}, {"selftext": "Yes, amd virtual super resolution, I use it to play games at 1440p or 4k on my 1080p monitor. VSR + FSR is better than native 1080p on my 27inc monitor.", "ups": 2, "depth": 0, "author": "nzmvisesta", "replies": [{"selftext": "No lol", "ups": -3, "depth": 1, "author": "Classic_Hat5642", "replies": []}]}, {"selftext": "definetely not worth it lol", "ups": 0, "depth": 0, "author": "tehbabuzka", "replies": []}, {"selftext": "No", "ups": 0, "depth": 0, "author": "Alternatetbh", "replies": []}, {"selftext": "[deleted]", "ups": 0, "depth": 0, "author": "[deleted]", "replies": [{"selftext": "That is not the same as dldsr", "ups": 1, "depth": 1, "author": "ARedditor397", "replies": [{"selftext": "[removed]", "ups": 0, "depth": 2, "author": "[deleted]", "replies": [{"selftext": "What an asshole, correcting you.", "ups": 2, "depth": 3, "author": "GimmeDatThroat", "replies": []}]}]}]}, {"selftext": "Lol projecting.\nFirst line u put was swearing lol funny.\nYes 1199 and 799. Ur really not getting this are u, he looked at 500.\nCan u not see u have more than doubled the price.\nIf u want to give any suggestions maybe u should reframe from swearing that way someone may take u seriously.\nMaybe just make a new comment would be easier and nicer for u.", "ups": 0, "depth": 0, "author": "Tiny_Object_6475", "replies": []}, {"selftext": "dlss=Nvidia's **upscaling** tech\n\ndldsr=Nvidia's **downscaling** tech\n\nI'm going to guess that you actually meant to type dlss instead of dlsr (which is not a thing) since you used the word upscaling.  To answer that question, yes AMD uses fsr.", "ups": -2, "depth": 0, "author": "Unable_Tell_4986", "replies": [{"selftext": "False", "ups": -3, "depth": 1, "author": "Classic_Hat5642", "replies": []}]}, {"selftext": "Last time i checked the DSR(nvidia) was SHIT compared to VSR(AMD), so you all can just stop saying that vsr is the dsr equivalent.\r  \nIs there some DLDSR vs VSR comparison on the internet? last time i checked there isn't, so you all can just stop write nonsense sentences", "ups": -1, "depth": 0, "author": "bstardust1", "replies": []}, {"selftext": "Looks like a good price but for extra $250 dollars u can get a 7900 xt 20gb.", "ups": -2, "depth": 0, "author": "Tiny_Object_6475", "replies": [{"selftext": "Fucking stupid comment.  A thread full of factual inaccuracies and failures to read OP\u2019s post and somehow this takes the shit biscuit.  \n\nThat is literally half as much again.  For an extra $375 again, you might be able to find a 4080!  What a no brainer right?!  Moron.", "ups": 2, "depth": 1, "author": "ATrayYou", "replies": [{"selftext": "Wow someone with some series issues.\nDo u have anger management issues, I made a suggestion and ur calculations are so far off, can u do normal addition and subtraction.\nI am talking new not used for one. And the difference between a rtx 4080 and rx 7900 xt not xtx dumbo is about $400 so let's add up since u can't, $250 plus $400 equals $650, wow don't ur figures look a little off.\nMaybe u should reading before u comment !!!!", "ups": -2, "depth": 2, "author": "Tiny_Object_6475", "replies": [{"selftext": "Wow you\u2019re really not following at all are you?\nLet\u2019s break this down.\n\n500.  Plus 250 (which is half as much again).  Is 750.  \n\n750.  Plus 375 (which is half as much again).  Equals 1125.  With me?\n\nYou can get a 4080, yes NEW, for $1100!  That is LESS than $1125 last I checked!\n\nI have to say, the whole anger issues thing strikes me as projecting having read the rest of your comment.  But then again, I did expend a few brain cells and caffeine molecules in the process, so could be wrong.  Why would I be angry anyway?  You\u2019re not exactly attacking my worldview, you\u2019re just exceptionally dim.  Caught me off guard even by Reddit standards.", "ups": 1, "depth": 3, "author": "ATrayYou", "replies": []}]}]}]}, {"selftext": "Go AMD if you are going to use real resolution and real fps. AMD isn't good at faking it.", "ups": -10, "depth": 0, "author": "el_pezz", "replies": [{"selftext": "\u201cReal Frames\u201d \ud83d\ude02", "ups": 10, "depth": 1, "author": "ARedditor397", "replies": []}, {"selftext": "*confused unga bunga", "ups": 5, "depth": 1, "author": "kingdom9214", "replies": [{"selftext": "What am I confused about? Maybe I am", "ups": 1, "depth": 2, "author": "el_pezz", "replies": []}]}]}, {"selftext": "Play native. The added latency by dlss and fsr is not worth the extra fps imo", "ups": -10, "depth": 0, "author": "Rescre14", "replies": [{"selftext": "DLSS and FSR decrease latency, they don't increase it.", "ups": 11, "depth": 1, "author": "Apophis_06", "replies": []}, {"selftext": "Upscalers literally decrease latency.", "ups": 5, "depth": 1, "author": "sackblaster32", "replies": [{"selftext": "Yeah and what about DLSS 3.0 aka frame gen? Unplayable if you are input lag sensitive.  Native resolution is always the way to go for competitive gaming.", "ups": -2, "depth": 2, "author": "Rescre14", "replies": [{"selftext": "Frame gen is a completely different thing and its known to increase input lag.", "ups": 4, "depth": 3, "author": "sackblaster32", "replies": []}]}]}]}, {"selftext": "What resolution are you playing at?", "ups": 1, "depth": 0, "author": "urlond", "replies": []}]}
{"post": {"title": "Sapphire PURE 7700xt and 7800xt availability", "subreddit": "Amd", "selftext": "Update September 13: The PURE 7800xt model is now listed on Newegg, sold and shipped by Newegg. It is currently out of stock at the time of update. Thanks to u/fdiv for sharing this!\n\n [SAPPHIRE PURE Radeon RX 7800 XT Video Card 11330-03-20G - Newegg.com](https://www.newegg.com/sapphire-radeon-rx-7800-xt-11330-03-20g/p/N82E16814202439?Item=N82E16814202439&amp;SoldByNewegg=1) \n\n&amp;#x200B;\n\nUpdate September 12: I now see a listing for the PURE 7700xt model on Newegg, sold and shipped by Newegg. It is currently out of stock at time of update.\n\n[SAPPHIRE PURE Radeon RX 7700 XT Video Card 11335-03-20G - Newegg.com](https://www.newegg.com/sapphire-radeon-rx-7700-xt-11335-03-20g/p/N82E16814202438?Item=N82E16814202438)\n\n&amp;#x200B;\n\nOriginal post below:\n\nDoes anyone know if Sapphire will be releasing the PURE 7700xt and 7800xt models in North America anytime soon? I can only find them on European and Australian websites. And the only reviews I have seen with them are from Europe and Australia too. I have searched on Sapphire's own website and social media and there is nothing about where to buy this model. None of the videos I have watched with the PURE model show purchasing details either.\n\nSapphireNation website article link featuring the new model here:\n\n[https://www.sapphirenation.net/sapphire-customized-lineup-for-the-amd-radeon-rx-7800-xt\\_rx-7700-xt](https://www.sapphirenation.net/sapphire-customized-lineup-for-the-amd-radeon-rx-7800-xt_rx-7700-xt)\n\nKitGuru website article link featuring new Sapphire 7700xt and 7800xt models:\n\n[https://www.kitguru.net/components/graphic-cards/matthew-wilson/sapphire-rolls-out-new-rx-7700-xt-and-rx-7800-xt-pure-series-graphics-cards/](https://www.kitguru.net/components/graphic-cards/matthew-wilson/sapphire-rolls-out-new-rx-7700-xt-and-rx-7800-xt-pure-series-graphics-cards/)\n\nHardware Unboxed video link with Sapphire PURE 7700xt here:\n\n[https://youtu.be/\\_LEBEqsCwiM?si=qtgmFwPfx\\_JVskPp](https://youtu.be/_LEBEqsCwiM?si=qtgmFwPfx_JVskPp)\n\neTeknix video link with Sapphire PURE 7700xt here:\n\n[https://youtu.be/xlWLmTge2hk?si=ppo3A0HrQM6mdAm1](https://youtu.be/xlWLmTge2hk?si=ppo3A0HrQM6mdAm1)", "ups": 13, "permalink": "/r/Amd/comments/16g3cs1/sapphire_pure_7700xt_and_7800xt_availability/", "num_comments": 26}, "replies": [{"selftext": "I think did saw pure on Amazon or new egg at launch of the card idk what happened to it now", "ups": 4, "depth": 0, "author": "Ghostlyruby026", "replies": [{"selftext": "I didn\u2019t see it but maybe it was gone in an instant or maybe it was a 3rd party seller? I\u2019ve been checking daily since launch and I haven\u2019t seen a listing for it in the U.S.\n\nedit: Both the 7700xt and 7800xt PURE models are now on Newegg. I updated my post with links.", "ups": 2, "depth": 1, "author": "Latter_Ad6153", "replies": []}]}, {"selftext": "I think it will be a few weeks at least. White cards are always sought after and it seems like AIBs like to play into it even harder when they can.", "ups": 3, "depth": 0, "author": "Jaidon24", "replies": []}, {"selftext": "Is anyone else bothered by the random red accent in the middle of the card? It would look better if it was all white since the red will clash with many builds", "ups": 3, "depth": 0, "author": "cs342", "replies": []}, {"selftext": "I too have been waiting patiently for these to show up in the states...it's the last missing piece of my new \"white and brown\" build. Been spamming F5 since the morning of the 6th and nothing has popped up...", "ups": 2, "depth": 0, "author": "fdiv", "replies": []}, {"selftext": "The 7800 XT Pure just popped up on Newegg in the last few minutes, showing \"out of stock\" from the get go. Must be placeholders for when inventory appears in the system. Must be getting close!\n\nhttps://www.newegg.com/sapphire-radeon-rx-7800-xt-11330-03-20g/p/N82E16814202439", "ups": 2, "depth": 0, "author": "fdiv", "replies": [{"selftext": "Awesome! Thanks for sharing. Gonna keep hitting that refresh button now. I was tempted by the white ASRock Steel Legend but I definitely prefer the Sapphire PURE so I will continue to hold out for it.", "ups": 1, "depth": 1, "author": "Latter_Ad6153", "replies": []}]}, {"selftext": "7800 XT just became available to backorder (ETA 9/21/2023) and 7700 XT is in stock at newegg for USA.\n\nEDIT: Oddly, they are the only two Sapphire models that don't come with a starfield code - guessing a newegg oversight for the recent listing.", "ups": 2, "depth": 0, "author": "fdiv", "replies": [{"selftext": "Noticed this. I want to go ahead and put in an order, but not if I'm going to get screwed out of Starfield. Also, does Newegg not take AMD reference models for their trade-in program?", "ups": 2, "depth": 1, "author": "bigsnyder98", "replies": []}, {"selftext": "Yeah I was going to back order it and then I realized the Starfield code is missing\u2026 wondering if it truly is a Newegg thing or a Sapphire thing. The skeptic in me wonders if it is intentional since the white GPUs usually sell out much easier with their lower supply and so they just decided to put the (perhaps) limited number of game codes with the bulk sellers.\n\nI want it but without Starfield I might as well spring for the Nitro+ or some other model.", "ups": 1, "depth": 1, "author": "Latter_Ad6153", "replies": [{"selftext": "Just wrote this to AMD after getting pushback from both Newegg and Sapphire about the Starfield not being included.  Still awaiting AMD reply:   \n\"Hello, I just purchased new release PURE AMD Radeon\u2122 RX 7800 XT 16GB yesterday from Newegg (which is a consumer authorized merchant partner for Sapphire as well as most of your AMD graphics cards.) According to all marketing from AMD as well as the Sapphire website: https://www.sapphiretech.com/en/consumer/pure-radeon-rx-7800-xt-16g-gddr6 , this graphics card qualifies for the Starfield Premium edition bundle. I already wrote to Newegg, and they said it does not qualify. Reading the Sapphire website disclaimer, it states \"Available through participating retailers only.\" Newegg is definitely a participating retailer, as they are showing this Starfield bundled with other 7800 XT brands (ASRock, etc...). Also, since the Sapphire website here shows the 7800 XT as a qualified graphics card, and the only place for a consumer (non commercial) to purchase this specific Sapphire 7800 XT, by default this would have to be a participating and qualified purchase. Sapphire summarily wrote back to me to contact you at AMD directly, as AMD is handling the Starfield game code distribution. I could almost understand the participating vendors disclaimer if there were multiple storefronts offering the Sapphire 7800 XT, however Newegg is the only storefront for consumer purchase. So with one default storefront, combined with the Sapphire 7800 XT as a qualified graphics card, this purchase on Newegg has to qualify, since Newegg is a participating merchant.\"", "ups": 1, "depth": 2, "author": "DragonfruitNovel7274", "replies": []}]}]}, {"selftext": "This or the Yeston are the only white 7800 xt cards, right?   Whichever is cheaper or comes out first, I\u2019m gonna jump on it.", "ups": 1, "depth": 0, "author": "mastrofdizastr", "replies": [{"selftext": "There is also the ASRock steel legend which is white and available at Newegg. I haven\u2019t seen a Yeston white model available yet.", "ups": 1, "depth": 1, "author": "Latter_Ad6153", "replies": [{"selftext": "Ok, the Yeston is partially white; and will probably have the \u201cwaifu tax\u201d but I\u2019m ok with that.  The color card will determine the color case I get.", "ups": 2, "depth": 2, "author": "mastrofdizastr", "replies": [{"selftext": "For sure! I have a white case and that\u2019s why I\u2019m looking for a white GPU as well. I\u2019m a bit biased towards Sapphire so I was excited when they announced the white PURE model.", "ups": 1, "depth": 3, "author": "Latter_Ad6153", "replies": [{"selftext": "The sapphire looks better than the as rock imo.  I don\u2019t plan on over clocking or anything, just going for a visually appealing build.", "ups": 2, "depth": 4, "author": "mastrofdizastr", "replies": [{"selftext": "There is also the ASUS TUF White model, it has a very clean white look. I won't buy it because I prefer Sapphire's business model, but it's an option.\n\nhttps://www.asus.com/us/motherboards-components/graphics-cards/tuf-gaming/tuf-rx7800xt-o16g-white-gaming/", "ups": 2, "depth": 5, "author": "fdiv", "replies": [{"selftext": "Hmm.  I do have a TUF mb.", "ups": 2, "depth": 6, "author": "mastrofdizastr", "replies": [{"selftext": "I also have a TUF motherboard and didn\u2019t realize you could get a white TUF 7800xt right on the Asus website. Then again I do personally hold Sapphire in higher regard. And I wonder if they would give you a Starfield game code if you buy it directly from Asus.\n\nupdate: I just chatted with ASUS support, and they insisted that the 7800xt is not included on the Starfield promo bundle. I even found a page on their own website that says the 7800xt models come with Starfield but the support person just told me to contact AMD if I have concerns. So if you want Starfield, I wouldn't buy directly from ASUS.", "ups": 1, "depth": 7, "author": "Latter_Ad6153", "replies": [{"selftext": "i saw someone in another thread had this problem, too. and asus just said to talk to amd, and amd said to talk to asus...so i worry that the PURE also won't have it since it's not directly mentioned...", "ups": 1, "depth": 8, "author": "skald_plays", "replies": []}]}]}]}]}]}]}]}]}, {"selftext": "Still can't buy the 7800 xt in UK", "ups": 1, "depth": 0, "author": "BwianBadonde", "replies": []}, {"selftext": "Real question is, will it have a starfield code?", "ups": 1, "depth": 0, "author": "Astrothunderkat", "replies": []}, {"selftext": "Just got a message that 7800 was in stock at Newegg.  Was able to add to cart and purchase.", "ups": 1, "depth": 0, "author": "DragonfruitNovel7274", "replies": [{"selftext": "Aaaaand they're out of stock again. My backorder just went to packaging too though so that's pretty sweet.", "ups": 1, "depth": 1, "author": "fdiv", "replies": [{"selftext": "After going back and forth on it I think I may have to pass since Starfield is not bundled. I wanted to buy the game so not including it basically makes the price $600 for Starfield plus the PURE, and at that point I can\u2019t really justify it.", "ups": 1, "depth": 2, "author": "Latter_Ad6153", "replies": []}]}]}, {"selftext": "Just fyi, my backorder went to packaging about an hour ago.", "ups": 1, "depth": 0, "author": "stupidest_llama_ever", "replies": []}]}
{"post": {"title": "XDNA Ai benchmarks?", "subreddit": "Amd", "selftext": "I've searched high and low but I can't find any Ai-related benchmark information for the new 7040 series APUs. \n\nI realize most users are more concerned with general CPU performance and gaming performance. The inclusion of the XDNA AI accelerator has got me curious. \nDoes anyone know of any benchmarking of the Ai performance of these devices?", "ups": 13, "permalink": "/r/Amd/comments/16g24xa/xdna_ai_benchmarks/", "num_comments": 17}, "replies": [{"selftext": "You\u2019re gonna have to do it yourself. Take a Resnet50 model, quantise and run it according to the [guide](https://ryzenai.docs.amd.com/en/latest/getstartex.html). See how long inference takes and compare it with whatever benchmarks/papers/stuffs people put out online, there\u2019s plenty of that.", "ups": 12, "depth": 0, "author": "CeladonBadger", "replies": [{"selftext": "It certainly seems like benchmarking ANY Ai is a complex PITA. I fear you're right. Reading the documentation around ML Perf inference rules has my head spinning.", "ups": 2, "depth": 1, "author": "evilgeniustodd", "replies": []}]}, {"selftext": "There aren't any AFAIK and that's simply because developers need time and incentive to add support for Ryzen AI accelerators into their tools and test suits. Benchmarking AI performance is still an area under heavy development, especially for specialized accelerators in the PC space. \n\nThere are some pretty good gains running AI tests using AVX-512 on the 7040. Doesn't use the AI accelerators AFAIK.\n\nhttps://www.phoronix.com/review/amd-ryzen7040-avx512", "ups": 7, "depth": 0, "author": "Proliator", "replies": []}, {"selftext": "it's coming \nhttps://www.amd.com/en/products/ryzen-ai\n\nThis is in partnership with Microsoft's Windows to build apps and developers APIs to use it!\n\nWatch AMD's official YouTube video:\n\nhttps://youtu.be/xzUOhkqAItM", "ups": 2, "depth": 0, "author": "evilgeniustodd", "replies": [{"selftext": "So no point buying anything at the moment?", "ups": 1, "depth": 1, "author": "Yaris_Fan", "replies": [{"selftext": "Well, you could buy one of the 7040 series of chips?", "ups": 1, "depth": 2, "author": "evilgeniustodd", "replies": []}]}]}, {"selftext": "For tflops, the cores are equivalent to \u201cXilinx AI engines\u201d. That\u2019s where they come from. You just have to see the grid side.\nPlus a benchmark isn\u2019t all that useful unless there is something to compare it against.\n(Like Apple M2)", "ups": 1, "depth": 0, "author": "titanking4", "replies": [{"selftext": "I hear you. It seems that AI workloads and models are so variant and situation-specific. It's quite difficult to cross-compare hardware scenarios. \nIt's not like a video game where resolution, detail, and frame rate are easily compared between vastly different HW configurations.", "ups": 1, "depth": 1, "author": "evilgeniustodd", "replies": []}]}, {"selftext": "AI tasks aren't being ran on laptops, they are being ran on GPUs or Threadrippers and similar high-performant CPUs. Laptop is low-power consumption device, it has been minimized to have the lowest computing power for a specified power consumption requirement (because of battery). So, I don't think you will find these kind of benchmarks. But if you decide to buy a GPU, here is a good physics project that has benchmarks for many GPUs, so you can make your choice. You will need RDNA 3 for AI since it has WMMA (matrix multiplication) instructions, a stuff similar to \"CUDA cores\" in NVIDIA. RDNA 2 doesn't have it.[https://github.com/ProjectPhysX/FluidX3D](https://github.com/ProjectPhysX/FluidX3D)", "ups": -12, "depth": 0, "author": "nuliknol", "replies": [{"selftext": "AMD literally has plans for AI acceleration on CPUs", "ups": 7, "depth": 1, "author": "EnderOfGender", "replies": []}, {"selftext": "WMMA is basic ai acceleration not a CUDA or Intel AI Acclerator equal", "ups": 4, "depth": 1, "author": "ARedditor397", "replies": [{"selftext": "It should act as a tensor core equal", "ups": 3, "depth": 2, "author": "OSSLover", "replies": []}]}, {"selftext": "[lol](https://www.notebookcheck.net/AMD-outlines-plans-to-integrate-AI-XDNA-IPUs-across-its-entire-processor-portfolio.717919.0.html)", "ups": 4, "depth": 1, "author": "rilgebat", "replies": []}, {"selftext": "dude, [really](https://i.imgur.com/jFWU9SF.png)? Maybe get a clue, shave your neck beard, and avoid threadjacking in the future.", "ups": 1, "depth": 1, "author": "evilgeniustodd", "replies": [{"selftext": "sorry, didn\u00b4t think I am answering to a 10 year old guy,  just ignore my reply and move on, you seems to know it all, the world will be yours, definitely", "ups": 1, "depth": 2, "author": "nuliknol", "replies": [{"selftext": "&gt; AI tasks aren't being ran on laptops\n\nTell that to everyone using Nvidia Broadcast, basically every Adobe product, Blender, etc. AI tools and extensions are being integrated all over the software space. Even MS Paint has some Ai functionality now. 61% of personal computer sales are laptops. \n\nThere is plenty to use AI on now and much more is in the development pipeline. \n\n&gt;  \"You will need RDNA 3 for AI\".\n\nFirst, that is incorrect. I'm currently running models on a 6700xt workstation and a 680m laptop. Both are RDNA 2. Both run a bunch of stuff well. \nSecond, the 780M that comes in the 7040 series **IS** RDNA 3. \n \nI asked about a tool for a specific AMD product and your response was \"That's the wrong tool for the job\" even though you don't know what the specific job is. \nThanks for the assumptions and incorrect/outdated information. Proper neckbeard shit my guy.", "ups": 3, "depth": 3, "author": "evilgeniustodd", "replies": []}, {"selftext": "He knows a hell of a lot more than you do, that's for certain.", "ups": 3, "depth": 3, "author": "rilgebat", "replies": []}]}]}]}]}
{"post": {"title": "7900 XTX ASRock Phantom gaming", "subreddit": "Amd", "selftext": "Hello, \n\nJust purchased 7900 XTX ASRock Phantom gaming because of very good price in my country for this card. ( about 30%cheaper than other manufacturers )\n\nCould you guys tell me if you experienced problems with thermal throttling and high hot spot temperatures ?\n\nI have read a lot of opinions that people experienced these problems but I decided to risk it because of the price.", "ups": 43, "permalink": "/r/Amd/comments/16fsg7e/7900_xtx_asrock_phantom_gaming/", "num_comments": 80}, "replies": [{"selftext": "I have a asrock aqua, sitting here, and i may switch. Ive heard nothing but good things. Seems their MOBO's are doing well also. Just what ive seen.", "ups": 11, "depth": 0, "author": "Koyote7676", "replies": [{"selftext": "I have been buying ASRock mobo for the last 10 years. The z97 fatality I had with my 4790k is still rocking in my dad's actual desktop.", "ups": 10, "depth": 1, "author": "tyanu_khah", "replies": [{"selftext": "&gt;Could you guys tell me if you experienced problems with thermal throttling and high hot spot temperatures ?\n\nI also had the z97 fatality from \\~10 years ago that I just sold to a nice fellow looking for a starter PC for his young brother. Machine was still running strong with no issues, haha. Naturally, I put together 3 more systems with b550m steel legend mobos.", "ups": 3, "depth": 2, "author": "os2firefox", "replies": []}]}, {"selftext": "why would you switch if you have an aqua?", "ups": 7, "depth": 1, "author": "jaymobe07", "replies": []}]}, {"selftext": "i have it and its great in my open air bench and it undervolts like a beast got mine to a stable 1050 with full power draw", "ups": 15, "depth": 0, "author": "Ninjathelittleshit", "replies": [{"selftext": "That's really good to hear :)\nThanks", "ups": 3, "depth": 1, "author": "FreddyKS", "replies": []}, {"selftext": "1050 watts?", "ups": 9, "depth": 1, "author": "NewVegasResident", "replies": [{"selftext": "\ud83e\udd23", "ups": 7, "depth": 2, "author": "feorun5", "replies": []}, {"selftext": "Millivolts.", "ups": 4, "depth": 2, "author": "DuskOfANewAge", "replies": []}, {"selftext": "voltage", "ups": 3, "depth": 2, "author": "Ninjathelittleshit", "replies": []}, {"selftext": "He's talking about undervolt", "ups": 1, "depth": 2, "author": "Melodias3", "replies": []}, {"selftext": "1050 Amps", "ups": 1, "depth": 2, "author": "casiwo1945", "replies": []}, {"selftext": "1050 mv", "ups": 1, "depth": 2, "author": "smblt", "replies": []}]}, {"selftext": "Could you share your tuning? What kind of temps are u getting?", "ups": 1, "depth": 1, "author": "AndersNR", "replies": [{"selftext": "i just set my power limit to max and Voltage to 1050 and then tuned the fan curve a little but thats it everything else i left as is and i get about 58 gpu temp with a 86 hotspot temp peaking sometimes at 90-92", "ups": 1, "depth": 2, "author": "Ninjathelittleshit", "replies": []}]}]}, {"selftext": "Yes temperatures on hotspot amd vram, no on throttling but that may have happened if  i didnt repaste.  Temperatures rose to bad levels in a few weeks.  A significant load is needed for testing too, like Cyberpunk with RT or PT, Control with RT on, or 3dmarl Speedway stress test. \n\nAfter my second repaste, ive got pretty awesome temperatures. Shows how awful stock was.   Dont worry til u see hotspot nearing or going beyond 105c (when fans are ramping adequately, stock fan curves are awful so dont count on them, make your own).", "ups": 6, "depth": 0, "author": "Electrical-Bobcat435", "replies": [{"selftext": "Did you bother with warranty ?\nSo repaste helped but were thermal pads ok after disassembly ? Was it hard to do it ? :D\n\nAlso when did temperatures rise like that ? After a month or two or later ?", "ups": 1, "depth": 1, "author": "FreddyKS", "replies": [{"selftext": "I should have RMAd but didnt.  Ive repasted many laptops amd desktop gpus, this one was fairly easy.  Temps started rising a little each week. \n\nThermal pads were fine and could be reused but my vram temperature issue was because the cooler was warping the pcb after fully tightened, making an air gap appear between vram pad on the edge and the cooler (105c vram temps).  They had used 2mm there, on my second repaste (because of vram), i used 2.5mm pad on all vram. Took a few days to squish down well but then fine.\n\nNow, i typically see low 90s vram and hotspot.  Under biggest load i can put it under (Control, max settings and RT),  vram still 90s but hotspot will climb to 103c in certain areas, which is fine. My high end Radeon 6000s also tended to run around 100c hotspot so no worries there. \n\n Max on Xtx is 110c when throttling should start although i never saw throttle even when hitting 110c, musta been just a brief peak to 110.", "ups": 3, "depth": 2, "author": "Electrical-Bobcat435", "replies": [{"selftext": "Could I see a pic of your fan curve? Thanks!", "ups": 1, "depth": 3, "author": "Ok-Syrup8959", "replies": [{"selftext": "No need, use whatever u like low end.\n\nThe last point u can manually set... set that one to 99c = 80% or higher fan speed.\n\n(they dont let us set a point higher than 99c).", "ups": 2, "depth": 4, "author": "Electrical-Bobcat435", "replies": [{"selftext": "Thanks!", "ups": 1, "depth": 5, "author": "Ok-Syrup8959", "replies": []}]}]}]}, {"selftext": "on my xt version i skipped warranty because i didn't feel like waiting on rma just for a repaste.  Then to probably have same issue in 3 months.  \n\nI switched to the xtx model after finding out a gpu block wouldn't fit.  Removing the cooler is pretty easy.  Backplate screws, two screws holding the IO bracket to the coolers support bracket.  Then 4 screws on the gpu spring bracket.   the pads were fine on the xtx model.  Xt model needed new pads on one bank of memory modules.  I think they were 1mm but dont quote me.", "ups": 2, "depth": 2, "author": "jaymobe07", "replies": [{"selftext": "Good to hear it's not hard to do. Bad to hear eventually most of cards will have rise in temp :/\nHopefully not all models are all bad.\n\nI think first of all I will RMA if I can and then try changing thermal paste and pads if problem will occur", "ups": 1, "depth": 3, "author": "FreddyKS", "replies": [{"selftext": "if you do repaste i highly recommend looking into PTM 7950, i recently replaced the paste on a 6950 XT with PTM 7950 due to high hot spot temps. it does a much better job of keeping hot spot temps down than paste, doesn't pump out over time and doesn't dry out (it has industrial applications so the life span is significantly longer than consumer pastes)\n\nhttps://imgur.com/a/4pOF0HO", "ups": 2, "depth": 4, "author": "MaddexS", "replies": []}]}]}, {"selftext": "I had one that repasting didn't fix but I made a fake sticker that looked identical and rma'd it cause I know how to do it.\n\nIdk if I was unlucky cause I don't think many people had this problem but my replacement has been excellent", "ups": 1, "depth": 2, "author": "tenfootgiant", "replies": [{"selftext": "Since replacement everything is OK for you ? :)", "ups": 1, "depth": 3, "author": "FreddyKS", "replies": [{"selftext": "Yeah it's been fantastic", "ups": 2, "depth": 4, "author": "tenfootgiant", "replies": []}]}]}]}, {"selftext": "What kind of curve are you running?", "ups": 1, "depth": 1, "author": "smblt", "replies": [{"selftext": "Run whatever u like, as I stated to another,.... that last fan point, 99c should be set and a fairly high fan speed at 99c, 80% or higher as a suggestion.   \n\nOtherwise, u may be running a high hotspot temperature simply because fans are not running high enough.   \n\nIf throttle temperature is 110c the fan should be blasting just before it reaches it. \n\n I wish we could set values over 100 but software doesnt let us.", "ups": 1, "depth": 2, "author": "Electrical-Bobcat435", "replies": []}]}]}, {"selftext": "I have the same card, I normally get sapphire cards, but having watched some card teardown videos and discovered that asrock actually make the power input circuitry better I decided to get one.\n\nIt is an excellent card, I turned it onto quiet mode in the adrenalin software and I'm getting better performance than my old 6800xt nitro+ se at about 150w. I play a lot of squad and normally it sits there at between 120 and 140 fps on fan stop at 51\u00b0.\n\nEven playing metro exodus enhanced edition and using 350w+ the highest temp I have seen is 70\u00b0", "ups": 8, "depth": 0, "author": "Lardinio", "replies": [{"selftext": "That's good to hear. Thanks", "ups": 4, "depth": 1, "author": "FreddyKS", "replies": []}, {"selftext": "&gt; I'm getting better performance than my old 6800xt\n\nI would hope so...", "ups": 2, "depth": 1, "author": "jonker5101", "replies": [{"selftext": "Were you dropped on your head as a child?", "ups": -1, "depth": 2, "author": "Lardinio", "replies": [{"selftext": "Do you think wattage is directly proportional to performance?", "ups": 1, "depth": 3, "author": "jonker5101", "replies": [{"selftext": "No, and I can't see where I said that either. The op was asking about temps, that is why I included the power usage.", "ups": 1, "depth": 4, "author": "Lardinio", "replies": []}]}]}]}]}, {"selftext": "This is for a 7900xt but its the same cooler.  When new the hotspot was usually maxing around 85c with an edge/hotspot delta of 20 or less.  Hotspot started to creep up around 3 months and the edge/hotspot delta had risen to 30c.  \n\nSwitched to xtx model after finding out alphacools block wouldnt fit the xt.  So i slapped ptm7950 on the xt and sold it.", "ups": 2, "depth": 0, "author": "jaymobe07", "replies": [{"selftext": "Did the honeywell thermal pad helped ? :D\n\nHow do you know if XTX won't do the same after some time ?", "ups": 1, "depth": 1, "author": "FreddyKS", "replies": [{"selftext": "On my liquid devil 7900 XTX hotspot creeped up with stock paste little by little first month getting really bad after 3 months it would not even go back down even with a low watertemp of 23c so i repasted 2 times with MX-4 gave 75c hotspot then 1-2 weeks later it was back at 92c hotspot for 2 times, 2e time i gave up but then remembered about PTM7950 did my research and put that on edge down to 42c at 22-23c watertemp and hotspot in the 60-70c range at 22-23c watertemp unleashed bios no undervolt at 410w tbp\n\nPTM7950 is amazing, currently stupid heatwave but my PTM7950 is on mor then a month now nearing 2 months probably soon no sign of degradation, in 2 more days heatwave is over again hopefully can do some testing again to see if anything changed.\n\nif you get PTM7950 safest but also more expensive is via moddiy i got mine via ebuy7 which linustechtips recommended out of 2 that he tested he also tested aliexpress one but provided no results of that.", "ups": 2, "depth": 2, "author": "Melodias3", "replies": [{"selftext": "Oh that's nice to hear. Did you also changed thermal pads to new ones as well ?\n\nWhat size of PTM7950 is required for this chip ?", "ups": 1, "depth": 3, "author": "FreddyKS", "replies": [{"selftext": "Just get bigger size and cut to size, you may fail first time pad rips easily when trying to apply i got 80\\*80\\*0,2mm from ebuy7 its more then you may need i failed once succeeded 2e time.\n\nI did not change the pads on memory and vrm as i undestood compression was fine vram never exceeds 58c which is good enough", "ups": 1, "depth": 4, "author": "Melodias3", "replies": [{"selftext": "Good to know. \nNevertheless I hope changing paste won't be necessary right after purchase :D", "ups": 1, "depth": 5, "author": "FreddyKS", "replies": []}]}]}]}, {"selftext": "I put the pad on the xtx when i put the waterblock on. So really i dont know for sure if it would have done same thing. I'm assuming it would have since its the same cooler and paste.  it is a pain to install so make sure you have extra because you'll probably rip the first one\n\nOn the xt it brought the temps from 95c hotspot to 80c.  Or 85c to 80c if comparing stock.  But that was in an open system and default settings.  Ambient around 22-25c.", "ups": 1, "depth": 2, "author": "jaymobe07", "replies": []}]}]}, {"selftext": "Got the card - Hotspot is a little high, but not hitting the throttle limit, so still okayish. It draws up to 400 watts due to OC and can be pushed to draw up to 450 watts which doesn't make sense for the minimal performance gain.\n\nA little bit of tweaking and I got it down to 280-310 Watts with little to none performance loss at all", "ups": 2, "depth": 0, "author": "L1ghtbird", "replies": [{"selftext": "By undervolting I suppose ? :)\nHow much did you squeeze ?\n\nThese is also possibility to reduce core and memory clock speeds in order to reduce temps but this would be wasting of performance in order to gain better temps", "ups": 1, "depth": 1, "author": "FreddyKS", "replies": [{"selftext": "Ye, it took me round about 2 hours to find the stable values for my card, but it's worth the effort.\n\nI have to look up the numbers when I'm at home, afak Voltage is at 91%; VRAM 2800, Chip limited to 2450, 2500 or something like that.\n\nI ran Red Dead Redemption 2 at 4k in the background for stability testing and FPS - had a spot where it ran with exactly 100 FPS which was also my target at the end of the tweaks", "ups": 1, "depth": 2, "author": "L1ghtbird", "replies": []}]}]}, {"selftext": "Asrock mid and top of the line products are actually surprisingly good. The only drawback is that it might not be packed with add-on features such as RGB or aesthetic designs or native software. However, their hardware except for the dirt  cheap  low-ends are exceptional and up there in terms of overclocking capability.\n\nAlso, their hardware design is also very mod-friendly (looking at you MSI when openRGB physically broke a bunch of rgb controllers).", "ups": 2, "depth": 0, "author": "minhquan3105", "replies": [{"selftext": "Well I heard different opinions of Asrock quality, at least of this model :)\n\nBut good to know", "ups": 1, "depth": 1, "author": "FreddyKS", "replies": []}]}, {"selftext": "I just repasted mine with nt-h1 yesterday and it was 100% worth it. Went from experiencing 100C+ hotspot temps when gaming at native 4k to like 90-95 max in brief moments with it mostly lingering in the 80s under the same workloads (card was undervolted each time). It's a night and day difference.\n\nThe stock paste seemed to be very pumped out from the core, it was clearly missing coverage on the die. Seems to be a pervasive issue with this specific model, but it's easily fixable.\n\nEdit: Using nt-h1 will definitely be a temporary solution as I thought, I'm already seeing temperatures climb back up. Gonna try applying a PTM7950 pad and see if that's as good as people say it is.", "ups": 2, "depth": 0, "author": "The_American_Viking", "replies": []}, {"selftext": "I have this card. Testing it with Starfield I\u2019m getting 65 C edge and 100 C hotspot temperatures. My understanding is that this temperature difference is too large and that I should RMA the card.", "ups": 2, "depth": 0, "author": "FronzelNeekburm", "replies": [{"selftext": "From I have read on internet this is big difference. I think it should be delta 20-25 C. On my actual card gigabyte rtx 3060ti hot point is max 10-15 C hotter than overall temp", "ups": 1, "depth": 1, "author": "FreddyKS", "replies": []}]}, {"selftext": "Seems like they tend to suffer from pump out. I have the XT and I can see it in my future already with hotspot temps creeping up. I think I'm going to do the PTM7950 at some point", "ups": 2, "depth": 0, "author": "RedLimes", "replies": []}, {"selftext": "I just started an RMA due to 110\u00b0 hotspot temp which was 30-35\u00b0 delta.\n\nHowever, with a slight undervolt, this card runs really nicely. This is the first time I've RMA'd a gpu so we'll see what happens. I can update my experience here.", "ups": 2, "depth": 0, "author": "poopsharpie", "replies": [{"selftext": "Oh that's bad to hear. How long did you have the card before it started ?\n\nYes please let us know about your experience with RMA if possible. It might come handy in future :D", "ups": 2, "depth": 1, "author": "FreddyKS", "replies": []}]}, {"selftext": "I've had one.\nFan Connector wasn't plugged in and even after plugging it in the Hotspot nearly reached 110\u00b0C.\nWith a Delta of around 30\u00b0C (78 hottest to 105 Hotspot)\nEven though i have a case with good airflow.\n\nSo most likely faulty card but my Sapphire Pulse, which I stayed with, also goes to nearly 100\u00b0C and I undervolted it which rarely makes it touch the 90+ Area on the Hotspot.\n\nDecide for yourself but feels like it may be the shipment to Germany or my Mainboard not being able to carry these big cards\ud83d\ude02", "ups": 2, "depth": 0, "author": "Metal_is_Perfection", "replies": []}, {"selftext": "I have the 7900 XT version, and it's been smooth sailing since I bought it, no noticeable coil whine, no thermal throttling, I have not noticed any hot spot temps either and the high wattage use has subsided since the last driver update.", "ups": 2, "depth": 0, "author": "Twinkalicious", "replies": []}, {"selftext": "Can't say for the 7900xtx, but I bought a 6950xt Phantom Gaming for the same reason - much cheaper than competition. \n\nMy observation is that the temperatures are quite alright (usually &lt;85 hotspot, sometimes 88-9, &lt;65 regular core). However, the chip itself performs slightly below average without thermal throttling, so I wouldn't expect much from the silicon lottery. Mine is pretty much a 6900xt because of this.", "ups": 2, "depth": 0, "author": "Trivo3", "replies": []}, {"selftext": "My first XTX was the Asrock Phantom gaming - I ended with a RMA due to coil whine and temp on 105 in first round of a game total war. \nThe\n\nI did pick Asrock PG due to price, but it is hit and miss with the PG model on how they have made the cooling. I got my PG replaced very easy due to it be a well know error. \n\nI did end up with paying more for the sapphire nitro+ because I like when it works out of the box and I don\u2019t want to repast it. \nWith Asrock I would go with their premium model and not this - there is a reason why PG have been on sale\u2026.", "ups": 1, "depth": 0, "author": "Nerd2much-energy", "replies": [{"selftext": "Well that's true. \nIt's shame that is it lottery but the price is in my case the key factor :/\n\nAre you satisfied with performance ? :D", "ups": 2, "depth": 1, "author": "FreddyKS", "replies": [{"selftext": "My Sapphire is a different world in experience of how well-made the GPU is and the Temp is not hitting anything near 100c as my Asrock did.   \nBut I get you - I really do, the price difference here in Denmark was 18% between the two GPU's.  If you get one of the ASrock PG without any error in cooling etc. the difference between is not 18%.   \n\n\nNormally I am a fan of ASrock, but this  generation both my GPU and Motherboard failed out of the box.   \n\n\nDid you end up with the Phantom Gaming? and did you win the Lottery? (Hope you did)", "ups": 1, "depth": 2, "author": "Nerd2much-energy", "replies": [{"selftext": "Well I don't really know but I guess not.\n\nOn stock settings delta between edge and hotspot was max 27 C under max load ( 97 C max hotspot temp )\n\nNow I undervolted and underclocked my GPU and I'm getting about 8% less performance but now delta is max 18 C . I guess with this price I will stick to this card if it won't get any worse", "ups": 2, "depth": 3, "author": "FreddyKS", "replies": []}]}]}]}, {"selftext": "I have ASRock RX 6800 XT Phantom Gaming. It's a good card.\n\nI'd recommend to check fan curve and configure it properly. By default, my card's fan limit is [57% at 80\u00b0C](https://ibb.co/HFzCk78). So, even if the card reaches 100+\u00b0C, the fan would not perform better. And that's the issue.\n\nHere is [the example](https://ibb.co/hMvjLVj) for my card.", "ups": 1, "depth": 0, "author": "uw_cma", "replies": []}, {"selftext": "Mine ran at 100c + but didn't throttle.  I went to a custom liquid cool build and it runs at 50-65c depending on the title.   Other than that runs everything perfectly.", "ups": 1, "depth": 0, "author": "XZelmanX", "replies": []}, {"selftext": "Undervolt the card and it will be fine", "ups": 1, "depth": 0, "author": "Firecracker048", "replies": []}, {"selftext": "I have this card, no issues at all. Playing mostly bf2042 and r6s at 2k. Paired with a i7 12700k and 32 gb 3200.", "ups": 1, "depth": 0, "author": "Havingfun2nightez", "replies": []}, {"selftext": "Got mine a few days ago, bought it because it was the cheapest xtx I can find.\nSo far, it\u2019s been a solid performer!\nI do get some strange vibration when the fan hits ~40% but it goes away as soon as it\u2019s above 50%. Not sure if it\u2019s cause for concern\u2026\nI have mine set to constant 30% fan speed, perfectly silent and keeps temp in the 60 degree region, which is great. I am not really loading the card that much honestly, playing Starfield at 1440p native and everything maxed. Getting a solid 60 fps, no dips whatsoever!", "ups": 1, "depth": 0, "author": "nmo90", "replies": [{"selftext": "That's good to hear. Thanks :)", "ups": 1, "depth": 1, "author": "FreddyKS", "replies": []}]}, {"selftext": "I don't know what you're talking about. I am rus.", "ups": 1, "depth": 0, "author": "TantArtem", "replies": []}, {"selftext": "I bought today 7800XT Phantom, curious how much UV can achieve! Design is sick, feeling premium in hands. Design is same as yours?", "ups": 1, "depth": 0, "author": "feorun5", "replies": [{"selftext": "Not exactly same design but quite similar :)\nHope it will work :D", "ups": 1, "depth": 1, "author": "FreddyKS", "replies": [{"selftext": "I won't overclock it as its 135+ factory overclocked, just UV.", "ups": 1, "depth": 2, "author": "feorun5", "replies": []}]}]}, {"selftext": "asrock is goated  Its just as good as sapphire", "ups": 1, "depth": 0, "author": "X-ATM095", "replies": [{"selftext": "Good to hear :D\n\nI hear many different opinions both that Asrock is good and bad so well, maybe truth lays between \nPersonally I owned cards only from gigabyte and MSI and I can recommend both", "ups": 1, "depth": 1, "author": "FreddyKS", "replies": [{"selftext": "Both of my pcs use asrock as motherboards \n\nMain pc Phantom gaming x x570 Its been through a 3800xt 3950x and a 5950x. Never had a issue.\n\nMy mini itx has a ASRock B550 Phantom Gaming-ITX/ax it's been through a 3600x 5600x to a 5800x3d never a issue\n\nAsrock has been great motherboard to me", "ups": 2, "depth": 2, "author": "X-ATM095", "replies": []}]}]}, {"selftext": "Once I bought an Asus anti-sag bracket, any temperature issues I've had went away, although I did replace the thermal paste with a TG cryosheet. Love the card.", "ups": 1, "depth": 0, "author": "deecop", "replies": []}, {"selftext": "To stay calm and get things properly working please buy a good liquid cooler, a good case and designing a good airflow (from bottom to top) you shouldn't have problems. Please buy a good thermal paste and apply a decent qty, good paste  not a lot of paste remember.", "ups": 1, "depth": 0, "author": "BlacksmithFelix", "replies": []}, {"selftext": "A common issue with such heavy radiators is for the radiator to bend in transport or sag over time during normal use, which can result in bad temps. Otherwise - no issues.", "ups": 1, "depth": 0, "author": "vice123", "replies": []}, {"selftext": "My local forums have info on hotspot issues, thermal paste is applied badly (too many of it). You can apply phase thermal interface  like Honeywell 7950 if your country's warranty laws are good enough. Or ask a service company to. Just mind the warranty. Don't use normal paste, it would get sucked out in a few months.\n\nASRock knows the issue and should have fixed it, but you don't know when your card is made.\n\nIf you don't have hotspot issues, then don't bother.", "ups": 1, "depth": 0, "author": "Vivicector", "replies": [{"selftext": "Good to know, thanks :)\n\nI will most probably apply termopad Honeywell 7950 myself after warranty period is over. \nI don't mind sending off my card for 2 weeks :D\n\nIt's a pity that top tier card have such problems :/\n\nI've got gigabyte ( I think it's not the best manufacturer) rtx 3060ti for few years now and it never exceed 60 C under heavy load, with max hot spot of 70 C", "ups": 1, "depth": 1, "author": "FreddyKS", "replies": []}]}, {"selftext": "ASRock has been good lately.\n\nThermal paste pump out is a problem with some 7000 series cards but happens with any brand and not with all cards. Just keep an eye on your hot spot temps. If you see them stable over time your good. If you see them creeping up towards 90+100+c after a while, means you have thermal paste pump out. Solution is either PTM7590 or thermal grizzly kryosheet.", "ups": 1, "depth": 0, "author": "jamexman", "replies": [{"selftext": "Well tbh hotspot was reaching 97 c just after taking out of box.\nMaybe in future I will change thermal paste to PTM", "ups": 1, "depth": 1, "author": "FreddyKS", "replies": []}]}]}
{"post": {"title": "Temps drop with SAM active", "subreddit": "Amd", "selftext": "So recently, I had rebuilt my PC and noticed that I never had SAM active in my Radeon settings after noticing I would consistently thermal throttle while playing Starfield.\n\nI'm not going to go into too much detail about what I did, but I did end up changing my drives from MBR to GTP and was finally able to activate SAM. I was curious to see what the performance increase would be with it on versus off, so I loaded up Starfield as it's currently the most GPU intensive game I have noticed that my junction temps would consistently hit between 98\u00b0C and 109\u00b0C with SAM off. The weirdest thing is that with SAM on, the junction temps would be anywhere from 15-20\u00b0C cooler in the same scenario.\n\nI didn't make any changes to the OC/UV that I have on my card, and I kept the aggressive fan curve that I have as well between runs, so it can't be anything that I've done on my end.\n\nBasically, I'm just wondering if there's some kind of explanation as to why my junction temps would be lower with SAM on versus being off.\n\nTLDR; I rebuilt my PC and was able to use SAM and found my temps were lower, and I'm just curious as to why.\n\nEdit: I realized that I never mentioned my delta, but it never went past 21\u00b0C. At the lowest, it's 15\u00b0C.\n\nEdit 2: I repasted with some kryonaut extreme after you guys recommended I do so. There was most definitely not enough paste on the die when I opened it up. I went from pushing 109\u00b0C on the junction to not even hitting 70\u00b0C, with my delta dropping by 5-10\u00b0C based on the load.", "ups": 7, "permalink": "/r/Amd/comments/16g7m9o/temps_drop_with_sam_active/", "num_comments": 20}, "replies": [{"selftext": "I think sam would split the workload across the memory modules better. Maybe that takes the temp from the hardest working module with it off", "ups": 3, "depth": 0, "author": "RedhawkAs", "replies": []}, {"selftext": "I'll test this later on.", "ups": 2, "depth": 0, "author": "kaisersolo", "replies": []}, {"selftext": "That makes just about no sense, unless SAM is actually worsening performance badly.\n\nAlso, those temps are nasty, needs a repaste badly.", "ups": 1, "depth": 0, "author": "LongFluffyDragon", "replies": [{"selftext": "Actually SAM is making it better, which coincides with the temps dropping. I just don't know how.\n\nAs for the repaste, I'm kinda nervous to take apart my gpu and I'm outside the warranty period to send it for a RMA.", "ups": 1, "depth": 1, "author": "Shockapow", "replies": [{"selftext": "Those temps are approaching broken territory and likely to cause damage over the long term anyway. Just approach disassembly carefully, the only real risk is greatly over-tightening the screws and squashing the GPU chip.", "ups": 1, "depth": 2, "author": "LongFluffyDragon", "replies": [{"selftext": "I'll look into doing the repasting, but I'm not too worried about temps anymore since they've dropped so much after enabling SAM. Starfield was the only game I had that got anywhere near 110. Most of the games I play would only ever reach, like, 80-85\u00b0C before I was able to use SAM.", "ups": 1, "depth": 3, "author": "Shockapow", "replies": [{"selftext": "That just means most of your games are comparatively undemanding, and any game capable of using your GPU effectively/not CPU bottlenecked will do the same.", "ups": 1, "depth": 4, "author": "LongFluffyDragon", "replies": [{"selftext": "That's the weird part for me though, I've never had a CPU bottleneck and I run my games either at ultra or until I have them running at around 60 fps. I looked my CPU utilization and it sits usually around 30-40%. For example Cyberpunk at max graphics with raytracing sits at around 34-38% CPU util in most areas and my GPU never even hits 90\u00b0C. I'm just super confused at this point.", "ups": 1, "depth": 5, "author": "Shockapow", "replies": [{"selftext": "you do realize you can have a cpu bottle neck without the number saying 100%, right?", "ups": 3, "depth": 6, "author": "pyr0kid", "replies": []}, {"selftext": "CPU core utilization is not an indicator of CPU bottleneck, in any way.", "ups": 0, "depth": 6, "author": "LongFluffyDragon", "replies": [{"selftext": "If it's not utilization, would it be temps? If that's the case, my CPU barely goes above 70\u00b0C.", "ups": 1, "depth": 7, "author": "Shockapow", "replies": [{"selftext": "You can be totally CPU bottlenecked at or even below one thread worth of resource usage, depending on how well-multithreaded a program is. It varies by orders of magnitude between different games.\n\nGPU usage/power dropping is a normal indicator of a bottleneck elsewhere.", "ups": 1, "depth": 8, "author": "LongFluffyDragon", "replies": [{"selftext": "Okay, so another thing that confuses me: If GPU usage/power drops, shouldn't that mean that temps should also drop since it's not working as hard/using as much power? It would make no sense for the opposite to happen.", "ups": 1, "depth": 9, "author": "Shockapow", "replies": []}]}]}]}]}]}]}]}]}]}, {"selftext": "surface - air - missile", "ups": 1, "depth": 0, "author": "Ok_Release_6303", "replies": []}, {"selftext": "100-110c junction is quite hot i would repaste, get higher quality paste or go for PTM7950 which is probably best that can get cos not only does it give much better results then thermal paste, it also last lifetime of the card.\n\nsince you got an aggressive fan curve may need to repaste anyway, if the card is pretty old may need to clean up the heatsink as well to remove any clogging dust.", "ups": 1, "depth": 0, "author": "Melodias3", "replies": [{"selftext": "The card is only about a year old, so I shouldn't need to clean the heatsink, but I was going to do so anyways since I'm already there", "ups": 1, "depth": 1, "author": "Shockapow", "replies": [{"selftext": "You might be seeing pump-out on the included paste, as other posts have shown, some cards just used a lower quality paste.", "ups": 1, "depth": 2, "author": "quotemycode", "replies": [{"selftext": "Which is such a disappointment since I have a Red Devil. I've never had a problem with temps with them before.", "ups": 1, "depth": 3, "author": "Shockapow", "replies": []}]}]}]}]}
{"post": {"title": "MSI and Asus 7800xt GPU", "subreddit": "Amd", "selftext": "Hi all\n\nHas anyone heard of or read about MSI releasing a Gaming X 7800xt or Asus releasing a Rog Strix 7800xt? I've had a google but can't find anythong about MSI 7800 xt cards and the only Asus card is the TUF Gaming 7800xt\n\nWondering if they have simply not bothered with this gen?\n\nThanks in advance!!\n\nEdit - seems the census is negative for MSI and Asus. I was looking at the Sapphire Nitro+ 7800xt anyways, just curious about the MSI and Asus card as I do like their aesthetics more so than the Sapphire or Asrock cards.\nThanks for all the responses! Much appreciated!", "ups": 16, "permalink": "/r/Amd/comments/16fs8ho/msi_and_asus_7800xt_gpu/", "num_comments": 27}, "replies": [{"selftext": "Xfx, powercoolor or sapphire..", "ups": 17, "depth": 0, "author": "ilordd", "replies": []}, {"selftext": "Here in Brazil:  \n\"Nvidia's Dogshit partner\": 36 months of warranty or more\n\nAMD golden partners (XFX, Sapphire, Powercolor): 6 months or with luck 12 months.", "ups": 10, "depth": 0, "author": "niecantrell", "replies": [{"selftext": "I talk to Sapphire\u2019s support and they tell me we have 2 year warranty", "ups": 1, "depth": 1, "author": "XAN6NA", "replies": [{"selftext": "Unfortunately its a International RMA while MSI, Asus and Gigabyte got local RMA.\nI prefer to not even imagine how much we would pay on taxes in the case of a International RMA.", "ups": 3, "depth": 2, "author": "niecantrell", "replies": []}]}]}, {"selftext": "Why do you wanz Nvidias dogshit partner cards anyway?\n\n\nGet a Sapphire, XFX, PowerColor or AsRock.", "ups": 20, "depth": 0, "author": "Edgar101420", "replies": [{"selftext": "If I was going to get thicc boy card, it would definitely be Sapphire Nitro", "ups": 5, "depth": 1, "author": "L1191", "replies": []}, {"selftext": "AsRock is up there with the big three? I cannot find any cards but the AsRock Steel Legend, I did not know how well AsRock was, so I held off. And now it's sold out as well.", "ups": 2, "depth": 1, "author": "OrangeYouGladish", "replies": []}, {"selftext": "What about Aorus?", "ups": 1, "depth": 1, "author": "Achro669", "replies": [{"selftext": "Also dog shit. Sapphire is the best bet. I don't particularly have high opinion of powercolor, and asrock either. Granted I am using a day 1 MBA XTX - probably the worst out of the bunch.", "ups": 7, "depth": 2, "author": "Medical-Tomorrow7727", "replies": [{"selftext": "I had several PowerColor and they are one of the best cards I've ever had. Even compared to Sapphire.   \nBut like someone said, you can't go wrong with PowerColor or Sapphire, both the top AMD brands.", "ups": 5, "depth": 3, "author": "Aaadvarke", "replies": []}]}]}, {"selftext": "Lol", "ups": -1, "depth": 1, "author": "feorun5", "replies": []}]}, {"selftext": "Don\u2019t bother with manufacturers that make Nvidia cards. They don\u2019t care enough for their line of AMD cards. It\u2019s proven many times. \n\nGo with XFX, or if you have enough budget, go with Sapphire and PowerColor.", "ups": 9, "depth": 0, "author": "ilGattoBipolare", "replies": [{"selftext": "It makes sense for an exclusive partner to make better products vs a brand that makes both.", "ups": 3, "depth": 1, "author": "Saffy_7", "replies": []}, {"selftext": "I keep reading this, why is it that people say MSI Radeon GPU's are bad?  \nI always had MSI NVIDIA GPU's since the 6600GT and never had any problems with them.  \nI'm new to Radeon GPU's, so I just went with what I know is good so I have an MSI RX6700XT and I'm pretty happy with it, no temperature issues, no noise complaints, no problems at all, or am I just lucky with this card?", "ups": 1, "depth": 1, "author": "Dreminator", "replies": [{"selftext": "It's not a blanket rule, but there have been a few times in the past when MSI and ASUS designed terrible coolers for AMD cards, sometimes it looked like they reused coolers made for Nvidia cards.\n\nBecause of that bad rep, people recommend AMD exclusive partners like sapphire, xfx, powercolor in general... unless you really do your research and it shows that MSI and ASUS didn't shit the bed... I did see a review that showed the ASUS 7800xt is a good card.", "ups": 11, "depth": 2, "author": "ToothChainzz", "replies": []}, {"selftext": "any models can be bad, just look at amd's reference 7900 XTX vapor camber issue that got fixed\n\nthis is why people should read reviews, the 7800 XT hellhound seem to be one of the best 7800 XT cards, as its really cool and quiet", "ups": 8, "depth": 2, "author": "nwgat", "replies": []}, {"selftext": "As other 3 said, every AIB has bad cards at some point. But it becomes an echo chamber on this Reddit without thinking much about it\n\nAlways best to wait for reviews to be sure instead of listening to people here who mindlessly repeating", "ups": 3, "depth": 2, "author": "Zoart666", "replies": []}, {"selftext": "Because those brands just lazily copy paste their designs onto Radeon cards.", "ups": 2, "depth": 2, "author": "TheContingencyMan", "replies": []}, {"selftext": "Don't worry mate, people are biased. I had and tested plenty of MSI cards and had no issues (6700xt and 6800xt) and also tested a Nitro + 6900xt and that one has temp issues. Should i now claim that all Nitro cards are trash? \n\nIt's impossible for all products to be perfect.", "ups": 3, "depth": 2, "author": "SonOfAnarchy91", "replies": []}, {"selftext": "because almost every single time someone complains about a bad/lemon card it's a ASUS one and sometimes a gigabyte and very rarely MSI. \n\nfrom worst to bad in this order ASUS &gt;&gt; gigabyte &gt; MSI. \n\nAMD partneres are always a better choice if you value stability and proper design.", "ups": 1, "depth": 2, "author": "Admixues", "replies": []}]}]}, {"selftext": "I've been looking at switching to AMD and I've noticed a lot of 2 year warranties when I'm so used to 3 from just about any Nvidia card I have ever bought. Really threw me off and is kinda pushing me towards the 3 year warranty ones like Asus, Gigabyte, and MSI. I think XFX and Asrock is 3 year as well. At least warranties for the US.", "ups": 2, "depth": 0, "author": "Sturmx", "replies": []}, {"selftext": "go sapphire", "ups": 2, "depth": 0, "author": "TheCaptainGhost", "replies": []}, {"selftext": "I doubt they will, plus their AMD graphics cards aren\u2019t even very good in the first place.", "ups": 3, "depth": 0, "author": "CheemsGD", "replies": []}, {"selftext": "Both make great NVIDIA custom cards but their AMD cards aren't good. Just get a high-end Sapphire, XFX, Powercolor or Asrock Radeon card.", "ups": 3, "depth": 0, "author": "ZeroSum12", "replies": []}, {"selftext": "Msi has good nvidia cards, but not as good for amd. If you want the best quality 7800xt, get the asrock phantom gaming or the sapphire nitro+.", "ups": 1, "depth": 0, "author": "Dinguil", "replies": []}, {"selftext": "MSI and Asus is becoming evga replacement for nvidia, thus ignore them from now on. Also, ignore Gigabyte and Biostar as well  they are literally the worsts", "ups": 1, "depth": 0, "author": "minhquan3105", "replies": []}, {"selftext": "I ordered an ASUS one and waiting for it, they were the cheapest option with 3 fans in my region, the other options was Gigabyte which I try to avoid. \n\nI got a gigabyte board and it's just bugy as fuck. Hope I made the right choice with the ASUS one...", "ups": 1, "depth": 0, "author": "Mardoo", "replies": []}]}
{"post": {"title": "VSR Supported Resolutions", "subreddit": "Amd", "selftext": "Why are the VSR supported resolutions so limited?\n\nIt's actually kind of impressive there isn't any support for anyone wandering out of the 1080p,1440p/16:9 world, what if I'm using a 1440p 32:9 monitor and I want to VSR 2160p?\n\nI'm using an older CRT monitor for gaming as a second monitor, it's 4:3 and can do 2048x1536, great stuff. Before, I used DSR to downscale 3072x2304 (2.25x) and had great results, the image was noticeably sharper, it's actually a great use for this feature. Now? Now I can't even enable VSR no matter what resolution I set my monitor to.  I guess that VRAM upgrade won't matter much after all.\n\nOh well, maybe I'm being unreasonable and using an MST hub to display something is a crazy unrealistic scenario never envisioned by any AMD software engineer and that's my issue here...\n\nOkay then, I guess I'll try on my other monitor, it's 1080p and set up in portrait so I can quickly read documents and have Discord on the side while gaming... This isn't a crazy use case, lots of people have portrait monitors. Well, VSR doesn't work on it either because it's 9:16. Same resolution and aspect ratio, just on the other side...\n\nWhy is it that this feature is so limited compared to Nvidia's offering? Why would it be limited in the first place? If I'm able to display a given resolution what is actually preventing me from displaying, say, 2.25x this resolution, other than the maximum resolution supported?\n\nHow can I manifest to AMD that this is a useful feature that is currently outclassed by its competing alternative, simply because of the seemingly arbitrary limited range of resolutions supported?", "ups": 6, "permalink": "/r/Amd/comments/16fubas/vsr_supported_resolutions/", "num_comments": 11}, "replies": [{"selftext": "I think it's becaues VSR is hardware based scalar while DSR is done in software. From my experience, VSR's downscaling was better while text became unreadable with DSR while going down from 1440p -&gt; 1080p.", "ups": 3, "depth": 0, "author": "bctoy", "replies": [{"selftext": "While searching for my issue I found multiple forum posts about people complaining their display had the VSR option removed after a driver update from 2018 onwards, some also went back to their previous driver and had the option appear again.\n\nThey've been removing support on this feature for a long time. The hardware is improving, the supported resolutions shouldn't be shrinking. The method used to achieve the result isn't a defining factor in the supported resolutions, the GPU is able to render both the VSR and output resolutions. Why would it suddenly not be able to render at 2.25x if it's supported?\n\nIt seems like a trivial fix to just enable the VSR option on any resolution and let users figure out which resolutions look good in VSR.", "ups": 3, "depth": 1, "author": "The_Ravio_Lee", "replies": [{"selftext": "Because GPU rendering a specific resolution does not mean that the hardware scalar being used for VSR will be able to scale it too. \n\nMaybe they were looking at approach similar to nvidia's but later on removed it. I did get an 8k option for 1080p monitors on 6800XT last year.", "ups": 2, "depth": 2, "author": "bctoy", "replies": []}]}]}, {"selftext": "[VSR is capable of a lot depending on the EDID of your display and the variations of resolutions...](https://i.imgur.com/t1qf4x6.jpg)\n\n&amp;#x200B;\n\nmaybe i'll elaborate more.\n\nPretty much any display regardless of the resolution ratios and such, will support up to a maximum of double the vertical and horizontal. Doesn't matter if it's 16:9... 21:9... 32:9... or rotated 9:16 or 9:21 or whatever, this also includes various other non-standard resolutions.\n\nThe only limiting favor i find is often the specific brand/model of displays that don't natively show any additional VSR resolutions which is real weird to see because you can grab another displays such as a samsung display and it'll all work flawlessly. My experience with LG, AOC, Asus, MSI and gigabyte, to name a few monitors, as well as several hisense, sharp, toshiba to name a few displays that for whatever reason are prone to fucking up even in normal resolution modes dude to shitty EDID information. Honestly i really don't get why these companies seem to always have something fucking up. Anyways, at that point setting a custom resolution may sort it out for gaining some odd-ball ratios, but getting VSR to work on such displays is a bit of a joke.\n\nAlso, DP \\~&gt; active VGA adapters are going to definitely throw wrenches into the mix.", "ups": 3, "depth": 0, "author": "DHJudas", "replies": []}, {"selftext": "16:10 is stuck with 16:10 VSR options as well.  With my old 1200p monitor my only choices were 2560x1600 or 3840x2400.  Thus I couldn't test 1440p or 4k with benchmarks that didn't have their own methods of faking it.", "ups": 1, "depth": 0, "author": "DuskOfANewAge", "replies": [{"selftext": "Oh man, I didn't even think about the 16:10 crowd... Want to use an eGPU to game on a 16:10 laptop with VSR? Nope.", "ups": 1, "depth": 1, "author": "The_Ravio_Lee", "replies": []}]}, {"selftext": "Try toggling the GPU scaling option in AMD, software, gaming, display and see if its better with AMD doing it or the monitor itself and maybe even the Scaling mode below that.", "ups": 1, "depth": 0, "author": "zeus1911", "replies": [{"selftext": "Enabling GPU scaling doesn't allow me to enable VSR.", "ups": 1, "depth": 1, "author": "The_Ravio_Lee", "replies": [{"selftext": "Mine says it only stops RSR if GPU scaling disabled, but I can have VSR and GPU scaling enabled. On 7000 series.", "ups": 1, "depth": 2, "author": "zeus1911", "replies": [{"selftext": "My problem comes from VSR only supporting a handful of resolutions and needing some sort of EDID handshake. I can set my monitor to a supported resolution since it's CRT, but I can never enable VSR because it doesn't recognize the EDID on my monitor.", "ups": 1, "depth": 3, "author": "The_Ravio_Lee", "replies": []}]}]}]}, {"selftext": "VSR works for my 21:9 2560x1080 monitor, it show up ultrawide 5k resolution", "ups": 1, "depth": 0, "author": "leonardcoutinho", "replies": []}]}
{"post": {"title": "I would advice not using memory context restore", "subreddit": "Amd", "selftext": "Why would u ask?\n\nBefore i had a MSI X670E Carbon wifi motherboard and when i turned it on my pc and bios went haywire i couldn't figure out what it was at first. \n\nGetting BSOD with Memory\\_Management errors. Even in BIOS would be crashing if i wanted to change it back to normal and just like that either my motherboard,memory and cpu all broke down.\n\nSo now i have a new cpu,memory and motherboard the rest was all send to RMA. \n\nInstalled it during this week with a new Asus TUF Gaming X670E motherboard, new 7800X3D and new Gskill Flare X 6000mhz 32gb.\n\nWhen using EXPO it all works fine, starting up pc from cold boot works fine as well takes longer but i don't care about that so clearly CPU and Memory talk to one another if all is working well, it would take 1 minute at most from coldboot to windows itself.\n\nThen yesterday on sunday i thought maybe Memory Context will work properly? And to my surprise i was greated again with Memory\\_mangement BSOD and BIOS going fully haywire again now i know the big nono that causes it that is Memory Context Restore.\n\nI did reset my BIOS and put everything how i had before but clearly turning on memory context can be very damaging for your pc as i noticed.\n\nFrom my experience for both memory and cpu this just to much and clearly not worth using for AM5\n\nJust wanna ask do other people even tried it as well here having the same issue? It was frustrating and tiresome having so much of this stuff happening. \n\n&amp;#x200B;", "ups": 1, "permalink": "/r/Amd/comments/16fzh5i/i_would_advice_not_using_memory_context_restore/", "num_comments": 21}, "replies": [{"selftext": "No issues using MCR with Gigabyte X670E Aorus Master.  I tuned my memory to the tightest stable timings and it still works.  Are you sure you have Power Down Enabled?  It's required with MCR currently.", "ups": 7, "depth": 0, "author": "DuskOfANewAge", "replies": [{"selftext": "I didn't know that was required and why is it required? Will this prevent my bsod and bios crashing? Because this is tricky for me. I don't want to destroy my hardware again.", "ups": 0, "depth": 1, "author": "Nokterian", "replies": []}, {"selftext": "Is power down = erp? Like that thing that makes the mobo pretty much fully shut down, no power on USB ports etc.", "ups": 1, "depth": 1, "author": "Sinniee", "replies": [{"selftext": "No it's a memory specific setting.  It enables the RAM to power down during inactivity.  There's a bug in the latest BIOS/AGESA versions that forces you to leave Power Down enabled if you have Memory Context Restore enabled or you get BSODs.  This will be fixed in AGESA 1.0.0.8.", "ups": 3, "depth": 2, "author": "DuskOfANewAge", "replies": [{"selftext": "Aight ty", "ups": 2, "depth": 3, "author": "Sinniee", "replies": []}, {"selftext": "Thank you for the answer. Clearly I thought everything went wrong on my side so I did the memory context restore and power down enabled in the ram settings in the bios let's hope I will not have to worry for now and it will be fixed in the next agesa.", "ups": 1, "depth": 3, "author": "Nokterian", "replies": [{"selftext": "&gt;Thank you for the answer. Clearly I thought everything went wrong on my side so I did the memory context restore and power down enabled in the ram settings in the bios let's hope I will not have to worry for now and it will be fixed in the next agesa.\n\nHey, i was having exactly the same BSODs and everything, and yes, since a week with Power Down enabled, my System is rock stable so i wouldnt worry.\n\n&amp;#x200B;\n\nAlso a Bluescreen doesnt mean it damages your system. It MIGHT damage your OS, but else than that, if something like that happens in the feature u dont have to RMA, just clear CMOS (bios) and ur good to go to start from scratch with trying settings\n\n  \n(Obviously you can damage hardware by putting to much voltage on certain parts but thats not being done with MCR, it basically just saves the trained RAM state from instead of retraining everytime", "ups": 1, "depth": 4, "author": "MichiDreibeiner", "replies": [{"selftext": "Hmm ok so I tried it and still got bsod memory management. Can someone show me the proper settings for Asus?", "ups": 1, "depth": 5, "author": "Nokterian", "replies": []}, {"selftext": "What a wonderful comment. :) Your gratitude puts you on our list for the most grateful users this week on Reddit! You can view the full list on r/TheGratitudeBot.", "ups": 1, "depth": 5, "author": "TheGratitudeBot", "replies": []}]}]}, {"selftext": "Really? I don\u2019t have power down activated but I do have memory context restore turned on. No bsods everything running smoothly. I\u2019m on an Asus B650E-E with g skills 6000 CL30. Is it a question of time before it happens?", "ups": 1, "depth": 3, "author": "FlamingSword47", "replies": []}]}]}]}, {"selftext": "It's not memory context restore causing issues for you, it's that AND power down mode disabled.\n\nPower down mode must be enabled also, or your BIOS will act up, and you will BSOD", "ups": 4, "depth": 0, "author": "Practical-Canary8782", "replies": [{"selftext": "WHY IS THIS A THING?!?! I had the same issue and found the same fix. Feels like I fixed most problems but still haven\u2019t killed it off, because PC simply refuses windows restarts. Dram orange light turns on and stays there. Quick power cycle fixes it, and PC runs perfectly fine with 0 issue on manual timings and 6000mhz. Default expo causes freezes even with power down enabled. \n\nI believe it\u2019s the bios because a memtest showed no errors, and I can run demanding games for many hours with no crashes (so prob not cpu).", "ups": 1, "depth": 1, "author": "Brokenbonesjunior", "replies": []}]}, {"selftext": "It wasnt working for me on MSI X670-P Wifi, then retuned the board. I have run agesa 1.0.0.8a. Got Asus Strix B650E-E instead, so much better board. MCR should be run with Power Down enabled, afaik there was some bug introduced with 1.0..0.8b, not sure with 1.0..0.8c it was fixed or not.", "ups": 3, "depth": 0, "author": "Monsicek", "replies": [{"selftext": "1.0.0.7", "ups": 1, "depth": 1, "author": "OSSLover", "replies": [{"selftext": "yeah, thanks, typo on my side", "ups": 1, "depth": 2, "author": "Monsicek", "replies": [{"selftext": "Yeah, I can't wait for 1.0.0.9 so don't make me envious ![gif](emote|free_emotes_pack|flip_out)", "ups": 1, "depth": 3, "author": "OSSLover", "replies": []}]}]}]}, {"selftext": "If anyone has a solution for me that it will work properly please say it! Would be helpful.", "ups": 2, "depth": 0, "author": "Nokterian", "replies": [{"selftext": "Update bios", "ups": 1, "depth": 1, "author": "BausTidus", "replies": []}]}, {"selftext": "It works just fine in the AsRock X670E Taichi", "ups": 2, "depth": 0, "author": "OSSLover", "replies": [{"selftext": "I decided to check why.  It's because ASRock is awesome and just sets Power Down Enable to Auto by default lol.  I was using MCR without even realizing it on my X670E Taichi.", "ups": 1, "depth": 1, "author": "BlizzrdSnowMew", "replies": []}]}, {"selftext": "* 7800x3D\n* ASUS rog strix x670E-F gaming wifi\n* G.SKILL F5-6000J3040G32GX2-TZ5NR (2x32GB) | DDR5-6000 CL30-40-40-96 1.40V\n\nEnabled \u201cMemory context restore\u201d and \u201cPower Down Enable\u201d, no issues.\n\nUsing ASUS custom EXPO profile, that was made for this G.SKILL kit.", "ups": 2, "depth": 0, "author": "lichtspieler", "replies": []}]}
