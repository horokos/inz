{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ef9095-649a-441f-8a97-945c068b5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e961bb86-ffa4-44ca-ad10-5654cb808fee",
   "metadata": {},
   "source": [
    "wczytanie modelu również z kwantyzacją"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c0ac623-c9c8-4ce8-af4f-4aaa3fe2dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "base_model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35148921-885b-471d-9209-3806e67c78f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hirek/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9816576663048b4b233377a9eb033f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hirek/.local/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hirek/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/hirek/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8843b159-64f4-437a-815d-0675e29223aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026709d-4eb8-4c2a-9fe2-b546fc601138",
   "metadata": {},
   "source": [
    "wczytanie dotrenowanego modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de443187-8d42-4bd4-a5df-d4eca30da28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"llama-inz/checkpoint-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2419e458-e63b-4a68-b7d3-78bd55676843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"\"\"[INST]Given a context, score a comment from 0 to 9. Respond with just one number and nothing else.\n",
    "    \n",
    "    ### context: {example['title']} {example['post_text']}\n",
    "    ### comment: {example['selftext']}[/INST] \"\"\"\n",
    "    return text\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a1d53-2e27-464d-940a-05540cf1780d",
   "metadata": {},
   "source": [
    "zbiór testowy\n",
    "\n",
    "po 100 przykłądów z każdą wartością metryki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55b99f75-e39c-4dc5-8638-8b7ecc1c94e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'post_text', 'grade', 'selftext'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('json', data_files='datasets/prepared_dataset_256max_small/reddit_posts_test.json')\n",
    "dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338cf0a-d4c2-4351-8878-19a2861fdb0f",
   "metadata": {},
   "source": [
    "wygenerowanie odpowiedzi modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "769f8615-fcd5-4490-ac31-a6d518f29f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 1000 437818.7891440501/s time: 0.00011420249938964844 left: 0.0022840499877929688\n",
      "50 / 1000 5.3708595262205465/s time: 9.309647560119629 left: 176.8804407119751\n",
      "100 / 1000 5.413454570094574/s time: 18.545953035354614 left: 166.25243425369263\n",
      "150 / 1000 5.405153518945791/s time: 27.79644751548767 left: 157.25732803344727\n",
      "200 / 1000 5.488141906102107/s time: 36.90706157684326 left: 145.76882553100586\n",
      "250 / 1000 5.458278545202735/s time: 46.067522048950195 left: 137.40595936775208\n",
      "300 / 1000 5.2373348408039275/s time: 55.614428758621216 left: 133.65576601028442\n",
      "350 / 1000 5.477302952114996/s time: 64.74307179450989 left: 118.67154431343079\n",
      "400 / 1000 5.373172446708501/s time: 74.04862308502197 left: 111.66587448120117\n",
      "450 / 1000 5.4229314935835236/s time: 83.26879715919495 left: 101.42115950584412\n",
      "500 / 1000 5.390103397667707/s time: 92.5451226234436 left: 92.76259899139404\n",
      "550 / 1000 5.364516934555028/s time: 101.86569046974182 left: 83.88453340530396\n",
      "600 / 1000 5.418011253972274/s time: 111.09423089027405 left: 73.82782745361328\n",
      "650 / 1000 5.421291168516622/s time: 120.31719541549683 left: 64.56026601791382\n",
      "700 / 1000 5.335319614253567/s time: 129.68877053260803 left: 56.22905874252319\n",
      "750 / 1000 5.284618178605536/s time: 139.1502559185028 left: 47.30710744857788\n",
      "800 / 1000 5.438828198754871/s time: 148.34347891807556 left: 36.772626876831055\n",
      "850 / 1000 5.430930165967504/s time: 157.55006647109985 left: 27.61957812309265\n",
      "900 / 1000 5.236013622635617/s time: 167.09938216209412 left: 19.098498821258545\n",
      "950 / 1000 5.397835390668141/s time: 176.36241722106934 left: 9.262972354888916\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "grades = []\n",
    "prev = time.time()\n",
    "start = prev\n",
    "for i in range(1000):\n",
    "    if i % 50 == 0:\n",
    "        now = time.time()\n",
    "        print(i, '/ 1000', f'{50/(now - prev)}/s', f'time: {now - start}', f'left: {(1000-i)/50*(now - prev)}')\n",
    "        prev = time.time()\n",
    "    eval_prompt = formatting_func(dataset['train'][i])\n",
    "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = tokenizer.decode(model.generate(**model_input, max_new_tokens=1, pad_token_id=tokenizer.eos_token_id)[0],\n",
    "                               skip_special_tokens=True)\n",
    "        pred_grade = out[-1]\n",
    "        grades.append([pred_grade, dataset['train'][i]['grade']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8e7fb-fe33-495a-b5e0-fc85c5362d8d",
   "metadata": {},
   "source": [
    "sprawdzenie idelnych trafień modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5ddb47f-2bca-4aa7-93ae-88aa2ab4051c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procent skuteczności 0.295\n",
      "błędy: 0\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "c = 0\n",
    "errors = 0\n",
    "for x in grades:\n",
    "    try:\n",
    "        if int(x[0]) == x[1]:\n",
    "            total += 1\n",
    "    except:\n",
    "        errors += 1\n",
    "        continue\n",
    "    c += 1\n",
    "\n",
    "print('procent skuteczności', total/c)\n",
    "print('błędy:', errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c2858-3660-4b3d-b9ad-ff89547bf9c3",
   "metadata": {},
   "source": [
    "trafienie z pozwoleniem na błąd\n",
    "\n",
    "różnica o 1 - 2/3 \n",
    "\n",
    "różnica o 2 - 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d00ff18b-762a-4bed-a6e3-610e5429b970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procent skuteczności 0.5190700000000041\n",
      "błędy: 0\n",
      "tabelka różnic, od 0 po lewej do 9 [295, 267, 145, 101, 16, 15, 28, 9, 66, 58]\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "t = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "c = 0\n",
    "errors = 0\n",
    "for x in grades:\n",
    "    try:\n",
    "        a = int(x[0]) - x[1]\n",
    "        if a < 0:\n",
    "            a = -a\n",
    "        if a == 0:\n",
    "            total += 1\n",
    "        elif a < 3:\n",
    "            total += 0.33 * (3-a)\n",
    "        t[a] += 1\n",
    "        c += 1\n",
    "    except:\n",
    "        errors += 1\n",
    "        continue\n",
    "    \n",
    "\n",
    "print('procent skuteczności', total/c)\n",
    "print('błędy:', errors)\n",
    "print('tabelka różnic, od 0 po lewej do 9', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3978c3d-2abb-4f94-8793-84bcd8093af4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
